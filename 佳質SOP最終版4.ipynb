{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zWEoDBbOiLQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e501a4ad-8f09-4d35-c919-a563d3c0b361"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/776.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m286.7/776.1 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m776.1/776.1 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m165.6/165.6 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.5/127.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: langchain-core in /usr/local/lib/python3.11/dist-packages (0.3.56)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (0.3.39)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (4.13.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (2.11.4)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core) (3.10.18)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core) (0.4.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core) (2.4.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core) (1.3.1)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m437.7/437.7 kB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m101.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m82.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hå¿…è¦çš„å‡½å¼åº«åŒ…å«å·²å®‰è£å®Œç•¢ã€‚\n"
          ]
        }
      ],
      "source": [
        "# @title 1. ä¸‹è¼‰å¥—ä»¶\n",
        "!pip install -q rich\n",
        "!pip install -q pyngrok\n",
        "!pip install -q line-bot-sdk\n",
        "!pip install -q langchain-groq\n",
        "!pip install langchain-core\n",
        "\n",
        "# å®‰è£æ‰€éœ€çš„ Groq å‡½å¼åº«\n",
        "!pip install -q groq\n",
        "\n",
        "# æ³¨æ„ï¼šå¦‚æœæ‚¨çš„ Colab ç’°å¢ƒæœ‰ GPUï¼Œå¯ä»¥å°‡ faiss-cpu æ”¹ç‚º faiss-gpu ä»¥ç²å¾—æ›´å¥½æ•ˆèƒ½\n",
        "!pip install -q langchain langchain-community langchain-huggingface faiss-cpu sentence-transformers\n",
        "print(\"å¿…è¦çš„å‡½å¼åº«åŒ…å«å·²å®‰è£å®Œç•¢ã€‚\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tgbEC7ZEO6vY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad515701-1f41-4216-ad35-7c231b7ae1ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# @title 2. å°å…¥éœ€è¦çš„å·¥å…·\n",
        "import os\n",
        "from google.colab import userdata\n",
        "from rich.pretty import pprint\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#æ–°å¢imagesè³‡æ–™å¤¾"
      ],
      "metadata": {
        "id": "PMsvZWsVKUNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #åˆªé™¤imagesçš„åœ–ç‰‡\n",
        "# import shutil\n",
        "# import os\n",
        "\n",
        "# image_folder = 'images'\n",
        "\n",
        "# if os.path.exists(image_folder):\n",
        "#     try:\n",
        "#         shutil.rmtree(image_folder)\n",
        "#         print(f\"è³‡æ–™å¤¾ '{image_folder}' åŠå…¶å…§å®¹å·²æˆåŠŸåˆªé™¤ã€‚\")\n",
        "#     except OSError as e:\n",
        "#         print(f\"åˆªé™¤è³‡æ–™å¤¾ '{image_folder}' æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}\")\n",
        "# else:\n",
        "#     print(f\"è³‡æ–™å¤¾ '{image_folder}' ä¸å­˜åœ¨ï¼Œç„¡éœ€åˆªé™¤ã€‚\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MR_GQl-ZGtvF",
        "outputId": "562c9bb8-3fdf-4e9a-f13c-59d892bda356"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "è³‡æ–™å¤¾ 'images' åŠå…¶å…§å®¹å·²æˆåŠŸåˆªé™¤ã€‚\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# #å°‡åœ–ç‰‡ä¸‹è¼‰åˆ°é›²ç«¯ç¡¬ç¢Ÿ\n",
        "# !cp -r images /content/drive/MyDrive/"
      ],
      "metadata": {
        "id": "6Sr151RAFunJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 4. åŒ¯å…¥ã€è¨­å®šé‡‘é‘°ã€å¸¸æ•¸èˆ‡æ¨¡å‹åˆå§‹åŒ–\n",
        "import os # ä¿ç•™ os æ¨¡çµ„ï¼Œä»¥é˜²æœªä¾†å…¶ä»–åœ°æ–¹ç”¨åˆ°\n",
        "from groq import Groq\n",
        "from langchain_groq import ChatGroq\n",
        "# ç¢ºä¿åŒ¯å…¥æ¸¬è©¦å€å¡Šå¯èƒ½éœ€è¦çš„æ¨¡çµ„\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "# åŒ¯å…¥ Colab Secrets åŠŸèƒ½\n",
        "from google.colab import userdata\n",
        "\n",
        "# --- API é‡‘é‘°è¨­å®š (ä½¿ç”¨ Colab Secrets) ---\n",
        "GROQ_API_KEY = None # å…ˆåˆå§‹åŒ–ç‚º None\n",
        "try:\n",
        "    # å¾ Colab Secrets è®€å–åç‚º 'GROQ_API_KEY' çš„å¯†é‘°\n",
        "    GROQ_API_KEY = userdata.get('GROQ_API_KEY')\n",
        "\n",
        "    if not GROQ_API_KEY:\n",
        "        # å¦‚æœå¯†é‘°ä¸å­˜åœ¨ã€å€¼ç‚ºç©ºæˆ– Notebook access æœªå•Ÿç”¨ï¼Œuserdata.get æœƒè¿”å› None\n",
        "         print(\"âš ï¸ è­¦å‘Šï¼šæœªèƒ½å¾ Colab Secrets ç²å– GROQ_API_KEYã€‚\")\n",
        "         print(\"   è«‹æª¢æŸ¥ä»¥ä¸‹è¨­å®šï¼š\")\n",
        "         print(\"   1. åœ¨ Colab å·¦å´çš„é‘°åŒ™(ğŸ”‘)åœ–ç¤ºé¢æ¿ä¸­ï¼Œæ˜¯å¦å·²å»ºç«‹åç¨±å®Œå…¨ç‚º 'GROQ_API_KEY' çš„ Secretï¼Ÿ\")\n",
        "         print(\"   2. è©² Secret çš„ã€å€¼ã€(Value) æ¬„ä½æ˜¯å¦å·²å¡«å…¥æ‚¨çš„ Groq API Keyï¼Ÿ\")\n",
        "         print(\"   3. è©² Secret çš„ 'Notebook access' (å…è¨±é€™å€‹ç­†è¨˜æœ¬å­˜å–) é–‹é—œæ˜¯å¦å·²å•Ÿç”¨ (ON)ï¼Ÿ\")\n",
        "         # æ­¤è™•ä¸æ‹‹å‡ºéŒ¯èª¤ï¼Œè®“å¾ŒçºŒçš„æª¢æŸ¥ä¾†è™•ç† GROQ_API_KEY ç‚º None çš„æƒ…æ³\n",
        "\n",
        "except Exception as e:\n",
        "    # è™•ç†è®€å– Secret éç¨‹ä¸­çš„å…¶ä»–æ½›åœ¨éŒ¯èª¤ (é›–ç„¶è¼ƒå°‘è¦‹)\n",
        "    print(f\"è®€å– Colab Secret 'GROQ_API_KEY' æ™‚ç™¼ç”Ÿéé æœŸéŒ¯èª¤ï¼š{e}\")\n",
        "    GROQ_API_KEY = None # ç¢ºä¿å‡ºéŒ¯æ™‚è®Šæ•¸ç‚º None\n",
        "\n",
        "# --- å¸¸æ•¸è¨­å®š ---\n",
        "LLM_MODEL_NAME = \"llama3-70b-8192\"  # æˆ–æ˜¯æ‚¨è¦ä½¿ç”¨çš„å…¶ä»–æ¨¡å‹\n",
        "\n",
        "# --- åˆå§‹åŒ– Groq Client ---\n",
        "client = None\n",
        "if GROQ_API_KEY: # åªæœ‰åœ¨æˆåŠŸç²å– API Key å¾Œæ‰å˜—è©¦åˆå§‹åŒ–\n",
        "    try:\n",
        "        # ä½¿ç”¨ç²å–åˆ°çš„ API Key åˆå§‹åŒ– Groq client\n",
        "        client = Groq(api_key=GROQ_API_KEY)\n",
        "        print(\"âœ… Groq client åˆå§‹åŒ–æˆåŠŸã€‚\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ åˆå§‹åŒ– Groq client æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}\")\n",
        "        print(\"   è«‹æª¢æŸ¥æ‚¨çš„ API Key æ ¼å¼æ˜¯å¦æ­£ç¢ºï¼Œæˆ–æ˜¯å¦æœ‰ç¶²è·¯é€£ç·šå•é¡Œã€‚\")\n",
        "        client = None # ç¢ºä¿åˆå§‹åŒ–å¤±æ•—æ™‚ client ç‚º None\n",
        "else:\n",
        "    print(\"âŒ ç”±æ–¼æœªèƒ½ç²å– GROQ_API_KEYï¼Œç„¡æ³•å˜—è©¦åˆå§‹åŒ– Groq clientã€‚\")\n",
        "\n",
        "# --- åˆå§‹åŒ– Langchain çš„ ChatGroq æ¨¡å‹ ---\n",
        "llm = None\n",
        "# åªæœ‰åœ¨ Groq client æˆåŠŸåˆå§‹åŒ–å¾Œæ‰é€²è¡Œ (é€™ä¹Ÿé–“æ¥ç¢ºèªäº† API Key å­˜åœ¨ä¸”åˆæ­¥æœ‰æ•ˆ)\n",
        "if client:\n",
        "    try:\n",
        "        llm = ChatGroq(model=LLM_MODEL_NAME, groq_api_key=GROQ_API_KEY)\n",
        "        print(\"âœ… Langchain ChatGroq æ¨¡å‹åˆå§‹åŒ–æˆåŠŸã€‚\")\n",
        "    except Exception as e_llm:\n",
        "        print(f\"âŒ åˆå§‹åŒ– Langchain ChatGroq æ¨¡å‹æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e_llm}\")\n",
        "        llm = None # ç¢ºä¿åˆå§‹åŒ–å¤±æ•—æ™‚ llm ç‚º None\n",
        "elif not GROQ_API_KEY:\n",
        "     print(\"âŒ ç”±æ–¼æœªç²å– API Keyï¼ŒLangchain ChatGroq æ¨¡å‹ç„¡æ³•åˆå§‹åŒ–ã€‚\")\n",
        "else: # API Key å­˜åœ¨ï¼Œä½† client åˆå§‹åŒ–å¤±æ•—\n",
        "     print(\"âŒ ç”±æ–¼ Groq client åˆå§‹åŒ–å¤±æ•—ï¼ŒLangchain ChatGroq æ¨¡å‹ç„¡æ³•åˆå§‹åŒ–ã€‚\")\n",
        "\n",
        "\n",
        "# --- é™¤éŒ¯è¨Šæ¯ ---\n",
        "print(\"\\n--- åˆå§‹åŒ–ç‹€æ…‹ ---\")\n",
        "print(f\"GROQ_API_KEY è¨­å®š: {'æ˜¯' if GROQ_API_KEY else 'å¦'}\") # ç¾åœ¨åæ˜ æ˜¯å¦æˆåŠŸå¾ Secrets å–å¾—\n",
        "print(f\"Groq client åˆå§‹åŒ–: {'æˆåŠŸ' if client else 'å¤±æ•—'}\")\n",
        "print(f\"LLM æ¨¡å‹åˆå§‹åŒ–: {'æˆåŠŸ' if llm else 'å¤±æ•—'}\")\n",
        "print(f\"ä½¿ç”¨çš„æ¨¡å‹åç¨±: {LLM_MODEL_NAME}\")\n",
        "print(\"------------------\\n\")\n",
        "\n",
        "# --- ç¯„ä¾‹æ¸¬è©¦ (å¯é¸) ---\n",
        "if client and llm:\n",
        "    print(\"åŸ·è¡Œ API é€£ç·šèˆ‡ Langchain æ•´åˆæ¸¬è©¦...\")\n",
        "    try:\n",
        "        # æ¸¬è©¦ Groq API\n",
        "        response = client.chat.completions.create(\n",
        "            model=LLM_MODEL_NAME,\n",
        "            messages=[{\"role\": \"user\", \"content\": \"ä½ å¥½ï¼ç°¡å–®å›ç­”å³å¯ã€‚\"}]\n",
        "        )\n",
        "        print(\"   - Groq API é€£ç·šæ¸¬è©¦æˆåŠŸã€‚\")\n",
        "        # print(response.choices[0].message.content) # å¯å–æ¶ˆè¨»è§£æŸ¥çœ‹å›æ‡‰\n",
        "\n",
        "        # æ¸¬è©¦ Langchain æ•´åˆ\n",
        "        prompt_template = ChatPromptTemplate.from_template(\"ç”¨ä¸€å¥è©±ç¸½çµ Langchainã€‚\")\n",
        "        chain = prompt_template | llm | StrOutputParser()\n",
        "        langchain_response = chain.invoke({})\n",
        "        print(\"   - Langchain æ•´åˆæ¸¬è©¦æˆåŠŸã€‚\")\n",
        "        # print(langchain_response) # å¯å–æ¶ˆè¨»è§£æŸ¥çœ‹å›æ‡‰\n",
        "\n",
        "    except Exception as test_error:\n",
        "        print(f\"\\n--- æ¸¬è©¦å¤±æ•— ---\")\n",
        "        print(f\"   æ¸¬è©¦éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤ï¼š{test_error}\")\n",
        "        print(\"   è«‹æª¢æŸ¥æ‚¨çš„ API é‡‘é‘°æœ‰æ•ˆæ€§ã€ç¶²è·¯é€£ç·šï¼Œä»¥åŠæ¨¡å‹åç¨±æ˜¯å¦æ­£ç¢ºã€‚\")\n",
        "        print(\"------------------\\n\")\n",
        "else:\n",
        "    print(\"\\n--- ç”±æ–¼ client æˆ– llm æœªæˆåŠŸåˆå§‹åŒ–ï¼Œè·³éæ¸¬è©¦ã€‚ ---\")\n",
        "    print(\"---------------\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZVHsFchezP2",
        "outputId": "7a584bd2-e497-4f9c-d99c-5212d7ab912e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Groq client åˆå§‹åŒ–æˆåŠŸã€‚\n",
            "âœ… Langchain ChatGroq æ¨¡å‹åˆå§‹åŒ–æˆåŠŸã€‚\n",
            "\n",
            "--- åˆå§‹åŒ–ç‹€æ…‹ ---\n",
            "GROQ_API_KEY è¨­å®š: æ˜¯\n",
            "Groq client åˆå§‹åŒ–: æˆåŠŸ\n",
            "LLM æ¨¡å‹åˆå§‹åŒ–: æˆåŠŸ\n",
            "ä½¿ç”¨çš„æ¨¡å‹åç¨±: llama3-70b-8192\n",
            "------------------\n",
            "\n",
            "åŸ·è¡Œ API é€£ç·šèˆ‡ Langchain æ•´åˆæ¸¬è©¦...\n",
            "   - Groq API é€£ç·šæ¸¬è©¦æˆåŠŸã€‚\n",
            "   - Langchain æ•´åˆæ¸¬è©¦æˆåŠŸã€‚\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- å¿…è¦çš„å‡½å¼åº« ---\n",
        "print(\"æ­£åœ¨è¼‰å…¥å¿…è¦çš„å‡½å¼åº«...\")\n",
        "from flask import Flask, request, abort, send_from_directory\n",
        "from pyngrok import ngrok, conf # type: ignore\n",
        "from linebot import LineBotApi, WebhookHandler\n",
        "from linebot.exceptions import InvalidSignatureError\n",
        "from linebot.models import MessageEvent, TextMessage, TextSendMessage\n",
        "import time\n",
        "import traceback\n",
        "import os\n",
        "import re\n",
        "from google.colab import userdata # type: ignore\n",
        "# Langchain / Groq imports\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "# å˜—è©¦åŒ¯å…¥ jieba\n",
        "try:\n",
        "    import jieba # type: ignore\n",
        "    print(\"   - jieba å‡½å¼åº«è¼‰å…¥æˆåŠŸã€‚\")\n",
        "except ImportError:\n",
        "    jieba = None\n",
        "    print(\"   - è­¦å‘Š: æœªå®‰è£ jieba å‡½å¼åº«ï¼Œå°‡ä½¿ç”¨åŸºæœ¬åˆ†è©ã€‚å»ºè­° pip install jieba\")\n",
        "\n",
        "print(\"å‡½å¼åº«è¼‰å…¥å®Œæˆã€‚\")\n",
        "\n",
        "# --- å¸¸æ•¸è¨­å®š ---\n",
        "SIMPLIFIED_MD_FILENAME = \"simplified_output_by_section.md\"\n",
        "TARGET_DESCRIPTION_KEYWORDS = [\"çµå¡Š\", \"éç¯©\", \"é †åº\", \"å¸æ¿•\", \"ç¨ åº¦\", \"é»ç¨ \", \"æµå‹•æ€§\"] # ä»å¯è¼”åŠ©è­˜åˆ¥ï¼Œä½†æœå°‹ä¸»è¦é åŸæ–™\n",
        "CHINESE_STOP_WORDS = {\"çš„\", \"å’Œ\", \"èˆ‡\", \"æˆ–\", \"äº†\", \"å‘¢\", \"å—\", \"å–”\", \"å•Š\", \"é—œæ–¼\", \"æœ‰é—œ\", \"è«‹\", \"è«‹å•\", \" \", \"\"}\n",
        "ALLOWED_WORKSHEET_IDENTIFIERS = [\n",
        "    \"å·¥ä½œè¡¨: 9\", \"å·¥ä½œè¡¨: 10\"\n",
        "]\n",
        "\n",
        "# --- SOP æŸ¥è©¢ç›¸é—œå‡½å¼ ---\n",
        "\n",
        "def load_markdown_sections(filename=SIMPLIFIED_MD_FILENAME):\n",
        "    \"\"\"è®€å–ä¸¦è§£æ Markdown æª”æ¡ˆ\"\"\"\n",
        "    sections = []; print(f\"æ­£åœ¨å˜—è©¦è¼‰å…¥æª”æ¡ˆ: {filename}\")\n",
        "    try:\n",
        "        with open(filename, 'r', encoding='utf-8') as f: markdown_content = f.read()\n",
        "        print(f\"æª”æ¡ˆ {filename} è®€å–æˆåŠŸã€‚\")\n",
        "    except FileNotFoundError: print(f\"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°æª”æ¡ˆ '{filename}'ã€‚\"); return []\n",
        "    except Exception as e: print(f\"âŒ è®€å–æª”æ¡ˆ '{filename}' æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}\"); return []\n",
        "    parts = re.split(r'(## å·¥ä½œè¡¨:.*)', markdown_content)\n",
        "    for i in range(1, len(parts), 2):\n",
        "        if i + 1 < len(parts):\n",
        "            title = parts[i].strip(); content = parts[i + 1].strip().removeprefix(\"Here is the simplified content:\").strip().removeprefix(\"Here is the simplified output:\").strip()\n",
        "            if title and content: sections.append({\"title\": title, \"content\": content})\n",
        "        elif i < len(parts):\n",
        "            title = parts[i].strip()\n",
        "            if title: sections.append({\"title\": title, \"content\": \"\"})\n",
        "    if not sections: print(f\"âš ï¸ è­¦å‘Šï¼šæœªèƒ½å¾æª”æ¡ˆ '{filename}' è§£æå‡ºä»»ä½•å·¥ä½œè¡¨å€å¡Šã€‚\")\n",
        "    return sections\n",
        "\n",
        "def filter_sections_by_title(all_sections, allowed_identifiers=ALLOWED_WORKSHEET_IDENTIFIERS):\n",
        "    \"\"\"æ ¹æ“šæ¨™é¡Œé—œéµå­—éæ¿¾å€å¡Šåˆ—è¡¨\"\"\"\n",
        "    if not all_sections: return []\n",
        "    return [sec for sec in all_sections if any(allowed_id in sec.get(\"title\", \"\") for allowed_id in allowed_identifiers)]\n",
        "\n",
        "# --- *** ä¿®æ”¹ï¼šåŸºæ–¼è¦å‰‡çš„é—œéµå­—æå–å‡½å¼ (æ›´å´é‡åŸæ–™) *** ---\n",
        "def extract_keywords_rule_based(user_input, target_keywords=TARGET_DESCRIPTION_KEYWORDS):\n",
        "    \"\"\"ä½¿ç”¨è¦å‰‡æ‹†åˆ†å’Œæ¯”å°é—œéµå­—ï¼Œä¸»è¦æå–åŸæ–™åç¨±ã€‚\"\"\"\n",
        "    print(f\"--- (éšæ®µ0) ä½¿ç”¨è¦å‰‡è§£æè¼¸å…¥ (ä¸»è¦æå–åŸæ–™): '{user_input}' ---\")\n",
        "    input_cleaned = user_input.strip().lower()\n",
        "    if not input_cleaned:\n",
        "        return None\n",
        "\n",
        "    if jieba:\n",
        "        tokens = list(jieba.cut_for_search(input_cleaned))\n",
        "    else:\n",
        "        input_no_punct = re.sub(r'[^\\w\\s]', ' ', input_cleaned)\n",
        "        tokens = input_no_punct.split()\n",
        "    print(f\"   åˆ†è©çµæœ: {tokens}\")\n",
        "\n",
        "    potential_materials = []\n",
        "    identified_characteristics = set() # ä»ç„¶è¨˜éŒ„ç‰¹æ€§è©ï¼Œä½†å®ƒå€‘ä¸ä½œç‚ºä¸»è¦æœå°‹ä¾æ“š\n",
        "\n",
        "    for token in tokens:\n",
        "        token_clean = token.strip()\n",
        "        if not token_clean or token_clean in CHINESE_STOP_WORDS:\n",
        "            continue\n",
        "\n",
        "        is_characteristic = False\n",
        "        # æª¢æŸ¥æ˜¯å¦ç‚ºç‰¹æ€§é—œéµå­— (å®Œå…¨åŒ¹é…)\n",
        "        for target_char in target_keywords:\n",
        "            if token_clean == target_char.lower():\n",
        "                identified_characteristics.add(target_char) # è¨˜éŒ„åŸå§‹å¤§å°å¯«çš„ç‰¹æ€§é—œéµå­—\n",
        "                is_characteristic = True\n",
        "                break\n",
        "\n",
        "        # å¦‚æœä¸æ˜¯ç‰¹æ€§è©ï¼Œä¸”ä¸æ˜¯æ•¸å­—ï¼Œä¸”é•·åº¦å¤§æ–¼0ï¼Œå‰‡è¦–ç‚ºæ½›åœ¨åŸæ–™\n",
        "        if not is_characteristic:\n",
        "            if not token_clean.isnumeric() and len(token_clean) > 0:\n",
        "                potential_materials.append(token_clean)\n",
        "\n",
        "    materials_list_unique = sorted(list(set(potential_materials)))\n",
        "    characteristics_list_sorted = sorted(list(identified_characteristics))\n",
        "\n",
        "    if not materials_list_unique:\n",
        "        print(\"âš ï¸ è¦å‰‡è§£æå™¨ï¼šæœªèƒ½è­˜åˆ¥å‡ºä»»ä½•æ½›åœ¨çš„åŸæ–™åç¨±ã€‚\")\n",
        "        # å³ä½¿æ²’æœ‰åŸæ–™ï¼Œä¹Ÿè¿”å›çµæ§‹ï¼Œè®“å¾ŒçºŒåˆ¤æ–·\n",
        "        # return {\"åŸæ–™åç¨±\": [], \"ç‰¹æ€§æè¿°\": characteristics_list_sorted}\n",
        "        return None # æˆ–è€…ç›´æ¥è¿”å›Noneï¼Œå¦‚æœåš´æ ¼è¦æ±‚å¿…é ˆæœ‰åŸæ–™åæ‰èƒ½æŸ¥è©¢\n",
        "\n",
        "    result = {\"åŸæ–™åç¨±\": materials_list_unique, \"ç‰¹æ€§æè¿°\": characteristics_list_sorted}\n",
        "    print(f\"   è¦å‰‡è§£æçµæœ (ä¸»è¦ç‚ºåŸæ–™ï¼Œè¼”åŠ©ç‰¹æ€§): {result}\")\n",
        "    return result\n",
        "# --- *** å‡½å¼çµæŸ *** ---\n",
        "\n",
        "def search_sections(sections_to_search, keywords):\n",
        "    \"\"\"åœ¨å·²éæ¿¾çš„å€å¡Šä¸­åˆæ­¥ç¯©é¸åŒ…å«ã€åŸæ–™åç¨±ã€‘é—œéµå­—çš„å·¥ä½œè¡¨\"\"\"\n",
        "    relevant_sections = []\n",
        "    # *** ä¿®æ”¹ï¼šåªä½¿ç”¨åŸæ–™åç¨±é€²è¡Œåˆæ­¥æœå°‹ ***\n",
        "    material_keywords = keywords.get(\"åŸæ–™åç¨±\", [])\n",
        "    all_keywords = [kw for kw in material_keywords if kw and isinstance(kw, str)]\n",
        "\n",
        "    if not all_keywords:\n",
        "        print(\"â„¹ï¸ æ²’æœ‰æœ‰æ•ˆçš„ã€åŸæ–™åç¨±ã€‘é—œéµå­—å¯ä¾›æœå°‹ã€‚\")\n",
        "        return []\n",
        "    print(f\"DEBUG: æ­£åœ¨ä½¿ç”¨ä»¥ä¸‹ä»»ä¸€ã€åŸæ–™åç¨±ã€‘é—œéµå­—æœå°‹: {all_keywords}\")\n",
        "    for section in sections_to_search:\n",
        "        text_to_search = section.get(\"title\", \"\") + section.get(\"content\", \"\")\n",
        "        if any(keyword.lower() in text_to_search.lower() for keyword in all_keywords): # å¿½ç•¥å¤§å°å¯«æœå°‹\n",
        "            relevant_sections.append(section)\n",
        "    return relevant_sections\n",
        "\n",
        "# --- *** ä¿®æ”¹ï¼šç¬¬ä¸€éšæ®µ LLM Promptï¼Œå¼·èª¿å°ç¯„åœã€åƒ…åŸæ–™ç›¸é—œ *** ---\n",
        "def extract_relevant_text(llm, sections, keywords):\n",
        "    \"\"\"(ç¬¬ä¸€éšæ®µ LLM) ä½¿ç”¨æ¥µåº¦èšç„¦çš„ Prompt æå–èˆ‡æŒ‡å®šã€åŸæ–™åç¨±ã€‘æœ€ç›´æ¥ç›¸é—œçš„ã€å°ç¯„åœæ–‡å­—ç‰‡æ®µã€‘ã€‚\"\"\"\n",
        "    material_names = keywords.get('åŸæ–™åç¨±', [])\n",
        "    if not material_names:\n",
        "        print(\"â„¹ï¸ extract_relevant_text: æŸ¥è©¢ä¸­æœªæä¾›åŸæ–™åç¨±ã€‚\")\n",
        "        return []\n",
        "    material_name_str = \"ã€\".join(material_names)\n",
        "\n",
        "    # ç‰¹æ€§æè¿°å¯ä»¥ä½œç‚ºè¼”åŠ©ä¸Šä¸‹æ–‡ï¼Œå¹«åŠ©LLMç†è§£ï¼Œä½†æå–ç„¦é»æ˜¯åŸæ–™\n",
        "    characteristics_list = keywords.get('ç‰¹æ€§æè¿°', [])\n",
        "    description_keywords_str = ', '.join(characteristics_list)\n",
        "\n",
        "    if characteristics_list:\n",
        "        # é›–ç„¶ä¸»è¦æœåŸæ–™ï¼Œä½†å‘ŠçŸ¥LLMä½¿ç”¨è€…ä¹Ÿæåˆ°äº†é€™äº›ç‰¹æ€§ï¼Œå¯èƒ½æœ‰åŠ©æ–¼LLMç†è§£ä¸Šä¸‹æ–‡\n",
        "        keywords_str_display_for_prompt = f\"ä¸»è¦æŸ¥è©¢çš„åŸæ–™åç¨±ï¼šã€{material_name_str}ã€‘ã€‚(ä½¿ç”¨è€…åŒæ™‚æåŠäº†é€™äº›ç›¸é—œè©å½™ï¼Œä¾›åƒè€ƒï¼š{description_keywords_str})\"\n",
        "    else:\n",
        "        keywords_str_display_for_prompt = f\"ä¸»è¦æŸ¥è©¢çš„åŸæ–™åç¨±ï¼šã€{material_name_str}ã€‘\"\n",
        "\n",
        "    prompt_base = \"\"\"\n",
        "        ä½ çš„ä»»å‹™æ˜¯ä½œç‚ºä¸€å€‹ç²¾ç¢ºçš„æ–‡å­—æå–å™¨ã€‚\n",
        "        åœ¨ä¸‹æ–¹æä¾›çš„ã€Œå·¥ä½œè¡¨å…§å®¹ã€ä¸­ï¼Œåƒ…æ‰¾å‡ºèˆ‡ã€Œä¸»è¦æŸ¥è©¢çš„åŸæ–™åç¨±ã€æœ€ç›´æ¥ç›¸é—œçš„ã€ä¸€å€‹æˆ–å¤šå€‹ç°¡çŸ­æ–‡å­—ç‰‡æ®µã€å¥å­æˆ–åˆ—è¡¨é …ã€‘ã€‚\n",
        "\n",
        "        {keywords_str_display_for_prompt}\n",
        "\n",
        "        å·¥ä½œè¡¨å…§å®¹ï¼š\n",
        "        ```markdown\n",
        "        {text}\n",
        "        ```\n",
        "\n",
        "        è¼¸å‡ºè¦æ±‚ï¼š\n",
        "        1.  ç²¾ç¢ºå®šä½åˆ°åŒ…å«ã€Œä¸»è¦æŸ¥è©¢çš„åŸæ–™åç¨±ã€çš„å¥å­ã€æ“ä½œæ­¥é©Ÿæˆ–å…¶éå¸¸ç·Šå¯†çš„ä¸Šä¸‹æ–‡ã€‚\n",
        "        2.  **ã€ç›´æ¥è¼¸å‡ºã€‘** æ‰¾åˆ°çš„ç›¸é—œå…§å®¹ã€‚æå–çš„ç¯„åœæ‡‰ç›¡å¯èƒ½å°è€Œç²¾æº–ã€‚ä¾‹å¦‚ï¼Œåƒ…æå–åŒ…å«è©²åŸæ–™åç¨±çš„é‚£å€‹æ­¥é©Ÿã€é‚£å¥æè¿°æˆ–ç›¸é—œçš„åˆ—è¡¨é …ã€‚\n",
        "        3.  **ã€é¿å…æå–ã€‘** éé•·çš„æ®µè½æˆ–æ•´å€‹å­æ¨™é¡Œä¸‹çš„æ‰€æœ‰å…§å®¹ï¼Œé™¤éè©²åŸæ–™åç¨±åœ¨æ•´å€‹æ®µè½/å­æ¨™é¡Œä¸­éƒ½é«˜åº¦ç›¸é—œä¸”å…§å®¹ä¸å¯åˆ†å‰²ã€‚å„ªå…ˆæå–æœ€å°çš„æœ‰æ•ˆè³‡è¨Šå–®ä½ã€‚\n",
        "        4.  **ã€çµ•å°ç¦æ­¢ã€‘** åŒ…å« '## å·¥ä½œè¡¨: ...' é€™å€‹æœ€é«˜ç´šåˆ¥çš„æ¨™é¡Œã€‚\n",
        "        5.  **ã€çµ•å°ç¦æ­¢ã€‘** åŒ…å«ä»»ä½• 'è£½è¡¨æ—¥æœŸ', 'è£½è¡¨äºº' ç­‰é è…³è³‡è¨Šã€‚\n",
        "        6.  **ã€çµ•å°ç¦æ­¢ã€‘** é€²è¡Œä»»ä½•æ‘˜è¦ã€æ”¹å¯«æˆ–æ·»åŠ ä»»ä½•ã€è§£é‡‹æ€§æ–‡å­—ã€å‰è¨€ã€çµèªã€‘(ä¾‹å¦‚ä¸è¦èªª \"ä»¥ä¸‹æ˜¯æ‰¾åˆ°çš„å…§å®¹ï¼š\" æˆ– \"æ ¹æ“šé—œéµå­—...\" ç­‰)ã€‚è¼¸å‡ºçµæœå¿…é ˆç›´æ¥å°±æ˜¯æå–çš„å…§å®¹æœ¬èº«ã€‚\n",
        "        7.  å¦‚æœåœ¨æ­¤ã€Œå·¥ä½œè¡¨å…§å®¹ã€ä¸­æ‰¾ä¸åˆ°ç›´æ¥èˆ‡ã€Œä¸»è¦æŸ¥è©¢çš„åŸæ–™åç¨±ã€ç›¸é—œçš„å…§å®¹ï¼Œã€åªè¼¸å‡ºã€‘ä»¥ä¸‹å›ºå®šæ–‡å­—ï¼šã€Œåœ¨æ­¤æ–‡ä»¶ä¸­æœªæ‰¾åˆ°èˆ‡æ­¤åŸæ–™ç›´æ¥ç›¸é—œçš„å…§å®¹ã€ã€‚\n",
        "        8.  å¿…é ˆä½¿ç”¨ã€ç¹é«”ä¸­æ–‡ã€‘è¼¸å‡º (å¦‚æœæå–çš„å…§å®¹æœ¬èº«æ˜¯ä¸­æ–‡)ã€‚\n",
        "        å†æ¬¡å¼·èª¿ï¼šç›´æ¥è¼¸å‡ºæå–çš„ã€èˆ‡åŸæ–™ç›´æ¥ç›¸é—œçš„ã€å°ç¯„åœã€‘å…§å®¹ï¼Œä¸è¦åŠ ä»»ä½•å…¶ä»–æ–‡å­—ã€‚\n",
        "        \"\"\"\n",
        "\n",
        "    # ä½¿ç”¨å¸¶æœ‰æ‰€æœ‰é æœŸè¼¸å…¥è®Šé‡çš„æ¨¡æ¿\n",
        "    prompt_template = ChatPromptTemplate.from_template(prompt_base)\n",
        "    chain = prompt_template | llm | StrOutputParser()\n",
        "\n",
        "    relevant_texts = []\n",
        "    # é¡¯ç¤ºä¸€æ¬¡å®Œæ•´çš„ keywords_str_display_for_promptï¼Œå› ç‚ºå®ƒå°æ–¼æ‰€æœ‰ section éƒ½æ˜¯ä¸€æ¨£çš„\n",
        "    print(f\"--- (éšæ®µ1) ä½¿ç”¨ LLM å®šä½ä¸¦æå–å­ç« ç¯€ï¼Œä½¿ç”¨çš„æŸ¥è©¢ä¸Šä¸‹æ–‡: {keywords_str_display_for_prompt} ---\")\n",
        "\n",
        "    for section in sections:\n",
        "        section_title = section.get(\"title\", \"ç„¡æ¨™é¡Œå€å¡Š\")\n",
        "        section_content = section.get(\"content\", \"\")\n",
        "        section_full_text = f\"{section_content}\" # ç°¡åŒ–ï¼Œåªå‚³éå…§å®¹è®“ LLM å°ˆæ³¨\n",
        "\n",
        "        print(f\"   æ­£åœ¨è™•ç†å€å¡Š: {section_title} (æœå°‹åŸæ–™ '{material_name_str}')...\")\n",
        "        try:\n",
        "            relevant_text = chain.invoke({\n",
        "                \"keywords_str_display_for_prompt\": keywords_str_display_for_prompt,\n",
        "                \"text\": section_full_text\n",
        "            })\n",
        "            relevant_text = relevant_text.strip()\n",
        "\n",
        "            # åŸºæœ¬çš„å¾Œè™•ç†ï¼Œç§»é™¤ LLM å¯èƒ½ä¸å°å¿ƒåŠ ä¸Šçš„ markdown æ¨™ç±¤æˆ–å¼•å°è©\n",
        "            if relevant_text.startswith(\"æ ¹æ“šé—œéµå­—\") or relevant_text.startswith(\"ä»¥ä¸‹æ˜¯æ‰¾åˆ°\"):\n",
        "                lines = relevant_text.splitlines()\n",
        "                relevant_text = \"\\n\".join(lines[1:]).strip()\n",
        "            if relevant_text.startswith(\"```markdown\"):\n",
        "                relevant_text = relevant_text.removeprefix(\"```markdown\").removesuffix(\"```\").strip()\n",
        "\n",
        "            is_found_message = \"åœ¨æ­¤æ–‡ä»¶ä¸­æœªæ‰¾åˆ°èˆ‡æ­¤åŸæ–™ç›´æ¥ç›¸é—œçš„å…§å®¹\" in relevant_text\n",
        "\n",
        "            if is_found_message or not relevant_text: # å¦‚æœæ˜¯ \"æœªæ‰¾åˆ°\" æˆ–æå–çµæœç‚ºç©º\n",
        "                print(f\"     â†³ åœ¨å€å¡Š '{section_title}' ä¸­æœªæ‰¾åˆ°æˆ–æå–åˆ°ç©ºå…§å®¹é—œæ–¼åŸæ–™ '{material_name_str}'ã€‚LLMå›æ‡‰: '{relevant_text}'\")\n",
        "                relevant_texts.append({\"title\": section_title, \"text\": relevant_text if relevant_text else \"å…§å®¹æå–ç‚ºç©º\", \"found\": False})\n",
        "            else:\n",
        "                print(f\"     â†³ å¾ '{section_title}' æå–åˆ°å…§å®¹ (å‰100å­—): \\\"{relevant_text[:100].replace(os.linesep, ' ')}...\\\"\")\n",
        "                relevant_texts.append({\"title\": section_title, \"text\": relevant_text, \"found\": True})\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ å¾å€å¡Š '{section_title}' æå–æ™‚å‡ºéŒ¯: {e}\")\n",
        "            traceback.print_exc()\n",
        "            relevant_texts.append({\"title\": section_title, \"text\": \"LLM æå–å¤±æ•—\", \"found\": False})\n",
        "    return relevant_texts\n",
        "# --- *** å‡½å¼çµæŸ *** ---\n",
        "\n",
        "# --- *** ä¿®æ”¹ï¼šç¬¬äºŒéšæ®µ LLM Promptï¼Œå¼·èª¿åŸæ–‡å‘ˆç¾ã€ä¸ä¿®é£¾ã€çµ±ä¸€åˆ—è¡¨ *** ---\n",
        "def synthesize_results(llm, keywords, extracted_texts):\n",
        "    \"\"\"(ç¬¬äºŒéšæ®µ LLM) å°‡æå–å‡ºçš„ã€å°ç¯„åœæ–‡å­—ç‰‡æ®µã€‘æ•´åˆæˆã€æ¥µç°¡ä¸”åŸæ–‡å‘ˆç¾çš„çµ±ä¸€æ ¼å¼åˆ—è¡¨ã€‘ã€‚\"\"\"\n",
        "    if not extracted_texts:\n",
        "        return \"æœªèƒ½æå–åˆ°ä»»ä½•ç›¸é—œå…§å®¹ä»¥ä¾›æ•´åˆã€‚\"\n",
        "\n",
        "    valid_extractions = [\n",
        "        item['text'] for item in extracted_texts\n",
        "        if item.get(\"found\", False) and \\\n",
        "           \"LLM æå–å¤±æ•—\" not in item['text'] and \\\n",
        "           \"åœ¨æ­¤æ–‡ä»¶ä¸­æœªæ‰¾åˆ°èˆ‡æ­¤åŸæ–™ç›´æ¥ç›¸é—œçš„å…§å®¹\" not in item['text'] and \\\n",
        "           item['text'] and item['text'].strip() # ç¢ºä¿æ–‡æœ¬æœ¬èº«ä¸ç‚ºç©ºæˆ–åƒ…åŒ…å«ç©ºç™½\n",
        "    ]\n",
        "\n",
        "    if not valid_extractions:\n",
        "        material_names_list = keywords.get('åŸæ–™åç¨±', [])\n",
        "        material_name_str = \"ã€\".join(material_names_list) if material_names_list else \"æ‰€æŸ¥è©¢çš„é …ç›®\"\n",
        "        # æª¢æŸ¥æ˜¯å¦æ‰€æœ‰å˜—è©¦æå–çš„å€å¡Šéƒ½è¿”å›äº† \"æœªæ‰¾åˆ°\" æˆ–é¡ä¼¼è¨Šæ¯\n",
        "        all_attempted_but_not_found = all(\n",
        "            not item.get(\"found\") or \\\n",
        "            \"åœ¨æ­¤æ–‡ä»¶ä¸­æœªæ‰¾åˆ°èˆ‡æ­¤åŸæ–™ç›´æ¥ç›¸é—œçš„å…§å®¹\" in item['text'] or \\\n",
        "            item['text'].strip() == \"\"\n",
        "            for item in extracted_texts\n",
        "        )\n",
        "        if extracted_texts and all_attempted_but_not_found: # ç¢ºä¿ extracted_texts ä¸æ˜¯ç©ºçš„\n",
        "            return f\"å·²æª¢æŸ¥æ‰€æœ‰ç›¸é—œSOPæ–‡ä»¶å€å¡Šï¼Œä½†å‡æœªæ‰¾åˆ°é—œæ–¼åŸæ–™ã€{material_name_str}ã€‘çš„ç›´æ¥æ“ä½œèªªæ˜æˆ–æ³¨æ„äº‹é …ã€‚\"\n",
        "        return f\"é›–ç„¶åˆæ­¥å®šä½åˆ°å¯èƒ½ç›¸é—œçš„SOPå€å¡Šï¼Œä½†åœ¨å…¶ä¸­æœªèƒ½æ‰¾åˆ°é—œæ–¼åŸæ–™ã€{material_name_str}ã€‘çš„å…·é«”æ“ä½œèªªæ˜æˆ–æ³¨æ„äº‹é …ã€‚\"\n",
        "\n",
        "\n",
        "    print(f\"\\nğŸ”„ (éšæ®µ2) æ­£åœ¨æ•´åˆ {len(valid_extractions)} ä»½æå–çš„é‡é»å…§å®¹ä¸¦çµ±ä¸€æ ¼å¼ (åŠ›æ±‚åŸæ–‡ã€ç°¡æ½”)...\")\n",
        "    # å°‡æ‰€æœ‰æå–ç‰‡æ®µçµ„åˆï¼Œç”¨ç‰¹æ®Šåˆ†éš”ç¬¦æ˜ç¢ºå‘ŠçŸ¥LLMé€™æ˜¯ä¸åŒä¾†æºçš„ç¨ç«‹ç‰‡æ®µ\n",
        "    combined_extracted_text = \"\\n\\nç‰‡æ®µåˆ†éš”ç·š (è«‹å°‡æ¯å€‹ç‰‡æ®µè¦–ç‚ºç¨ç«‹è³‡è¨Šä¾†æº)\\n\\n\".join(valid_extractions)\n",
        "\n",
        "    material_names_list = keywords.get('åŸæ–™åç¨±', [])\n",
        "    material_name = \"ã€\".join(material_names_list) if material_names_list else \"æœªæŒ‡å®šåŸæ–™\"\n",
        "\n",
        "    characteristics_list = keywords.get('ç‰¹æ€§æè¿°', [])\n",
        "    if characteristics_list:\n",
        "        keywords_str_for_prompt = f\"ä½¿ç”¨è€…ä¸»è¦æŸ¥è©¢çš„åŸæ–™åç¨±ç‚ºã€{material_name}ã€‘ã€‚(ä½¿ç”¨è€…æŸ¥è©¢æ™‚æåŠçš„ç›¸é—œè©å½™ï¼Œä¾›æ‚¨ç†è§£ä¸Šä¸‹æ–‡ï¼š{', '.join(characteristics_list)})\"\n",
        "    else:\n",
        "        keywords_str_for_prompt = f\"ä½¿ç”¨è€…ä¸»è¦æŸ¥è©¢çš„åŸæ–™åç¨±ç‚ºã€{material_name}ã€‘ã€‚\"\n",
        "\n",
        "    synthesis_prompt_template_str = \"\"\"\n",
        "        æ‚¨æ˜¯ä¸€ä½SOPå…§å®¹æ•´ç†å“¡ã€‚æ‚¨çš„ä»»å‹™æ˜¯å°‡ä¸‹æ–¹æä¾›çš„ã€å·²å¾SOPæ–‡ä»¶ä¸­æå–å‡ºçš„ã€èˆ‡æŒ‡å®šåŸæ–™ç›¸é—œçš„ã€å¤šå€‹ç¨ç«‹çš„ç°¡çŸ­æ–‡å­—ç‰‡æ®µã€‘ï¼Œæ•´ç†æˆä¸€å€‹ã€æ¥µç°¡çš„ã€çµ±ä¸€æ ¼å¼çš„æ•¸å­—ç·¨è™Ÿåˆ—è¡¨ã€‘ã€‚\n",
        "\n",
        "        {keywords_str_for_prompt}\n",
        "\n",
        "        å·²æå–çš„ç›¸é—œSOPç‰‡æ®µ (é€™äº›ç‰‡æ®µä¾†è‡ªä¸åŒåœ°æ–¹ï¼Œè«‹å°‡å®ƒå€‘è¦–ç‚ºç¨ç«‹çš„è³‡è¨Šé»ï¼Œä½¿ç”¨ã€Œç‰‡æ®µåˆ†éš”ç·šã€éš”é–‹)ï¼š\n",
        "        ---\n",
        "        {combined_extracted_text}\n",
        "        ---\n",
        "\n",
        "        æ‚¨çš„ä»»å‹™èˆ‡è¼¸å‡ºè¦æ±‚ï¼š\n",
        "        1.  ä»”ç´°é–±è®€æ‰€æœ‰ã€Œå·²æå–çš„ç›¸é—œSOPç‰‡æ®µã€ã€‚æ¯å€‹ç‰‡æ®µéƒ½æ˜¯é—œæ–¼ä¸Šè¿°ã€Œä¸»è¦æŸ¥è©¢çš„åŸæ–™åç¨±ã€çš„ç›´æ¥ç›¸é—œå…§å®¹ã€‚\n",
        "        2.  **ã€æ ¸å¿ƒä»»å‹™ã€‘ï¼šå°‡é€™äº›ç‰‡æ®µä¸­çš„ã€æ¯ä¸€å€‹ç¨ç«‹çš„è³‡è¨Šé»ã€æ“ä½œæ­¥é©Ÿã€æˆ–æ³¨æ„äº‹é …ã€‘æ•´ç†å‡ºä¾†ï¼Œä½œç‚ºåˆ—è¡¨ä¸­çš„ä¸€å€‹ç¨ç«‹é …ç›®ã€‚**\n",
        "        3.  **ã€æ ¼å¼çµ±ä¸€ã€‘ï¼š** ä½¿ç”¨å¾ 1 é–‹å§‹çš„æ•¸å­—ç·¨è™Ÿåˆ—è¡¨ (æ ¼å¼å¦‚ï¼š1., 2., 3., ...)ã€‚\n",
        "        4.  **ã€åŸæ–‡å‘ˆç¾ã€‘ï¼š** ç›¡æœ€å¤§å¯èƒ½ã€ç›´æ¥ä½¿ç”¨ã€‘æå–ç‰‡æ®µä¸­çš„ã€åŸæ–‡è¡¨è¿°ã€‘ã€‚**ã€åš´æ ¼ç¦æ­¢ã€‘ä»»ä½•å½¢å¼çš„æ”¹å¯«ã€æ‘˜è¦ã€è§£é‡‹ã€æ­¸ç´æˆ–æ·»åŠ æ‚¨è‡ªå·±çš„æ–‡å­—æˆ–è©•è«–ã€‚** æ‚¨çš„å·¥ä½œæ˜¯ã€Œå½™æ•´åŸæ–‡ã€å’Œã€Œæ ¼å¼åŒ–ç‚ºåˆ—è¡¨ã€ï¼Œè€Œä¸æ˜¯ã€Œå†å‰µä½œã€æˆ–ã€Œè§£é‡‹ã€ã€‚\n",
        "        5.  å¦‚æœåŸå§‹ç‰‡æ®µæœ¬èº«å°±æ˜¯ä¸€å€‹çŸ­åˆ—è¡¨æˆ–åŒ…å«å­æ­¥é©Ÿï¼Œè«‹åœ¨æ–°çš„æ•¸å­—ç·¨è™Ÿä¸‹ï¼Œç›¡å¯èƒ½ä¿æŒå…¶åŸå§‹çµæ§‹ï¼Œä¾‹å¦‚ä½¿ç”¨ç¸®æ’å’Œ '-' æˆ– '*' ä¾†è¡¨ç¤ºå±¤ç´šé—œä¿‚ã€‚\n",
        "        6.  **ã€æ¥µç°¡è¼¸å‡ºã€‘ï¼š** æ‚¨çš„æœ€çµ‚è¼¸å‡ºã€å¿…é ˆç›´æ¥æ˜¯é€™å€‹æ•¸å­—ç·¨è™Ÿåˆ—è¡¨æœ¬èº«ã€‘ã€‚**ã€åš´æ ¼ç¦æ­¢ã€‘** åŒ…å«ä»»ä½•å‰è¨€ï¼ˆä¾‹å¦‚ \"å¥½çš„ï¼Œé€™æ˜¯æ•´ç†å¾Œçš„åˆ—è¡¨ï¼š\" æˆ– \"é—œæ–¼æ‚¨æŸ¥è©¢çš„...\"ï¼‰ã€æ¨™é¡Œã€å¼•å°èªæˆ–çµèªã€‚\n",
        "        7.  å¦‚æœå¤šå€‹ç‰‡æ®µæè¿°çš„æ˜¯ã€å®Œå…¨ç›¸åŒæˆ–å¹¾ä¹å®Œå…¨é‡è¤‡ã€‘çš„è³‡è¨Šï¼Œè«‹åªé¸æ“‡å…¶ä¸­ä¸€å€‹æœ€æ¸…æ™°çš„è¡¨è¿°æ”¾å…¥åˆ—è¡¨ï¼Œä»¥é¿å…ä¸å¿…è¦çš„å†—é¤˜ã€‚ä½†è‹¥æœ‰äº›å¾®å·®ç•°æˆ–è£œå……ï¼Œå¯§å¯ä¿ç•™å…©è€…ï¼Œé¿å…è³‡è¨Šéºå¤±ã€‚**åˆ¤æ–·é‡è¤‡çš„æ¨™æº–è¦éå¸¸åš´æ ¼ã€‚**\n",
        "        8.  ä½¿ç”¨**ç¹é«”ä¸­æ–‡**ã€‚\n",
        "        9.  å³ä½¿æœ€çµ‚æ•´ç†å‡ºä¾†çš„è³‡è¨Šé»å¾ˆå°‘ï¼ˆä¾‹å¦‚åªæœ‰ä¸€å…©æ¢ï¼‰ï¼Œä¹Ÿç›´æ¥æŒ‰åˆ—è¡¨æ ¼å¼è¼¸å‡ºã€‚\n",
        "        10. **ã€çµ•å°ç¦æ­¢ã€‘** åœ¨è¼¸å‡ºä¸­æåŠä»»ä½•ã€Œå·¥ä½œè¡¨æ¨™é¡Œã€ã€ã€ŒSOPæ–‡ä»¶ä¾†æºã€æˆ–ã€Œç‰‡æ®µåˆ†éš”ç·šã€ç­‰å…ƒä¿¡æ¯ã€‚\n",
        "        11. **ã€çµ•å°ç¦æ­¢ã€‘** æ·»åŠ ä»»ä½•è¶…å‡ºæ‰€æä¾›ç‰‡æ®µå…§å®¹ä¹‹å¤–çš„è³‡è¨Šæˆ–å»ºè­°ã€‚\n",
        "\n",
        "        è«‹ç›´æ¥é–‹å§‹è¼¸å‡ºåˆ—è¡¨ï¼š\n",
        "        \"\"\"\n",
        "    synthesis_prompt_template = ChatPromptTemplate.from_template(synthesis_prompt_template_str)\n",
        "    synthesis_chain = synthesis_prompt_template | llm | StrOutputParser()\n",
        "\n",
        "    try:\n",
        "        final_response = synthesis_chain.invoke({\n",
        "            \"keywords_str_for_prompt\": keywords_str_for_prompt,\n",
        "            \"combined_extracted_text\": combined_extracted_text\n",
        "        })\n",
        "        final_response = final_response.strip()\n",
        "\n",
        "        # å¾Œè™•ç†ï¼šå¼·åŠ›ç§»é™¤å·²çŸ¥çš„å‰ç¶´ (LLMæœ‰æ™‚é‚„æ˜¯æœƒå¿ä¸ä½åŠ ä¸Š)\n",
        "        unwanted_prefixes = [\n",
        "            \"å¥½çš„ï¼Œé€™æ˜¯æ•´ç†å¾Œçš„åˆ—è¡¨ï¼š\", \"é€™æ˜¯ç‚ºæ‚¨æ•´ç†çš„åˆ—è¡¨ï¼š\", \"ä»¥ä¸‹æ˜¯æ•´ç†å¾Œçš„åˆ—è¡¨ï¼š\",\n",
        "            \"æ ¹æ“šæ‚¨æä¾›çš„è³‡è¨Šï¼š\", \"é—œæ–¼æ‚¨æŸ¥è©¢çš„åŸæ–™\", \"é—œæ–¼æ‚¨æŸ¥è©¢çš„\",\n",
        "            \"Okay, here is the list:\", \"Here is the list:\", \"Here is the output:\",\n",
        "            \"é€™æ˜¯æ‚¨æŸ¥è©¢çš„çµæœï¼š\", \"é€™æ˜¯ç›¸é—œçš„è³‡è¨Šï¼š\", \"æŸ¥è©¢çµæœå¦‚ä¸‹ï¼š\",\n",
        "            \"1. \" # æœ‰æ™‚LLMæœƒè‡ªå·±åŠ åˆ—è¡¨ç·¨è™Ÿï¼Œä½†æˆ‘å€‘çš„promptè¦æ±‚å®ƒå¾1é–‹å§‹ï¼Œæ‰€ä»¥å¦‚æœå®ƒä»¥ \"1. \" é–‹é ­ï¼Œé€šå¸¸æ˜¯æ­£ç¢ºçš„\n",
        "        ]\n",
        "        # ç§»é™¤å‰ç¶´çš„é‚è¼¯éœ€è¦å°å¿ƒï¼Œé¿å…èª¤åˆªæ­£å¸¸çš„åˆ—è¡¨é–‹é ­\n",
        "        cleaned_response = final_response\n",
        "        for prefix in unwanted_prefixes:\n",
        "            # ç¢ºä¿ prefix ä¸æ˜¯ \"1. \" é€™ç¨®å¯èƒ½èˆ‡æ­£ç¢ºè¼¸å‡ºè¡çªçš„\n",
        "            if prefix.strip().startswith(\"1.\") and cleaned_response.strip().startswith(\"1.\"):\n",
        "                continue\n",
        "            if final_response.lower().startswith(prefix.lower()):\n",
        "                # è¨ˆç®—å‰ç¶´å¯¦éš›é•·åº¦ï¼Œç§»é™¤æ­¤å‰ç¶´åŠå…¶å¾Œçš„ç©ºç™½å’Œå†’è™Ÿ\n",
        "                prefix_len = len(prefix)\n",
        "                temp_response = final_response[prefix_len:].lstrip(\"ï¼š: \").strip()\n",
        "                # åªæœ‰ç•¶ç§»é™¤å¾Œé‚„æœ‰å…§å®¹ï¼Œæˆ–è€…ç§»é™¤çš„æ˜¯æ˜ç¢ºçš„å¼•å°èªæ™‚æ‰æ›´æ–°\n",
        "                if temp_response or prefix.lower() not in [\"1.\"]: # é¿å… \"1. \" è¢«éŒ¯èª¤ç§»é™¤å¾Œè®Šç©º\n",
        "                    cleaned_response = temp_response\n",
        "                    print(f\"   DEBUG: (Synthesize) ç§»é™¤äº†ä¸éœ€è¦çš„å‰ç¶´ '{prefix}'\")\n",
        "                    break # æ‰¾åˆ°ä¸¦ç§»é™¤ä¸€å€‹å°±ä¸å†æª¢æŸ¥å…¶ä»–å‰ç¶´\n",
        "\n",
        "        # å†ä¸€æ¬¡æª¢æŸ¥æ˜¯å¦ä»¥æ•¸å­—åˆ—è¡¨é–‹é ­ï¼Œå¦‚æœä¸æ˜¯ï¼Œä¸”å…§å®¹çœ‹èµ·ä¾†åƒå¼•è¨€ï¼Œå˜—è©¦ç§»é™¤\n",
        "        lines = cleaned_response.splitlines()\n",
        "        if lines and not re.match(r\"^\\s*\\d+\\.\\s*\", lines[0]): # å¦‚æœç¬¬ä¸€è¡Œä¸æ˜¯ \"æ•¸å­—.\" é–‹é ­\n",
        "            # ä¸¦ä¸”ç¬¬ä¸€è¡Œçœ‹èµ·ä¾†åƒæ˜¯ä¸€å¥ç°¡çŸ­çš„å¼•è¨€è€Œä¸æ˜¯å¯¦éš›å…§å®¹\n",
        "            first_line_lower = lines[0].lower()\n",
        "            is_likely_intro = True\n",
        "            # å¦‚æœç¬¬ä¸€è¡ŒåŒ…å«åŸæ–™åï¼Œå¯èƒ½ä¸æ˜¯å¼•è¨€ï¼Œé™¤ééå¸¸çŸ­\n",
        "            for mat_kw in keywords.get(\"åŸæ–™åç¨±\", []):\n",
        "                if mat_kw.lower() in first_line_lower:\n",
        "                    if len(lines[0]) > 20 : # å¦‚æœåŒ…å«åŸæ–™åä½†è¼ƒé•·ï¼Œå¯èƒ½å°±æ˜¯å…§å®¹\n",
        "                        is_likely_intro = False\n",
        "                    break\n",
        "            if len(lines[0]) > 50: # å¦‚æœç¬¬ä¸€è¡Œå¾ˆé•·ï¼Œä¸åƒå¼•è¨€\n",
        "                is_likely_intro = False\n",
        "\n",
        "            if is_likely_intro and len(lines) > 1 : # ç¢ºä¿æœ‰å¤šè¡Œï¼Œç§»é™¤å¾Œé‚„æœ‰å…§å®¹\n",
        "                print(f\"   DEBUG: (Synthesize) è¼¸å‡ºé¦–è¡Œ '{lines[0]}' ä¸¦éæ¨™æº–åˆ—è¡¨é …ï¼Œå˜—è©¦ç§»é™¤ä½œç‚ºå¼•è¨€ã€‚\")\n",
        "                cleaned_response = \"\\n\".join(lines[1:]).strip()\n",
        "            elif is_likely_intro and len(lines) == 1: # åªæœ‰ä¸€è¡Œä¸”ä¸åƒåˆ—è¡¨é …\n",
        "                print(f\"   DEBUG: (Synthesize) è¼¸å‡ºåªæœ‰ä¸€è¡Œ '{lines[0]}' ä¸”ä¸¦éæ¨™æº–åˆ—è¡¨é …ï¼Œå¯èƒ½LLMæœªéµå¾ªæŒ‡ç¤ºã€‚\")\n",
        "                # æ­¤æ™‚å¯ä»¥è€ƒæ…®è¿”å›ä¸€å€‹å›ºå®šè¨Šæ¯æˆ–åŸå§‹æå–å…§å®¹çš„ç°¡å–®çµ„åˆ\n",
        "                # cleaned_response = \"LLMæœªèƒ½æŒ‰è¦æ±‚æ ¼å¼åŒ–è³‡è¨Šï¼Œè«‹é‡è©¦æˆ–æª¢æŸ¥æ—¥èªŒã€‚\"\n",
        "\n",
        "        if not cleaned_response.strip() and valid_extractions:\n",
        "            print(f\"   âš ï¸ è­¦å‘Š: (Synthesize) LLM æœ€çµ‚è¼¸å‡ºç‚ºç©ºæˆ–åƒ…å«ç©ºç™½ï¼Œä½†ä¹‹å‰æœ‰ {len(valid_extractions)} æ¢æœ‰æ•ˆæå–å…§å®¹ã€‚å¯èƒ½æ•´åˆå¤±æ•—ã€‚\")\n",
        "            return f\"å·²æ‰¾åˆ°é—œæ–¼åŸæ–™ã€{material_name}ã€‘çš„ç›¸é—œè³‡è¨Šï¼Œä½†åœ¨æœ€çµ‚æ•´ç†æ ¼å¼æ™‚å‡ºç¾å•é¡Œã€‚è«‹ç¨å¾Œå†è©¦ã€‚\"\n",
        "\n",
        "        # å¦‚æœæ¸…ç†å¾Œ response è®Šç©ºï¼Œä½† valid_extractions æœ‰å…§å®¹ï¼Œèªªæ˜ LLM å¯èƒ½å®Œå…¨æ²’æŒ‰æŒ‡ç¤ºè¼¸å‡º\n",
        "        if not cleaned_response and valid_extractions:\n",
        "             return f\"å·²æ‰¾åˆ°é—œæ–¼åŸæ–™ã€{material_name}ã€‘çš„ç›¸é—œè³‡è¨Šç‰‡æ®µï¼Œä½†ç„¡æ³•æŒ‰é æœŸæ ¼å¼åŒ–å‘ˆç¾ã€‚è«‹ç¨å¾Œé‡è©¦ã€‚\"\n",
        "\n",
        "\n",
        "        return cleaned_response\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ æ•´åˆçµæœæ™‚ç™¼ç”Ÿåš´é‡éŒ¯èª¤ï¼š{e}\")\n",
        "        traceback.print_exc()\n",
        "        return \"æ•´åˆçµæœæ™‚ç™¼ç”Ÿåš´é‡éŒ¯èª¤ï¼Œè«‹æª¢æŸ¥ç³»çµ±æ—¥èªŒã€‚\"\n",
        "# --- *** å‡½å¼çµæŸ *** ---\n",
        "\n",
        "# --- å…¨åŸŸè®Šæ•¸ ---\n",
        "llm = None\n",
        "sections_to_search = []\n",
        "line_bot_api = None\n",
        "handler = None\n",
        "public_url = None\n",
        "LLM_MODEL_NAME = \"llama3-70b-8192\" # æˆ–è€…æ‚¨ä½¿ç”¨çš„æ¨¡å‹\n",
        "app = Flask(__name__)\n",
        "initialization_success = False # æ–°å¢ä¸€å€‹å…¨åŸŸè®Šæ•¸ä¾†è¿½è¹¤åˆå§‹åŒ–ç‹€æ…‹\n",
        "\n",
        "# --- è¨Šæ¯è™•ç†å‡½å¼å®šç¾© ---\n",
        "def handle_message(event):\n",
        "    global llm, sections_to_search, line_bot_api, initialization_success\n",
        "    msg = event.message.text\n",
        "    reply_token = event.reply_token\n",
        "    user_id = event.source.user_id\n",
        "    print(f\"æ”¶åˆ°ä¾†è‡ª {user_id} çš„è¨Šæ¯: '{msg}'\")\n",
        "    start_time = time.time()\n",
        "    reply_text = \"æŠ±æ­‰ï¼Œè™•ç†æ‚¨çš„è«‹æ±‚æ™‚ç™¼ç”Ÿå…§éƒ¨å•é¡Œï¼Œè«‹ç¨å¾Œå†è©¦ã€‚\" # é è¨­éŒ¯èª¤è¨Šæ¯\n",
        "\n",
        "    if not initialization_success or not llm or not sections_to_search: # æª¢æŸ¥åˆå§‹åŒ–ç‹€æ…‹\n",
        "        reply_text = \"ç³»çµ±æ­£åœ¨åˆå§‹åŒ–æˆ–é‡åˆ°å•Ÿå‹•å•é¡Œï¼Œè«‹ç¨å¾Œå†è©¦ã€‚å¦‚æœå•é¡ŒæŒçºŒï¼Œè«‹è¯ç¹«ç®¡ç†å“¡ã€‚\"\n",
        "        print(\"âŒ éŒ¯èª¤ï¼šç³»çµ±æœªå®Œå…¨åˆå§‹åŒ– (LLM æˆ– SOP sections æœªå°±ç·’ï¼Œæˆ–åˆå§‹åŒ–éç¨‹å¤±æ•—)ã€‚\")\n",
        "    else:\n",
        "        try:\n",
        "            # --- ä½¿ç”¨ä¿®æ”¹å¾Œçš„è¦å‰‡è§£æè¼¸å…¥ ---\n",
        "            keywords_extraction_result = extract_keywords_rule_based(msg, TARGET_DESCRIPTION_KEYWORDS) # Stage 0\n",
        "\n",
        "            if keywords_extraction_result and keywords_extraction_result.get(\"åŸæ–™åç¨±\"):\n",
        "                # ä¸»è¦ä½¿ç”¨åŸæ–™åç¨±é€²è¡Œæœå°‹\n",
        "                # search_query_keywords å‚³çµ¦ search_sections\n",
        "                # keywords_extraction_result (åŒ…å«åŸæ–™å’Œå¯èƒ½çš„ç‰¹æ€§) å‚³çµ¦ LLM éšæ®µä½œç‚ºå®Œæ•´ä¸Šä¸‹æ–‡\n",
        "\n",
        "                # ** ä¿®æ”¹ï¼šç›´æ¥ä½¿ç”¨ keywords_extraction_result çµ¦ search_sectionsï¼Œè®“å…¶å…§éƒ¨æ±ºå®šç”¨å“ªäº›é—œéµå­— **\n",
        "                # search_sections å…§éƒ¨å·²ä¿®æ”¹ç‚ºåªç”¨ \"åŸæ–™åç¨±\"\n",
        "                relevant_sections = search_sections(sections_to_search, keywords_extraction_result) # Stage 1 Search\n",
        "\n",
        "                if relevant_sections:\n",
        "                    print(f\"   åˆæ­¥æ‰¾åˆ° {len(relevant_sections)} å€‹å¯èƒ½ç›¸é—œå€å¡Šï¼Œé‡å°æŸ¥è©¢: {keywords_extraction_result.get('åŸæ–™åç¨±')}\")\n",
        "                    # å‚³éå®Œæ•´çš„ keywords_extraction_result çµ¦ LLMï¼Œè®“ LLM æœ‰æ›´å®Œæ•´çš„ä¸Šä¸‹æ–‡\n",
        "                    extracted_texts = extract_relevant_text(llm, relevant_sections, keywords_extraction_result) # Stage 2 Extract\n",
        "                    final_summary = synthesize_results(llm, keywords_extraction_result, extracted_texts) # Stage 3 Synthesize\n",
        "                    reply_text = final_summary\n",
        "                else:\n",
        "                    material_name_str = \"ã€\".join(keywords_extraction_result.get(\"åŸæ–™åç¨±\", [\"æœªçŸ¥åŸæ–™\"]))\n",
        "                    reply_text = f\"åœ¨æŒ‡å®šçš„å·¥ä½œè¡¨ç¯„åœå…§ï¼Œæ‰¾ä¸åˆ°èˆ‡åŸæ–™ã€{material_name_str}ã€‘ç›´æ¥ç›¸é—œçš„SOPå…§å®¹ã€‚\"\n",
        "                    print(f\"   åœ¨æŒ‡å®šç¯„åœå…§æœªæ‰¾åˆ°èˆ‡åŸæ–™ã€{material_name_str}ã€‘ç›¸é—œçš„å€å¡Šã€‚\")\n",
        "            else:\n",
        "                reply_text = \"ç„¡æ³•å¾æ‚¨çš„è¨Šæ¯ä¸­è§£æå‡ºæœ‰æ•ˆçš„åŸæ–™åç¨±é€²è¡ŒæŸ¥è©¢ã€‚è«‹å˜—è©¦è¼¸å…¥æ˜ç¢ºçš„åŸæ–™åã€‚\"\n",
        "                print(\"   æœªèƒ½å¾è¼¸å…¥è§£æå‡ºæœ‰æ•ˆçš„åŸæ–™åç¨± (è¦å‰‡è§£æå¤±æ•—æˆ–ç„¡åŸæ–™çµæœ)ã€‚\")\n",
        "        except Exception as e:\n",
        "            print(f\"!!!!!!!!!! è™•ç†è¨Šæ¯ '{msg}' æ™‚ç™¼ç”Ÿåš´é‡éŒ¯èª¤ !!!!!!!!!!\")\n",
        "            traceback.print_exc()\n",
        "            reply_text = \"æŠ±æ­‰ï¼Œè™•ç†æ‚¨çš„æŸ¥è©¢æ™‚é‡åˆ°æœªé æœŸçš„éŒ¯èª¤ï¼Œå·¥ç¨‹å¸«å·²æ”¶åˆ°é€šçŸ¥ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚\"\n",
        "\n",
        "    end_time = time.time()\n",
        "    processing_time = end_time - start_time\n",
        "    print(f\"è¨Šæ¯ \\\"{msg}\\\" è™•ç†å®Œæˆï¼Œè€—æ™‚ {processing_time:.2f} ç§’ã€‚æœ€çµ‚å›è¦† (å‰100å­—): {reply_text[:100].replace(os.linesep, ' ')}\")\n",
        "    try:\n",
        "        if line_bot_api:\n",
        "            # æª¢æŸ¥ reply_text æ˜¯å¦ç‚ºç©ºï¼Œå¦‚æœæ˜¯ç©ºï¼Œçµ¦ä¸€å€‹é€šç”¨è¨Šæ¯\n",
        "            if not reply_text or not reply_text.strip():\n",
        "                print(\"âš ï¸è­¦å‘Šï¼šæœ€çµ‚å›è¦†å…§å®¹ç‚ºç©ºï¼Œå°‡ç™¼é€é€šç”¨æç¤ºè¨Šæ¯ã€‚\")\n",
        "                reply_text = \"æŠ±æ­‰ï¼Œæœªèƒ½æ‰¾åˆ°æ˜ç¢ºçš„è³‡è¨Šæˆ–è™•ç†æ™‚ç™¼ç”Ÿå•é¡Œã€‚è«‹å˜—è©¦æ›´æ›æŸ¥è©¢è©æˆ–ç¨å¾Œå†è©¦ã€‚\"\n",
        "\n",
        "            messages_to_reply = [TextSendMessage(text=reply_text)]\n",
        "            line_bot_api.reply_message(reply_token, messages=messages_to_reply)\n",
        "            print(f\"   âœ… æˆåŠŸå›è¦†è¨Šæ¯è‡³ LINEã€‚\")\n",
        "        else:\n",
        "            print(\"âŒ éŒ¯èª¤ï¼šline_bot_api æœªåˆå§‹åŒ–ï¼Œç„¡æ³•å›è¦†è¨Šæ¯ã€‚\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ å›è¦†è¨Šæ¯è‡³ LINE æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}\")\n",
        "        traceback.print_exc()\n",
        "\n",
        "# --- åˆå§‹åŒ–è¨­å®š ---\n",
        "def initialize_bot():\n",
        "    global llm, sections_to_search, line_bot_api, handler, public_url, initialization_success, app, LLM_MODEL_NAME # åŠ å…¥ initialization_success\n",
        "    print(\"--- é–‹å§‹åŸ·è¡Œåˆå§‹åŒ–è¨­å®š ---\")\n",
        "    initialization_success = False # é‡ç½®ç‹€æ…‹\n",
        "    LINE_ACCESS_TOKEN = LINE_SECRET = GROQ_API_KEY = NGROK_AUTH_TOKEN = None\n",
        "    # 1. è¼‰å…¥ API é‡‘é‘°\n",
        "    print(\"1. è¼‰å…¥ API é‡‘é‘°...\")\n",
        "    try:\n",
        "        LINE_ACCESS_TOKEN = userdata.get('LINE_ACCESS_TOKEN')\n",
        "        LINE_SECRET = userdata.get('LINE_SECRET')\n",
        "        GROQ_API_KEY = userdata.get('GROQ_API_KEY')\n",
        "        NGROK_AUTH_TOKEN = userdata.get('ngrok_token')\n",
        "        if not all([LINE_ACCESS_TOKEN, LINE_SECRET, GROQ_API_KEY]): # NGROK_AUTH_TOKEN is optional for local dev\n",
        "            print(\"âŒ éŒ¯èª¤ï¼šç¼ºå°‘å¿…è¦çš„ LINE æˆ– GROQ API é‡‘é‘°ã€‚\")\n",
        "            return False # ç›´æ¥è¿”å›ï¼Œä¸å†ç¹¼çºŒ\n",
        "        else:\n",
        "            print(\"   âœ… API é‡‘é‘°è¼‰å…¥æˆåŠŸã€‚\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ å¾ Colab Secrets è¼‰å…¥é‡‘é‘°æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}\")\n",
        "        return False\n",
        "\n",
        "    # 2. åˆå§‹åŒ– LLM\n",
        "    print(f\"2. åˆå§‹åŒ– LLM ({LLM_MODEL_NAME})...\")\n",
        "    try:\n",
        "        llm = ChatGroq(model=LLM_MODEL_NAME, groq_api_key=GROQ_API_KEY, temperature=0.05) # æº«åº¦å¯ä»¥èª¿ä½ï¼Œè®“è¼¸å‡ºæ›´ç©©å®š\n",
        "        print(f\"   âœ… LLM ({LLM_MODEL_NAME}) åˆå§‹åŒ–æˆåŠŸã€‚\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ åˆå§‹åŒ– LLM ({LLM_MODEL_NAME}) æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}\")\n",
        "        llm = None # ç¢ºä¿ llm æ˜¯ None\n",
        "        return False\n",
        "\n",
        "    # 3. è¼‰å…¥ä¸¦éæ¿¾ SOP æ–‡ä»¶å€å¡Š\n",
        "    print(\"3. è¼‰å…¥ä¸¦éæ¿¾ SOP æ–‡ä»¶å€å¡Š...\")\n",
        "    all_sections = load_markdown_sections()\n",
        "    if all_sections:\n",
        "        original_count = len(all_sections)\n",
        "        sections_to_search = filter_sections_by_title(all_sections)\n",
        "        if sections_to_search:\n",
        "            allowed_titles_str = ', '.join([s.get('title','?') for s in sections_to_search])\n",
        "            print(f\"   âœ… æˆåŠŸè¼‰å…¥ {original_count} å€å¡Šï¼Œå·²éæ¿¾å‡º {len(sections_to_search)} å€‹ç›®æ¨™å€å¡Š: [{allowed_titles_str}]\")\n",
        "        else:\n",
        "            print(f\"âŒ è­¦å‘Šï¼šé›–ç„¶è¼‰å…¥SOPæ–‡ä»¶ï¼Œä½†æœªèƒ½æ‰¾åˆ°ä»»ä½•æŒ‡å®šçš„å·¥ä½œè¡¨ {ALLOWED_WORKSHEET_IDENTIFIERS}ã€‚ç³»çµ±ä»å¯å•Ÿå‹•ï¼Œä½†æŸ¥è©¢ç¯„åœå¯èƒ½å—é™ã€‚\")\n",
        "            # å³ä½¿æ‰¾ä¸åˆ°ç‰¹å®šå·¥ä½œè¡¨ï¼Œä¹Ÿå…è¨±ç¹¼çºŒï¼Œä½† sections_to_search æœƒæ˜¯ç©ºçš„\n",
        "            # æˆ–è€…ï¼Œå¯ä»¥é¸æ“‡åœ¨é€™è£¡ return False å¦‚æœåš´æ ¼è¦æ±‚å¿…é ˆæœ‰é€™äº›å·¥ä½œè¡¨\n",
        "    else:\n",
        "        print(\"âŒ éŒ¯èª¤ï¼šæœªèƒ½è¼‰å…¥ä»»ä½• SOP æ–‡ä»¶å€å¡Šã€‚ç³»çµ±ç„¡æ³•è™•ç†æŸ¥è©¢ã€‚\")\n",
        "        sections_to_search = [] # ç¢ºä¿æ˜¯ç©ºåˆ—è¡¨\n",
        "        return False\n",
        "\n",
        "    # 4. åˆå§‹åŒ– LINE Bot API\n",
        "    print(\"4. åˆå§‹åŒ– LINE Bot...\")\n",
        "    try:\n",
        "        line_bot_api = LineBotApi(LINE_ACCESS_TOKEN)\n",
        "        handler = WebhookHandler(LINE_SECRET)\n",
        "        # æ³¨æ„ï¼šhandle_message ä¸å†ä½¿ç”¨ @handler.add è£é£¾å™¨ï¼Œè€Œæ˜¯åœ¨é€™è£¡æ‰‹å‹•è¨»å†Š\n",
        "        # handler.add(MessageEvent, message=TextMessage)(handle_message) # é€™æ˜¯èˆŠæ–¹æ³•\n",
        "        # æ–°æ–¹æ³•ï¼šå®šç¾©ä¸€å€‹ wrapper æˆ–ç›´æ¥å‚³é\n",
        "        @handler.add(MessageEvent, message=TextMessage)\n",
        "        def message_handler_wrapper(event):\n",
        "            handle_message(event) # å‘¼å«æˆ‘å€‘çš„ä¸»è™•ç†å‡½æ•¸\n",
        "        print(\"   âœ… LINE Bot Handler åˆå§‹åŒ–å®Œæˆï¼Œä¸¦å·²è¨»å†Šè¨Šæ¯è™•ç†å™¨ã€‚\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ åˆå§‹åŒ– LineBotApi æˆ– WebhookHandler æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}\")\n",
        "        line_bot_api = None\n",
        "        handler = None\n",
        "        return False\n",
        "\n",
        "    # 5. è¨­å®šä¸¦å•Ÿå‹• Ngrok (å¦‚æœéœ€è¦)\n",
        "    # æª¢æŸ¥æ˜¯å¦åœ¨ Colab ç’°å¢ƒæˆ–æœ‰ NGROK_AUTH_TOKEN æ‰å•Ÿå‹• ngrok\n",
        "    run_ngrok = 'google.colab' in str(get_ipython()) if 'get_ipython' in globals() else False # type: ignore\n",
        "    if not run_ngrok and NGROK_AUTH_TOKEN: # å¦‚æœä¸åœ¨Colabä½†æœ‰tokenï¼Œä¹Ÿå¯èƒ½æƒ³ç”¨ngrok\n",
        "        run_ngrok = True\n",
        "    elif not NGROK_AUTH_TOKEN and run_ngrok: # åœ¨colabä½†æ²’æœ‰token\n",
        "        print(\"   âš ï¸ è­¦å‘Š: åœ¨Colabç’°å¢ƒä½†æœªè¨­å®šNgrokèªè­‰æ¬Šæ–(ngrok_token)ï¼Œngroké€šé“å¯èƒ½ä¸ç©©å®šæˆ–å—é™ã€‚\")\n",
        "\n",
        "\n",
        "    if run_ngrok:\n",
        "        print(\"5. è¨­å®šä¸¦å•Ÿå‹• Ngrok...\"); flask_port = int(os.environ.get(\"PORT\", 5000))\n",
        "        try:\n",
        "            if NGROK_AUTH_TOKEN:\n",
        "                conf.get_default().auth_token = NGROK_AUTH_TOKEN # æ–°ç‰ˆ pyngrok è¨­å®šæ–¹å¼\n",
        "                print(\"   - Ngrok èªè­‰æ¬Šæ–å·²è¨­å®šã€‚\")\n",
        "            # ngrok.kill() # å¯èƒ½éœ€è¦ try-except åŒ…è£¹ï¼Œå¦‚æœæ²’æœ‰æ­£åœ¨é‹è¡Œçš„é€šé“æœƒå ±éŒ¯\n",
        "            try: ngrok.kill()\n",
        "            except: pass\n",
        "\n",
        "            # æª¢æŸ¥æ˜¯å¦æœ‰PYTHONASYNCIODEBUGç’°å¢ƒè®Šé‡ï¼Œå¦‚æœæœ‰ï¼Œngrok.connectå¯èƒ½æœƒå‡ºå•é¡Œ\n",
        "            if os.environ.get('PYTHONASYNCIODEBUG'):\n",
        "                print(\"   âš ï¸ åµæ¸¬åˆ° PYTHONASYNCIODEBUG ç’°å¢ƒè®Šé‡ï¼Œæš«æ™‚ç§»é™¤ä»¥é¿å…å½±éŸ¿ ngrokã€‚\")\n",
        "                original_async_debug = os.environ.pop('PYTHONASYNCIODEBUG')\n",
        "                public_url = ngrok.connect(flask_port).public_url\n",
        "                os.environ['PYTHONASYNCIODEBUG'] = original_async_debug # æ¢å¾©\n",
        "            else:\n",
        "                public_url = ngrok.connect(flask_port).public_url\n",
        "\n",
        "            print(f\"   âœ… ngrok é€šé“å·²å•Ÿå‹•: \\\"{public_url}\\\" -> \\\"[http://127.0.0.1](http://127.0.0.1):{flask_port}\\\"\")\n",
        "            print(f\"   ğŸ‘‰ è«‹å°‡ LINE Webhook URL è¨­ç‚º: {public_url}/\") # LINE Webhook é€šå¸¸ä¸éœ€è¦è·¯å¾‘ï¼Œç›´æ¥æ ¹ç›®éŒ„\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ å•Ÿå‹• ngrok æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}\")\n",
        "            traceback.print_exc()\n",
        "            if \"ERR_NGROK_4018\" in str(e) or \"authentication failed\" in str(e).lower():\n",
        "                print(\"   >>> Ngrok èªè­‰å¤±æ•—ï¼Œè«‹æª¢æŸ¥ Colab Secret 'ngrok_token' æˆ– Ngrok å„€è¡¨æ¿è¨­å®šã€‚<<<\")\n",
        "            public_url = None # ç¢ºä¿ public_url æ˜¯ None\n",
        "            # Ngrok å¤±æ•—ä¸ä¸€å®šä»£è¡¨æ•´å€‹æœå‹™å¤±æ•—ï¼Œå¦‚æœåªæ˜¯æœ¬åœ°æ¸¬è©¦å¯ä»¥ä¸ç”¨ ngrok\n",
        "            # ä½†å°æ–¼ LINE Botï¼Œngrok é€šå¸¸æ˜¯å¿…è¦çš„\n",
        "            return False # å¦‚æœ ngrok æ˜¯å¿…è¦çš„ï¼Œé€™è£¡æ‡‰è©²è¿”å› False\n",
        "    else:\n",
        "        print(\"5. è·³é Ngrok å•Ÿå‹• (å¯èƒ½ç‚ºæœ¬åœ°é–‹ç™¼æˆ–ä¸éœ€è¦å¤–éƒ¨è¨ªå•)ã€‚\")\n",
        "        public_url = None\n",
        "\n",
        "\n",
        "    initialization_success = True # æ‰€æœ‰æ­¥é©ŸæˆåŠŸ\n",
        "    print(\"--- åˆå§‹åŒ–è¨­å®šæˆåŠŸå®Œæˆ ---\")\n",
        "    return True\n",
        "\n",
        "# --- Webhook ç«¯é» ---\n",
        "@app.route(\"/\", methods=['POST'])\n",
        "def callback():\n",
        "    global handler\n",
        "    if not handler:\n",
        "        print(\"âŒ Callback éŒ¯èª¤ï¼šWebhook Handler æœªåˆå§‹åŒ–ã€‚\")\n",
        "        abort(500)\n",
        "    signature = request.headers.get('X-Line-Signature', '')\n",
        "    body = request.get_data(as_text=True)\n",
        "    # print(f\"Webhook received. Signature: {signature[:10]}... Body: {body[:100]}...\") # Debug\n",
        "    try:\n",
        "        handler.handle(body, signature)\n",
        "    except InvalidSignatureError:\n",
        "        print(\"âŒ éŒ¯èª¤ï¼šLINE Webhook ç°½åé©—è­‰å¤±æ•—ã€‚è«‹æª¢æŸ¥æ‚¨çš„ Channel Secret æ˜¯å¦æ­£ç¢ºã€‚\")\n",
        "        abort(400)\n",
        "    except AttributeError as ae:\n",
        "        print(f\"âŒ éŒ¯èª¤ï¼šHandler ç‰©ä»¶ä¸å­˜åœ¨æˆ–ç„¡æ•ˆ (AttributeError): {ae}\")\n",
        "        traceback.print_exc()\n",
        "        abort(500)\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ è™•ç† Webhook æ™‚ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤: {e}\")\n",
        "        traceback.print_exc()\n",
        "        abort(500)\n",
        "    return 'OK'\n",
        "\n",
        "# --- åŸ·è¡Œ Flask App ---\n",
        "if __name__ == \"__main__\":\n",
        "    # æª¢æŸ¥æ˜¯å¦åœ¨ Colab Notebook ç’°å¢ƒä¸­\n",
        "    is_colab = 'google.colab' in str(get_ipython()) if 'get_ipython' in globals() else False # type: ignore\n",
        "\n",
        "    if initialize_bot():\n",
        "        print(\"\\n--- æ‰€æœ‰å…ƒä»¶å·²å°±ç·’ï¼Œæ­£åœ¨æº–å‚™å•Ÿå‹• Flask app ---\")\n",
        "        port = int(os.environ.get(\"PORT\", 5000))\n",
        "        if is_colab and public_url:\n",
        "            print(f\"Flask app å°‡åœ¨èƒŒæ™¯é‹è¡Œã€‚LINE Webhook URL: {public_url}/\")\n",
        "            # åœ¨ Colab ä¸­ï¼Œapp.run() æœƒé˜»å¡ï¼Œä½† ngrok å·²ç¶“å•Ÿå‹•ã€‚\n",
        "            # é€šå¸¸ Colab Cell åŸ·è¡Œå®Œç•¢å¾Œï¼ŒèƒŒæ™¯æœå‹™å¯èƒ½çµ‚æ­¢ã€‚\n",
        "            # é€™è£¡è®“å®ƒé‹è¡Œï¼Œä½†ä½¿ç”¨è€…éœ€è¦çŸ¥é“å¦‚ä½•åœæ­¢ (é€šå¸¸æ˜¯ä¸­æ–· kernel)ã€‚\n",
        "            # app.run(host='0.0.0.0', port=port) # é€™æ¨£å¯ä»¥åœ¨ Colab ä¸­é‹è¡Œ\n",
        "            print(f\"åœ¨ Colab ç’°å¢ƒä¸­ï¼ŒFlask app (port {port}) å·²é…ç½®ã€‚è«‹ç¢ºä¿æ­¤ Cell æŒçºŒé‹è¡Œä»¥ä¿æŒæœå‹™ã€‚\")\n",
        "            # è‹¥è¦åœ¨ Colab ä¸­çœŸæ­£å¾Œå°é‹è¡Œï¼Œéœ€è¦æ›´è¤‡é›œçš„è™•ç†æˆ–ä½¿ç”¨ `nohup` ç­‰ï¼ˆä½†ä¸é©ç”¨æ–¼ Notebook cellï¼‰\n",
        "            # ç°¡å–®èµ·è¦‹ï¼Œå‘ŠçŸ¥ç”¨æˆ¶ä¿æŒ Cell é‹è¡Œ\n",
        "            try:\n",
        "                 app.run(host='0.0.0.0', port=port) # å¦‚æœåœ¨ Colabï¼Œngrok å·²ç¶“è™•ç†äº†å¤–éƒ¨è¨ªå•\n",
        "            except KeyboardInterrupt:\n",
        "                 print(\"\\n--- Flask app å·²æ‰‹å‹•åœæ­¢ (KeyboardInterrupt) ---\")\n",
        "            except Exception as run_err:\n",
        "                 print(f\"\\n--- Flask app é‹è¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤: {run_err} ---\")\n",
        "                 traceback.print_exc()\n",
        "            finally:\n",
        "                if public_url:\n",
        "                    try: ngrok.disconnect(public_url); print(\"   - ngrok é€šé“å·²é—œé–‰ã€‚\")\n",
        "                    except Exception as ng_err: print(f\"   - é—œé–‰ ngrok é€šé“ '{public_url}' æ™‚ç™¼ç”ŸéŒ¯èª¤: {ng_err}\")\n",
        "                try: ngrok.kill(); print(\"   - æ‰€æœ‰ ngrok é€šé“å·²å˜—è©¦é—œé–‰ã€‚\")\n",
        "                except Exception as ng_kill_err: print(f\"   - å˜—è©¦é—œé–‰æ‰€æœ‰ ngrok é€šé“æ™‚ç™¼ç”ŸéŒ¯èª¤: {ng_kill_err}\")\n",
        "                print(\"--- ç¨‹å¼çµæŸ ---\")\n",
        "\n",
        "        else: # é Colab ç’°å¢ƒæˆ– ngrok æœªå•Ÿå‹•\n",
        "            print(f\"Flask app æº–å‚™åœ¨æœ¬åœ°ç«¯å£ {port} å•Ÿå‹•...\")\n",
        "            if public_url:\n",
        "                 print(f\"LINE Webhook URL æ‡‰æŒ‡å‘: {public_url}/\")\n",
        "            else:\n",
        "                 print(\"Ngrok æœªå•Ÿå‹•æˆ–é…ç½®å¤±æ•—ã€‚å¦‚æœéœ€è¦ LINE Bot é€£æ¥ï¼Œè«‹ç¢ºä¿æœ¬åœ°æœå‹™å¯è¢«å¤–éƒ¨è¨ªå•ã€‚\")\n",
        "            try:\n",
        "                app.run(host='0.0.0.0', port=port, debug=False) # debug=True å¯èƒ½å°è‡´ ngrok å•é¡Œå’Œé‡è¤‡åˆå§‹åŒ–\n",
        "            except KeyboardInterrupt:\n",
        "                print(\"\\n--- Flask app å·²æ‰‹å‹•åœæ­¢ ---\")\n",
        "            except Exception as run_err:\n",
        "                print(f\"\\n--- Flask app é‹è¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤: {run_err} ---\")\n",
        "                traceback.print_exc()\n",
        "            finally:\n",
        "                if public_url: # ç¢ºä¿å³ä½¿åœ¨éColabç’°å¢ƒä¹Ÿé—œé–‰ngrok\n",
        "                    try: ngrok.disconnect(public_url); print(\"   - ngrok é€šé“å·²é—œé–‰ã€‚\")\n",
        "                    except Exception as ng_err: print(f\"   - é—œé–‰ ngrok é€šé“ '{public_url}' æ™‚ç™¼ç”ŸéŒ¯èª¤: {ng_err}\")\n",
        "                try: ngrok.kill(); print(\"   - æ‰€æœ‰ ngrok é€šé“å·²å˜—è©¦é—œé–‰ã€‚\")\n",
        "                except Exception as ng_kill_err: print(f\"   - å˜—è©¦é—œé–‰æ‰€æœ‰ ngrok é€šé“æ™‚ç™¼ç”ŸéŒ¯èª¤: {ng_kill_err}\")\n",
        "                print(\"--- ç¨‹å¼çµæŸ ---\")\n",
        "    else:\n",
        "        print(\"\\nâŒ å› åˆå§‹åŒ–å¤±æ•—ï¼Œç„¡æ³•å•Ÿå‹• Flask appã€‚è«‹æª¢æŸ¥ä¸Šæ–¹éŒ¯èª¤è¨Šæ¯ã€‚\")\n",
        "\n",
        "# åœ¨ Colab Cell ä¸­ï¼Œæ‚¨éœ€è¦åŸ·è¡Œæ­¤ Cell ä¾†å•Ÿå‹•æœå‹™ã€‚\n",
        "# è‹¥è¦åœæ­¢æœå‹™ï¼Œè«‹é»æ“Š Cell å·¦å´çš„åœæ­¢æŒ‰éˆ•æˆ–ä¸­æ–· Kernelã€‚"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9EPVW-fwQQrX",
        "outputId": "e8289d38-35b5-4bfb-ccc8-30f68eca60cd",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "æ­£åœ¨è¼‰å…¥å¿…è¦çš„å‡½å¼åº«...\n",
            "   - jieba å‡½å¼åº«è¼‰å…¥æˆåŠŸã€‚\n",
            "å‡½å¼åº«è¼‰å…¥å®Œæˆã€‚\n",
            "--- é–‹å§‹åŸ·è¡Œåˆå§‹åŒ–è¨­å®š ---\n",
            "1. è¼‰å…¥ API é‡‘é‘°...\n",
            "   âœ… API é‡‘é‘°è¼‰å…¥æˆåŠŸã€‚\n",
            "2. åˆå§‹åŒ– LLM (llama3-70b-8192)...\n",
            "   âœ… LLM (llama3-70b-8192) åˆå§‹åŒ–æˆåŠŸã€‚\n",
            "3. è¼‰å…¥ä¸¦éæ¿¾ SOP æ–‡ä»¶å€å¡Š...\n",
            "æ­£åœ¨å˜—è©¦è¼‰å…¥æª”æ¡ˆ: simplified_output_by_section.md\n",
            "æª”æ¡ˆ simplified_output_by_section.md è®€å–æˆåŠŸã€‚\n",
            "   âœ… æˆåŠŸè¼‰å…¥ 23 å€å¡Šï¼Œå·²éæ¿¾å‡º 2 å€‹ç›®æ¨™å€å¡Š: [## å·¥ä½œè¡¨: 9.æ–°å“-æ¨£å“è£½ä½œæ³¨æ„äº‹é …SOP, ## å·¥ä½œè¡¨: 10.æ–°å“-åŸæ–™ä½¿ç”¨æ³¨æ„äº‹é …]\n",
            "4. åˆå§‹åŒ– LINE Bot...\n",
            "   âœ… LINE Bot Handler åˆå§‹åŒ–å®Œæˆï¼Œä¸¦å·²è¨»å†Šè¨Šæ¯è™•ç†å™¨ã€‚\n",
            "5. è¨­å®šä¸¦å•Ÿå‹• Ngrok...\n",
            "   - Ngrok èªè­‰æ¬Šæ–å·²è¨­å®šã€‚\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-6e18aa3f33ff>:482: LineBotSdkDeprecatedIn30: Call to deprecated class LineBotApi. (Use v3 class; linebot.v3.<feature>. See https://github.com/line/line-bot-sdk-python/blob/master/README.rst for more details.) -- Deprecated since version 3.0.0.\n",
            "  line_bot_api = LineBotApi(LINE_ACCESS_TOKEN)\n",
            "<ipython-input-4-6e18aa3f33ff>:483: LineBotSdkDeprecatedIn30: Call to deprecated class WebhookHandler. (Use 'from linebot.v3.webhook import WebhookHandler' instead. See https://github.com/line/line-bot-sdk-python/blob/master/README.rst for more details.) -- Deprecated since version 3.0.0.\n",
            "  handler = WebhookHandler(LINE_SECRET)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   âœ… ngrok é€šé“å·²å•Ÿå‹•: \"https://8db1-35-225-61-135.ngrok-free.app\" -> \"[http://127.0.0.1](http://127.0.0.1):5000\"\n",
            "   ğŸ‘‰ è«‹å°‡ LINE Webhook URL è¨­ç‚º: https://8db1-35-225-61-135.ngrok-free.app/\n",
            "--- åˆå§‹åŒ–è¨­å®šæˆåŠŸå®Œæˆ ---\n",
            "\n",
            "--- æ‰€æœ‰å…ƒä»¶å·²å°±ç·’ï¼Œæ­£åœ¨æº–å‚™å•Ÿå‹• Flask app ---\n",
            "Flask app å°‡åœ¨èƒŒæ™¯é‹è¡Œã€‚LINE Webhook URL: https://8db1-35-225-61-135.ngrok-free.app/\n",
            "åœ¨ Colab ç’°å¢ƒä¸­ï¼ŒFlask app (port 5000) å·²é…ç½®ã€‚è«‹ç¢ºä¿æ­¤ Cell æŒçºŒé‹è¡Œä»¥ä¿æŒæœå‹™ã€‚\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on all addresses (0.0.0.0)\n",
            " * Running on http://127.0.0.1:5000\n",
            " * Running on http://172.28.0.12:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug:127.0.0.1 - - [09/May/2025 23:00:33] \"POST / HTTP/1.1\" 200 -\n",
            "Building prefix dict from the default dictionary ...\n",
            "DEBUG:jieba:Building prefix dict from the default dictionary ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "æ”¶åˆ°ä¾†è‡ª Ub0b9891b0b6e502ebb21a0a2c19a95eb çš„è¨Šæ¯: 'å¥¶é…ª'\n",
            "--- (éšæ®µ0) ä½¿ç”¨è¦å‰‡è§£æè¼¸å…¥ (ä¸»è¦æå–åŸæ–™): 'å¥¶é…ª' ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Dumping model to file cache /tmp/jieba.cache\n",
            "DEBUG:jieba:Dumping model to file cache /tmp/jieba.cache\n",
            "Loading model cost 0.886 seconds.\n",
            "DEBUG:jieba:Loading model cost 0.886 seconds.\n",
            "Prefix dict has been built successfully.\n",
            "DEBUG:jieba:Prefix dict has been built successfully.\n",
            "<ipython-input-4-6e18aa3f33ff>:421: LineBotSdkDeprecatedIn30: Call to deprecated method reply_message. (Use 'from linebot.v3.messaging import MessagingApi' and 'MessagingApi(...).reply_message(...)' instead. See https://github.com/line/line-bot-sdk-python/blob/master/README.rst for more details.) -- Deprecated since version 3.0.0.\n",
            "  line_bot_api.reply_message(reply_token, messages=messages_to_reply)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   åˆ†è©çµæœ: ['å¥¶é…ª']\n",
            "   è¦å‰‡è§£æçµæœ (ä¸»è¦ç‚ºåŸæ–™ï¼Œè¼”åŠ©ç‰¹æ€§): {'åŸæ–™åç¨±': ['å¥¶é…ª'], 'ç‰¹æ€§æè¿°': []}\n",
            "DEBUG: æ­£åœ¨ä½¿ç”¨ä»¥ä¸‹ä»»ä¸€ã€åŸæ–™åç¨±ã€‘é—œéµå­—æœå°‹: ['å¥¶é…ª']\n",
            "   åœ¨æŒ‡å®šç¯„åœå…§æœªæ‰¾åˆ°èˆ‡åŸæ–™ã€å¥¶é…ªã€‘ç›¸é—œçš„å€å¡Šã€‚\n",
            "è¨Šæ¯ \"å¥¶é…ª\" è™•ç†å®Œæˆï¼Œè€—æ™‚ 0.89 ç§’ã€‚æœ€çµ‚å›è¦† (å‰100å­—): åœ¨æŒ‡å®šçš„å·¥ä½œè¡¨ç¯„åœå…§ï¼Œæ‰¾ä¸åˆ°èˆ‡åŸæ–™ã€å¥¶é…ªã€‘ç›´æ¥ç›¸é—œçš„SOPå…§å®¹ã€‚\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [09/May/2025 23:00:52] \"POST / HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   âœ… æˆåŠŸå›è¦†è¨Šæ¯è‡³ LINEã€‚\n",
            "æ”¶åˆ°ä¾†è‡ª Ub0b9891b0b6e502ebb21a0a2c19a95eb çš„è¨Šæ¯: 'é¹½'\n",
            "--- (éšæ®µ0) ä½¿ç”¨è¦å‰‡è§£æè¼¸å…¥ (ä¸»è¦æå–åŸæ–™): 'é¹½' ---\n",
            "   åˆ†è©çµæœ: ['é¹½']\n",
            "   è¦å‰‡è§£æçµæœ (ä¸»è¦ç‚ºåŸæ–™ï¼Œè¼”åŠ©ç‰¹æ€§): {'åŸæ–™åç¨±': ['é¹½'], 'ç‰¹æ€§æè¿°': []}\n",
            "DEBUG: æ­£åœ¨ä½¿ç”¨ä»¥ä¸‹ä»»ä¸€ã€åŸæ–™åç¨±ã€‘é—œéµå­—æœå°‹: ['é¹½']\n",
            "   åˆæ­¥æ‰¾åˆ° 1 å€‹å¯èƒ½ç›¸é—œå€å¡Šï¼Œé‡å°æŸ¥è©¢: ['é¹½']\n",
            "--- (éšæ®µ1) ä½¿ç”¨ LLM å®šä½ä¸¦æå–å­ç« ç¯€ï¼Œä½¿ç”¨çš„æŸ¥è©¢ä¸Šä¸‹æ–‡: ä¸»è¦æŸ¥è©¢çš„åŸæ–™åç¨±ï¼šã€é¹½ã€‘ ---\n",
            "   æ­£åœ¨è™•ç†å€å¡Š: ## å·¥ä½œè¡¨: 9.æ–°å“-æ¨£å“è£½ä½œæ³¨æ„äº‹é …SOP (æœå°‹åŸæ–™ 'é¹½')...\n",
            "     â†³ å¾ '## å·¥ä½œè¡¨: 9.æ–°å“-æ¨£å“è£½ä½œæ³¨æ„äº‹é …SOP' æå–åˆ°å…§å®¹ (å‰100å­—): \"b-1.éç²—ç¯©çš„åŸæ–™ï¼š     â˜… ä¸€èˆ¬ç‚ºçµæ™¶ç‹€æœƒç”¢ç”Ÿå‡æ€§çµå¡Šçš„åŸæ–™ï¼Œéœ€éç²—ç¯©å¾Œæ‰èƒ½èˆ‡å…¶ä»–åŸæ–™æ··åˆã€‚         å¦‚ï¼šç´°ç ‚ç³–ã€è‘¡è„ç³–ã€é¹½ã€é¹½10%ã€æª¸æª¬é…¸ã€æª¸æª¬é…¸éˆ‰ã€â€¦ç­‰ã€‚...\"\n",
            "\n",
            "ğŸ”„ (éšæ®µ2) æ­£åœ¨æ•´åˆ 1 ä»½æå–çš„é‡é»å…§å®¹ä¸¦çµ±ä¸€æ ¼å¼ (åŠ›æ±‚åŸæ–‡ã€ç°¡æ½”)...\n",
            "è¨Šæ¯ \"é¹½\" è™•ç†å®Œæˆï¼Œè€—æ™‚ 1.51 ç§’ã€‚æœ€çµ‚å›è¦† (å‰100å­—): 1. é¹½ç‚ºçµæ™¶ç‹€æœƒç”¢ç”Ÿå‡æ€§çµå¡Šçš„åŸæ–™ï¼Œéœ€éç²—ç¯©å¾Œæ‰èƒ½èˆ‡å…¶ä»–åŸæ–™æ··åˆã€‚ 2. å¦‚ï¼šç´°ç ‚ç³–ã€è‘¡è„ç³–ã€é¹½ã€é¹½10%ã€æª¸æª¬é…¸ã€æª¸æª¬é…¸éˆ‰ã€â€¦ç­‰ã€‚\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-6e18aa3f33ff>:421: LineBotSdkDeprecatedIn30: Call to deprecated method reply_message. (Use 'from linebot.v3.messaging import MessagingApi' and 'MessagingApi(...).reply_message(...)' instead. See https://github.com/line/line-bot-sdk-python/blob/master/README.rst for more details.) -- Deprecated since version 3.0.0.\n",
            "  line_bot_api.reply_message(reply_token, messages=messages_to_reply)\n",
            "INFO:werkzeug:127.0.0.1 - - [09/May/2025 23:01:21] \"POST / HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   âœ… æˆåŠŸå›è¦†è¨Šæ¯è‡³ LINEã€‚\n",
            "æ”¶åˆ°ä¾†è‡ª Ub0b9891b0b6e502ebb21a0a2c19a95eb çš„è¨Šæ¯: 'é†¬æ²¹ç²‰'\n",
            "--- (éšæ®µ0) ä½¿ç”¨è¦å‰‡è§£æè¼¸å…¥ (ä¸»è¦æå–åŸæ–™): 'é†¬æ²¹ç²‰' ---\n",
            "   åˆ†è©çµæœ: ['é†¬æ²¹ç²‰']\n",
            "   è¦å‰‡è§£æçµæœ (ä¸»è¦ç‚ºåŸæ–™ï¼Œè¼”åŠ©ç‰¹æ€§): {'åŸæ–™åç¨±': ['é†¬æ²¹ç²‰'], 'ç‰¹æ€§æè¿°': []}\n",
            "DEBUG: æ­£åœ¨ä½¿ç”¨ä»¥ä¸‹ä»»ä¸€ã€åŸæ–™åç¨±ã€‘é—œéµå­—æœå°‹: ['é†¬æ²¹ç²‰']\n",
            "   åˆæ­¥æ‰¾åˆ° 1 å€‹å¯èƒ½ç›¸é—œå€å¡Šï¼Œé‡å°æŸ¥è©¢: ['é†¬æ²¹ç²‰']\n",
            "--- (éšæ®µ1) ä½¿ç”¨ LLM å®šä½ä¸¦æå–å­ç« ç¯€ï¼Œä½¿ç”¨çš„æŸ¥è©¢ä¸Šä¸‹æ–‡: ä¸»è¦æŸ¥è©¢çš„åŸæ–™åç¨±ï¼šã€é†¬æ²¹ç²‰ã€‘ ---\n",
            "   æ­£åœ¨è™•ç†å€å¡Š: ## å·¥ä½œè¡¨: 9.æ–°å“-æ¨£å“è£½ä½œæ³¨æ„äº‹é …SOP (æœå°‹åŸæ–™ 'é†¬æ²¹ç²‰')...\n",
            "     â†³ å¾ '## å·¥ä½œè¡¨: 9.æ–°å“-æ¨£å“è£½ä½œæ³¨æ„äº‹é …SOP' æå–åˆ°å…§å®¹ (å‰100å­—): \"â˜… é†¬æ²¹ç²‰ï¼šå¿…é ˆè¦èˆ‡10%äºŒçŸ½å…ˆæ··åˆå‡å‹»éç¯©å¾Œï¼Œæ‰å¯ä»¥åŠ å…¥ç¬¬ä¸‰æ®µèˆ‡å…¶ä»–ç²‰æ–™æ··å‹»ã€‚...\"\n",
            "\n",
            "ğŸ”„ (éšæ®µ2) æ­£åœ¨æ•´åˆ 1 ä»½æå–çš„é‡é»å…§å®¹ä¸¦çµ±ä¸€æ ¼å¼ (åŠ›æ±‚åŸæ–‡ã€ç°¡æ½”)...\n",
            "è¨Šæ¯ \"é†¬æ²¹ç²‰\" è™•ç†å®Œæˆï¼Œè€—æ™‚ 1.19 ç§’ã€‚æœ€çµ‚å›è¦† (å‰100å­—): 1. é†¬æ²¹ç²‰ï¼šå¿…é ˆè¦èˆ‡10%äºŒçŸ½å…ˆæ··åˆå‡å‹»éç¯©å¾Œï¼Œæ‰å¯ä»¥åŠ å…¥ç¬¬ä¸‰æ®µèˆ‡å…¶ä»–ç²‰æ–™æ··å‹»ã€‚\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-6e18aa3f33ff>:421: LineBotSdkDeprecatedIn30: Call to deprecated method reply_message. (Use 'from linebot.v3.messaging import MessagingApi' and 'MessagingApi(...).reply_message(...)' instead. See https://github.com/line/line-bot-sdk-python/blob/master/README.rst for more details.) -- Deprecated since version 3.0.0.\n",
            "  line_bot_api.reply_message(reply_token, messages=messages_to_reply)\n",
            "INFO:werkzeug:127.0.0.1 - - [09/May/2025 23:03:29] \"POST / HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   âœ… æˆåŠŸå›è¦†è¨Šæ¯è‡³ LINEã€‚\n",
            "æ”¶åˆ°ä¾†è‡ª Ub0b9891b0b6e502ebb21a0a2c19a95eb çš„è¨Šæ¯: 'å¥¶ç²¾'\n",
            "--- (éšæ®µ0) ä½¿ç”¨è¦å‰‡è§£æè¼¸å…¥ (ä¸»è¦æå–åŸæ–™): 'å¥¶ç²¾' ---\n",
            "   åˆ†è©çµæœ: ['å¥¶ç²¾']\n",
            "   è¦å‰‡è§£æçµæœ (ä¸»è¦ç‚ºåŸæ–™ï¼Œè¼”åŠ©ç‰¹æ€§): {'åŸæ–™åç¨±': ['å¥¶ç²¾'], 'ç‰¹æ€§æè¿°': []}\n",
            "DEBUG: æ­£åœ¨ä½¿ç”¨ä»¥ä¸‹ä»»ä¸€ã€åŸæ–™åç¨±ã€‘é—œéµå­—æœå°‹: ['å¥¶ç²¾']\n",
            "   åˆæ­¥æ‰¾åˆ° 1 å€‹å¯èƒ½ç›¸é—œå€å¡Šï¼Œé‡å°æŸ¥è©¢: ['å¥¶ç²¾']\n",
            "--- (éšæ®µ1) ä½¿ç”¨ LLM å®šä½ä¸¦æå–å­ç« ç¯€ï¼Œä½¿ç”¨çš„æŸ¥è©¢ä¸Šä¸‹æ–‡: ä¸»è¦æŸ¥è©¢çš„åŸæ–™åç¨±ï¼šã€å¥¶ç²¾ã€‘ ---\n",
            "   æ­£åœ¨è™•ç†å€å¡Š: ## å·¥ä½œè¡¨: 9.æ–°å“-æ¨£å“è£½ä½œæ³¨æ„äº‹é …SOP (æœå°‹åŸæ–™ 'å¥¶ç²¾')...\n",
            "     â†³ å¾ '## å·¥ä½œè¡¨: 9.æ–°å“-æ¨£å“è£½ä½œæ³¨æ„äº‹é …SOP' æå–åˆ°å…§å®¹ (å‰100å­—): \"c.  åŸæ–™æœ¬èº«å¸æ¿•æ€§é«˜ï¼š     â˜… å¥¶ç²¾ï¼šæ”¾ç¬¬äºŒæ®µï¼Œéå¸¸å®¹æ˜“å¸æ¿•ä¸å¯ä»¥æ”¾åœ¨ç¬¬ä¸€æ®µï¼Œå› æµå‹•æ€§å¥½ä¸é ˆéç¯©ã€‚...\"\n",
            "\n",
            "ğŸ”„ (éšæ®µ2) æ­£åœ¨æ•´åˆ 1 ä»½æå–çš„é‡é»å…§å®¹ä¸¦çµ±ä¸€æ ¼å¼ (åŠ›æ±‚åŸæ–‡ã€ç°¡æ½”)...\n",
            "è¨Šæ¯ \"å¥¶ç²¾\" è™•ç†å®Œæˆï¼Œè€—æ™‚ 1.16 ç§’ã€‚æœ€çµ‚å›è¦† (å‰100å­—): 1. å¥¶ç²¾ï¼šæ”¾ç¬¬äºŒæ®µï¼Œéå¸¸å®¹æ˜“å¸æ¿•ä¸å¯ä»¥æ”¾åœ¨ç¬¬ä¸€æ®µï¼Œå› æµå‹•æ€§å¥½ä¸é ˆéç¯©ã€‚\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-6e18aa3f33ff>:421: LineBotSdkDeprecatedIn30: Call to deprecated method reply_message. (Use 'from linebot.v3.messaging import MessagingApi' and 'MessagingApi(...).reply_message(...)' instead. See https://github.com/line/line-bot-sdk-python/blob/master/README.rst for more details.) -- Deprecated since version 3.0.0.\n",
            "  line_bot_api.reply_message(reply_token, messages=messages_to_reply)\n",
            "INFO:werkzeug:127.0.0.1 - - [09/May/2025 23:04:22] \"POST / HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   âœ… æˆåŠŸå›è¦†è¨Šæ¯è‡³ LINEã€‚\n",
            "æ”¶åˆ°ä¾†è‡ª Ub0b9891b0b6e502ebb21a0a2c19a95eb çš„è¨Šæ¯: 'è‰²ç´ '\n",
            "--- (éšæ®µ0) ä½¿ç”¨è¦å‰‡è§£æè¼¸å…¥ (ä¸»è¦æå–åŸæ–™): 'è‰²ç´ ' ---\n",
            "   åˆ†è©çµæœ: ['è‰²ç´ ']\n",
            "   è¦å‰‡è§£æçµæœ (ä¸»è¦ç‚ºåŸæ–™ï¼Œè¼”åŠ©ç‰¹æ€§): {'åŸæ–™åç¨±': ['è‰²ç´ '], 'ç‰¹æ€§æè¿°': []}\n",
            "DEBUG: æ­£åœ¨ä½¿ç”¨ä»¥ä¸‹ä»»ä¸€ã€åŸæ–™åç¨±ã€‘é—œéµå­—æœå°‹: ['è‰²ç´ ']\n",
            "   åˆæ­¥æ‰¾åˆ° 1 å€‹å¯èƒ½ç›¸é—œå€å¡Šï¼Œé‡å°æŸ¥è©¢: ['è‰²ç´ ']\n",
            "--- (éšæ®µ1) ä½¿ç”¨ LLM å®šä½ä¸¦æå–å­ç« ç¯€ï¼Œä½¿ç”¨çš„æŸ¥è©¢ä¸Šä¸‹æ–‡: ä¸»è¦æŸ¥è©¢çš„åŸæ–™åç¨±ï¼šã€è‰²ç´ ã€‘ ---\n",
            "   æ­£åœ¨è™•ç†å€å¡Š: ## å·¥ä½œè¡¨: 9.æ–°å“-æ¨£å“è£½ä½œæ³¨æ„äº‹é …SOP (æœå°‹åŸæ–™ 'è‰²ç´ ')...\n",
            "     â†³ å¾ '## å·¥ä½œè¡¨: 9.æ–°å“-æ¨£å“è£½ä½œæ³¨æ„äº‹é …SOP' æå–åˆ°å…§å®¹ (å‰100å­—): \"è‰²ç´ ç²‰çš„è£½ä½œæ–¹å¼æœ‰äºŒç¨®ï¼Œa.ç›´æ¥åŠ å…¥æ¶²é«”é¦™æ–™b.ä½¿ç”¨PGæº¶è§£ a.  ç§¤å–æ¶²é«”é¦™æ–™å¾Œå†åŠ å…¥è‰²ç´ ï¼Œä»”ç´°å°‡è‰²ç²‰ç²’æ“é–‹å¾Œå†æµ¸æ³¡ã€‚ b.  å› ç‚ºé¦™æ–™æœ¬èº«æº¶åŠ‘çš„ä¸åŒï¼Œæœ‰äº›é¦™æ–™ç„¡æ³•æº¶è§£è‰²ç´ ï¼Œæ­¤æ™‚éœ€ä½¿ç”¨PGæº¶è‰²ç´ ã€‚...\"\n",
            "\n",
            "ğŸ”„ (éšæ®µ2) æ­£åœ¨æ•´åˆ 1 ä»½æå–çš„é‡é»å…§å®¹ä¸¦çµ±ä¸€æ ¼å¼ (åŠ›æ±‚åŸæ–‡ã€ç°¡æ½”)...\n",
            "è¨Šæ¯ \"è‰²ç´ \" è™•ç†å®Œæˆï¼Œè€—æ™‚ 10.73 ç§’ã€‚æœ€çµ‚å›è¦† (å‰100å­—): 1. è‰²ç´ ç²‰çš„è£½ä½œæ–¹å¼æœ‰äºŒç¨®ï¼Œa.ç›´æ¥åŠ å…¥æ¶²é«”é¦™æ–™b.ä½¿ç”¨PGæº¶è§£ 2. a. ç§¤å–æ¶²é«”é¦™æ–™å¾Œå†åŠ å…¥è‰²ç´ ï¼Œä»”ç´°å°‡è‰²ç²‰ç²’æ“é–‹å¾Œå†æµ¸æ³¡ã€‚ 3. b. å› ç‚ºé¦™æ–™æœ¬èº«æº¶åŠ‘çš„ä¸åŒï¼Œæœ‰äº›é¦™æ–™ç„¡æ³•æº¶è§£è‰²ç´ ï¼Œæ­¤æ™‚éœ€ä½¿\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-6e18aa3f33ff>:421: LineBotSdkDeprecatedIn30: Call to deprecated method reply_message. (Use 'from linebot.v3.messaging import MessagingApi' and 'MessagingApi(...).reply_message(...)' instead. See https://github.com/line/line-bot-sdk-python/blob/master/README.rst for more details.) -- Deprecated since version 3.0.0.\n",
            "  line_bot_api.reply_message(reply_token, messages=messages_to_reply)\n",
            "INFO:werkzeug:127.0.0.1 - - [09/May/2025 23:04:50] \"POST / HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   âœ… æˆåŠŸå›è¦†è¨Šæ¯è‡³ LINEã€‚\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#md è½‰ pdf"
      ],
      "metadata": {
        "id": "pkeLyOc4Tv49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # @title å®‰è£ pandoc å’Œ LaTeX (æ”¯æ´ä¸­æ–‡)\n",
        "# print(\"æ­£åœ¨æ›´æ–°å¥—ä»¶åˆ—è¡¨...\")\n",
        "# !apt-get update -qq\n",
        "# print(\"æ­£åœ¨å®‰è£ pandoc, texlive-xetex åŠç›¸é—œå­—å‹...\")\n",
        "# !apt-get install -y pandoc texlive-xetex texlive-fonts-recommended texlive-plain-generic fonts-noto-cjk -qq\n",
        "# print(\"å®‰è£å®Œæˆï¼\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Mr5zUvgTTnf_",
        "outputId": "62140818-89c4-4a72-f8ea-54146ba15c5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "æ­£åœ¨æ›´æ–°å¥—ä»¶åˆ—è¡¨...\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "æ­£åœ¨å®‰è£ pandoc, texlive-xetex åŠç›¸é—œå­—å‹...\n",
            "Extracting templates from packages: 100%\n",
            "Preconfiguring packages ...\n",
            "Selecting previously unselected package fonts-droid-fallback.\n",
            "(Reading database ... 126333 files and directories currently installed.)\n",
            "Preparing to unpack .../00-fonts-droid-fallback_1%3a6.0.1r16-1.1build1_all.deb ...\n",
            "Unpacking fonts-droid-fallback (1:6.0.1r16-1.1build1) ...\n",
            "Selecting previously unselected package fonts-lato.\n",
            "Preparing to unpack .../01-fonts-lato_2.0-2.1_all.deb ...\n",
            "Unpacking fonts-lato (2.0-2.1) ...\n",
            "Selecting previously unselected package poppler-data.\n",
            "Preparing to unpack .../02-poppler-data_0.4.11-1_all.deb ...\n",
            "Unpacking poppler-data (0.4.11-1) ...\n",
            "Selecting previously unselected package tex-common.\n",
            "Preparing to unpack .../03-tex-common_6.17_all.deb ...\n",
            "Unpacking tex-common (6.17) ...\n",
            "Selecting previously unselected package fonts-urw-base35.\n",
            "Preparing to unpack .../04-fonts-urw-base35_20200910-1_all.deb ...\n",
            "Unpacking fonts-urw-base35 (20200910-1) ...\n",
            "Selecting previously unselected package libgs9-common.\n",
            "Preparing to unpack .../05-libgs9-common_9.55.0~dfsg1-0ubuntu5.11_all.deb ...\n",
            "Unpacking libgs9-common (9.55.0~dfsg1-0ubuntu5.11) ...\n",
            "Selecting previously unselected package libidn12:amd64.\n",
            "Preparing to unpack .../06-libidn12_1.38-4ubuntu1_amd64.deb ...\n",
            "Unpacking libidn12:amd64 (1.38-4ubuntu1) ...\n",
            "Selecting previously unselected package libijs-0.35:amd64.\n",
            "Preparing to unpack .../07-libijs-0.35_0.35-15build2_amd64.deb ...\n",
            "Unpacking libijs-0.35:amd64 (0.35-15build2) ...\n",
            "Selecting previously unselected package libjbig2dec0:amd64.\n",
            "Preparing to unpack .../08-libjbig2dec0_0.19-3build2_amd64.deb ...\n",
            "Unpacking libjbig2dec0:amd64 (0.19-3build2) ...\n",
            "Selecting previously unselected package libgs9:amd64.\n",
            "Preparing to unpack .../09-libgs9_9.55.0~dfsg1-0ubuntu5.11_amd64.deb ...\n",
            "Unpacking libgs9:amd64 (9.55.0~dfsg1-0ubuntu5.11) ...\n",
            "Selecting previously unselected package libkpathsea6:amd64.\n",
            "Preparing to unpack .../10-libkpathsea6_2021.20210626.59705-1ubuntu0.2_amd64.deb ...\n",
            "Unpacking libkpathsea6:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n",
            "Selecting previously unselected package libwoff1:amd64.\n",
            "Preparing to unpack .../11-libwoff1_1.0.2-1build4_amd64.deb ...\n",
            "Unpacking libwoff1:amd64 (1.0.2-1build4) ...\n",
            "Selecting previously unselected package dvisvgm.\n",
            "Preparing to unpack .../12-dvisvgm_2.13.1-1_amd64.deb ...\n",
            "Unpacking dvisvgm (2.13.1-1) ...\n",
            "Selecting previously unselected package fonts-lmodern.\n",
            "Preparing to unpack .../13-fonts-lmodern_2.004.5-6.1_all.deb ...\n",
            "Unpacking fonts-lmodern (2.004.5-6.1) ...\n",
            "Selecting previously unselected package fonts-noto-cjk.\n",
            "Preparing to unpack .../14-fonts-noto-cjk_1%3a20220127+repack1-1_all.deb ...\n",
            "Unpacking fonts-noto-cjk (1:20220127+repack1-1) ...\n",
            "Selecting previously unselected package fonts-noto-mono.\n",
            "Preparing to unpack .../15-fonts-noto-mono_20201225-1build1_all.deb ...\n",
            "Unpacking fonts-noto-mono (20201225-1build1) ...\n",
            "Selecting previously unselected package fonts-texgyre.\n",
            "Preparing to unpack .../16-fonts-texgyre_20180621-3.1_all.deb ...\n",
            "Unpacking fonts-texgyre (20180621-3.1) ...\n",
            "Selecting previously unselected package libapache-pom-java.\n",
            "Preparing to unpack .../17-libapache-pom-java_18-1_all.deb ...\n",
            "Unpacking libapache-pom-java (18-1) ...\n",
            "Selecting previously unselected package libcmark-gfm0.29.0.gfm.3:amd64.\n",
            "Preparing to unpack .../18-libcmark-gfm0.29.0.gfm.3_0.29.0.gfm.3-3_amd64.deb ...\n",
            "Unpacking libcmark-gfm0.29.0.gfm.3:amd64 (0.29.0.gfm.3-3) ...\n",
            "Selecting previously unselected package libcmark-gfm-extensions0.29.0.gfm.3:amd64.\n",
            "Preparing to unpack .../19-libcmark-gfm-extensions0.29.0.gfm.3_0.29.0.gfm.3-3_amd64.deb ...\n",
            "Unpacking libcmark-gfm-extensions0.29.0.gfm.3:amd64 (0.29.0.gfm.3-3) ...\n",
            "Selecting previously unselected package libcommons-parent-java.\n",
            "Preparing to unpack .../20-libcommons-parent-java_43-1_all.deb ...\n",
            "Unpacking libcommons-parent-java (43-1) ...\n",
            "Selecting previously unselected package libcommons-logging-java.\n",
            "Preparing to unpack .../21-libcommons-logging-java_1.2-2_all.deb ...\n",
            "Unpacking libcommons-logging-java (1.2-2) ...\n",
            "Selecting previously unselected package libptexenc1:amd64.\n",
            "Preparing to unpack .../22-libptexenc1_2021.20210626.59705-1ubuntu0.2_amd64.deb ...\n",
            "Unpacking libptexenc1:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n",
            "Selecting previously unselected package rubygems-integration.\n",
            "Preparing to unpack .../23-rubygems-integration_1.18_all.deb ...\n",
            "Unpacking rubygems-integration (1.18) ...\n",
            "Selecting previously unselected package ruby3.0.\n",
            "Preparing to unpack .../24-ruby3.0_3.0.2-7ubuntu2.10_amd64.deb ...\n",
            "Unpacking ruby3.0 (3.0.2-7ubuntu2.10) ...\n",
            "Selecting previously unselected package ruby-rubygems.\n",
            "Preparing to unpack .../25-ruby-rubygems_3.3.5-2_all.deb ...\n",
            "Unpacking ruby-rubygems (3.3.5-2) ...\n",
            "Selecting previously unselected package ruby.\n",
            "Preparing to unpack .../26-ruby_1%3a3.0~exp1_amd64.deb ...\n",
            "Unpacking ruby (1:3.0~exp1) ...\n",
            "Selecting previously unselected package rake.\n",
            "Preparing to unpack .../27-rake_13.0.6-2_all.deb ...\n",
            "Unpacking rake (13.0.6-2) ...\n",
            "Selecting previously unselected package ruby-net-telnet.\n",
            "Preparing to unpack .../28-ruby-net-telnet_0.1.1-2_all.deb ...\n",
            "Unpacking ruby-net-telnet (0.1.1-2) ...\n",
            "Selecting previously unselected package ruby-webrick.\n",
            "Preparing to unpack .../29-ruby-webrick_1.7.0-3ubuntu0.1_all.deb ...\n",
            "Unpacking ruby-webrick (1.7.0-3ubuntu0.1) ...\n",
            "Selecting previously unselected package ruby-xmlrpc.\n",
            "Preparing to unpack .../30-ruby-xmlrpc_0.3.2-1ubuntu0.1_all.deb ...\n",
            "Unpacking ruby-xmlrpc (0.3.2-1ubuntu0.1) ...\n",
            "Selecting previously unselected package libruby3.0:amd64.\n",
            "Preparing to unpack .../31-libruby3.0_3.0.2-7ubuntu2.10_amd64.deb ...\n",
            "Unpacking libruby3.0:amd64 (3.0.2-7ubuntu2.10) ...\n",
            "Selecting previously unselected package libsynctex2:amd64.\n",
            "Preparing to unpack .../32-libsynctex2_2021.20210626.59705-1ubuntu0.2_amd64.deb ...\n",
            "Unpacking libsynctex2:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n",
            "Selecting previously unselected package libteckit0:amd64.\n",
            "Preparing to unpack .../33-libteckit0_2.5.11+ds1-1_amd64.deb ...\n",
            "Unpacking libteckit0:amd64 (2.5.11+ds1-1) ...\n",
            "Selecting previously unselected package libtexlua53:amd64.\n",
            "Preparing to unpack .../34-libtexlua53_2021.20210626.59705-1ubuntu0.2_amd64.deb ...\n",
            "Unpacking libtexlua53:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n",
            "Selecting previously unselected package libtexluajit2:amd64.\n",
            "Preparing to unpack .../35-libtexluajit2_2021.20210626.59705-1ubuntu0.2_amd64.deb ...\n",
            "Unpacking libtexluajit2:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n",
            "Selecting previously unselected package libzzip-0-13:amd64.\n",
            "Preparing to unpack .../36-libzzip-0-13_0.13.72+dfsg.1-1.1_amd64.deb ...\n",
            "Unpacking libzzip-0-13:amd64 (0.13.72+dfsg.1-1.1) ...\n",
            "Selecting previously unselected package xfonts-encodings.\n",
            "Preparing to unpack .../37-xfonts-encodings_1%3a1.0.5-0ubuntu2_all.deb ...\n",
            "Unpacking xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Selecting previously unselected package xfonts-utils.\n",
            "Preparing to unpack .../38-xfonts-utils_1%3a7.7+6build2_amd64.deb ...\n",
            "Unpacking xfonts-utils (1:7.7+6build2) ...\n",
            "Selecting previously unselected package lmodern.\n",
            "Preparing to unpack .../39-lmodern_2.004.5-6.1_all.deb ...\n",
            "Unpacking lmodern (2.004.5-6.1) ...\n",
            "Selecting previously unselected package pandoc-data.\n",
            "Preparing to unpack .../40-pandoc-data_2.9.2.1-3ubuntu2_all.deb ...\n",
            "Unpacking pandoc-data (2.9.2.1-3ubuntu2) ...\n",
            "Selecting previously unselected package pandoc.\n",
            "Preparing to unpack .../41-pandoc_2.9.2.1-3ubuntu2_amd64.deb ...\n",
            "Unpacking pandoc (2.9.2.1-3ubuntu2) ...\n",
            "Selecting previously unselected package preview-latex-style.\n",
            "Preparing to unpack .../42-preview-latex-style_12.2-1ubuntu1_all.deb ...\n",
            "Unpacking preview-latex-style (12.2-1ubuntu1) ...\n",
            "Selecting previously unselected package t1utils.\n",
            "Preparing to unpack .../43-t1utils_1.41-4build2_amd64.deb ...\n",
            "Unpacking t1utils (1.41-4build2) ...\n",
            "Selecting previously unselected package teckit.\n",
            "Preparing to unpack .../44-teckit_2.5.11+ds1-1_amd64.deb ...\n",
            "Unpacking teckit (2.5.11+ds1-1) ...\n",
            "Selecting previously unselected package tex-gyre.\n",
            "Preparing to unpack .../45-tex-gyre_20180621-3.1_all.deb ...\n",
            "Unpacking tex-gyre (20180621-3.1) ...\n",
            "Selecting previously unselected package texlive-binaries.\n",
            "Preparing to unpack .../46-texlive-binaries_2021.20210626.59705-1ubuntu0.2_amd64.deb ...\n",
            "Unpacking texlive-binaries (2021.20210626.59705-1ubuntu0.2) ...\n",
            "Selecting previously unselected package texlive-base.\n",
            "Preparing to unpack .../47-texlive-base_2021.20220204-1_all.deb ...\n",
            "Unpacking texlive-base (2021.20220204-1) ...\n",
            "Selecting previously unselected package texlive-fonts-recommended.\n",
            "Preparing to unpack .../48-texlive-fonts-recommended_2021.20220204-1_all.deb ...\n",
            "Unpacking texlive-fonts-recommended (2021.20220204-1) ...\n",
            "Selecting previously unselected package texlive-latex-base.\n",
            "Preparing to unpack .../49-texlive-latex-base_2021.20220204-1_all.deb ...\n",
            "Unpacking texlive-latex-base (2021.20220204-1) ...\n",
            "Selecting previously unselected package libfontbox-java.\n",
            "Preparing to unpack .../50-libfontbox-java_1%3a1.8.16-2_all.deb ...\n",
            "Unpacking libfontbox-java (1:1.8.16-2) ...\n",
            "Selecting previously unselected package libpdfbox-java.\n",
            "Preparing to unpack .../51-libpdfbox-java_1%3a1.8.16-2_all.deb ...\n",
            "Unpacking libpdfbox-java (1:1.8.16-2) ...\n",
            "Selecting previously unselected package texlive-latex-recommended.\n",
            "Preparing to unpack .../52-texlive-latex-recommended_2021.20220204-1_all.deb ...\n",
            "Unpacking texlive-latex-recommended (2021.20220204-1) ...\n",
            "Selecting previously unselected package texlive-pictures.\n",
            "Preparing to unpack .../53-texlive-pictures_2021.20220204-1_all.deb ...\n",
            "Unpacking texlive-pictures (2021.20220204-1) ...\n",
            "Selecting previously unselected package texlive-latex-extra.\n",
            "Preparing to unpack .../54-texlive-latex-extra_2021.20220204-1_all.deb ...\n",
            "Unpacking texlive-latex-extra (2021.20220204-1) ...\n",
            "Selecting previously unselected package texlive-plain-generic.\n",
            "Preparing to unpack .../55-texlive-plain-generic_2021.20220204-1_all.deb ...\n",
            "Unpacking texlive-plain-generic (2021.20220204-1) ...\n",
            "Selecting previously unselected package tipa.\n",
            "Preparing to unpack .../56-tipa_2%3a1.3-21_all.deb ...\n",
            "Unpacking tipa (2:1.3-21) ...\n",
            "Selecting previously unselected package texlive-xetex.\n",
            "Preparing to unpack .../57-texlive-xetex_2021.20220204-1_all.deb ...\n",
            "Unpacking texlive-xetex (2021.20220204-1) ...\n",
            "Setting up fonts-lato (2.0-2.1) ...\n",
            "Setting up fonts-noto-mono (20201225-1build1) ...\n",
            "Setting up libwoff1:amd64 (1.0.2-1build4) ...\n",
            "Setting up libtexlua53:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n",
            "Setting up libijs-0.35:amd64 (0.35-15build2) ...\n",
            "Setting up libtexluajit2:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n",
            "Setting up libfontbox-java (1:1.8.16-2) ...\n",
            "Setting up fonts-noto-cjk (1:20220127+repack1-1) ...\n",
            "Setting up rubygems-integration (1.18) ...\n",
            "Setting up libzzip-0-13:amd64 (0.13.72+dfsg.1-1.1) ...\n",
            "Setting up fonts-urw-base35 (20200910-1) ...\n",
            "Setting up poppler-data (0.4.11-1) ...\n",
            "Setting up tex-common (6.17) ...\n",
            "update-language: texlive-base not installed and configured, doing nothing!\n",
            "Setting up libjbig2dec0:amd64 (0.19-3build2) ...\n",
            "Setting up libteckit0:amd64 (2.5.11+ds1-1) ...\n",
            "Setting up libapache-pom-java (18-1) ...\n",
            "Setting up ruby-net-telnet (0.1.1-2) ...\n",
            "Setting up xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Setting up t1utils (1.41-4build2) ...\n",
            "Setting up libidn12:amd64 (1.38-4ubuntu1) ...\n",
            "Setting up fonts-texgyre (20180621-3.1) ...\n",
            "Setting up libkpathsea6:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n",
            "Setting up ruby-webrick (1.7.0-3ubuntu0.1) ...\n",
            "Setting up libcmark-gfm0.29.0.gfm.3:amd64 (0.29.0.gfm.3-3) ...\n",
            "Setting up fonts-lmodern (2.004.5-6.1) ...\n",
            "Setting up libcmark-gfm-extensions0.29.0.gfm.3:amd64 (0.29.0.gfm.3-3) ...\n",
            "Setting up fonts-droid-fallback (1:6.0.1r16-1.1build1) ...\n",
            "Setting up pandoc-data (2.9.2.1-3ubuntu2) ...\n",
            "Setting up ruby-xmlrpc (0.3.2-1ubuntu0.1) ...\n",
            "Setting up libsynctex2:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n",
            "Setting up libgs9-common (9.55.0~dfsg1-0ubuntu5.11) ...\n",
            "Setting up teckit (2.5.11+ds1-1) ...\n",
            "Setting up libpdfbox-java (1:1.8.16-2) ...\n",
            "Setting up libgs9:amd64 (9.55.0~dfsg1-0ubuntu5.11) ...\n",
            "Setting up preview-latex-style (12.2-1ubuntu1) ...\n",
            "Setting up libcommons-parent-java (43-1) ...\n",
            "Setting up dvisvgm (2.13.1-1) ...\n",
            "Setting up libcommons-logging-java (1.2-2) ...\n",
            "Setting up xfonts-utils (1:7.7+6build2) ...\n",
            "Setting up libptexenc1:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n",
            "Setting up pandoc (2.9.2.1-3ubuntu2) ...\n",
            "Setting up texlive-binaries (2021.20210626.59705-1ubuntu0.2) ...\n",
            "update-alternatives: using /usr/bin/xdvi-xaw to provide /usr/bin/xdvi.bin (xdvi.bin) in auto mode\n",
            "update-alternatives: using /usr/bin/bibtex.original to provide /usr/bin/bibtex (bibtex) in auto mode\n",
            "Setting up lmodern (2.004.5-6.1) ...\n",
            "Setting up texlive-base (2021.20220204-1) ...\n",
            "/usr/bin/ucfr\n",
            "/usr/bin/ucfr\n",
            "/usr/bin/ucfr\n",
            "/usr/bin/ucfr\n",
            "mktexlsr: Updating /var/lib/texmf/ls-R-TEXLIVEDIST... \n",
            "mktexlsr: Updating /var/lib/texmf/ls-R-TEXMFMAIN... \n",
            "mktexlsr: Updating /var/lib/texmf/ls-R... \n",
            "mktexlsr: Done.\n",
            "tl-paper: setting paper size for dvips to a4: /var/lib/texmf/dvips/config/config-paper.ps\n",
            "tl-paper: setting paper size for dvipdfmx to a4: /var/lib/texmf/dvipdfmx/dvipdfmx-paper.cfg\n",
            "tl-paper: setting paper size for xdvi to a4: /var/lib/texmf/xdvi/XDvi-paper\n",
            "tl-paper: setting paper size for pdftex to a4: /var/lib/texmf/tex/generic/tex-ini-files/pdftexconfig.tex\n",
            "Setting up tex-gyre (20180621-3.1) ...\n",
            "Setting up texlive-plain-generic (2021.20220204-1) ...\n",
            "Setting up texlive-latex-base (2021.20220204-1) ...\n",
            "Setting up texlive-latex-recommended (2021.20220204-1) ...\n",
            "Setting up texlive-pictures (2021.20220204-1) ...\n",
            "Setting up texlive-fonts-recommended (2021.20220204-1) ...\n",
            "Setting up tipa (2:1.3-21) ...\n",
            "Setting up texlive-latex-extra (2021.20220204-1) ...\n",
            "Setting up texlive-xetex (2021.20220204-1) ...\n",
            "Setting up rake (13.0.6-2) ...\n",
            "Setting up libruby3.0:amd64 (3.0.2-7ubuntu2.10) ...\n",
            "Setting up ruby3.0 (3.0.2-7ubuntu2.10) ...\n",
            "Setting up ruby (1:3.0~exp1) ...\n",
            "Setting up ruby-rubygems (3.3.5-2) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "Processing triggers for tex-common (6.17) ...\n",
            "Running updmap-sys. This may take some time... done.\n",
            "Running mktexlsr /var/lib/texmf ... done.\n",
            "Building format(s) --all.\n",
            "\tThis may take some time... done.\n",
            "å®‰è£å®Œæˆï¼\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # @title ä½¿ç”¨ pandoc å°‡ Markdown è½‰æ›ç‚º PDF\n",
        "\n",
        "# import os # åŒ¯å…¥ os æ¨¡çµ„\n",
        "\n",
        "# # --- è¨­å®š ---\n",
        "# markdown_input_file = \"simplified_output_by_section.md\" # æ‚¨çš„ç°¡åŒ–å¾Œ Markdown æª”æ¡ˆ\n",
        "# pdf_output_file = \"simplified_output.pdf\"             # æ‚¨æƒ³è¼¸å‡ºçš„ PDF æª”æ¡ˆåç¨±\n",
        "\n",
        "# # --- æª¢æŸ¥è¼¸å…¥æª”æ¡ˆæ˜¯å¦å­˜åœ¨ ---\n",
        "# if not os.path.exists(markdown_input_file):\n",
        "#     print(f\"éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°è¼¸å…¥æª”æ¡ˆ '{markdown_input_file}'ã€‚è«‹ç¢ºèªæª”æ¡ˆå­˜åœ¨æ–¼ Colab ç’°å¢ƒä¸­ã€‚\")\n",
        "# else:\n",
        "#     print(f\"æ‰¾åˆ°è¼¸å…¥æª”æ¡ˆ '{markdown_input_file}'ï¼Œé–‹å§‹è½‰æ›...\")\n",
        "#     # --- åŸ·è¡Œ pandoc æŒ‡ä»¤ ---\n",
        "#     # --pdf-engine=xelatex: ä½¿ç”¨æ”¯æ´ Unicode (ä¸­æ–‡) çš„ XeLaTeX å¼•æ“\n",
        "#     # -V mainfont=\"Noto Sans CJK TC\": æŒ‡å®šæ”¯æ´ç¹é«”ä¸­æ–‡çš„ä¸»å­—å‹ (Noto Sans CJK TC æ˜¯ Colab é€šå¸¸æœ‰çš„)\n",
        "#     # æ‚¨ä¹Ÿå¯ä»¥å˜—è©¦ \"Noto Serif CJK TC\"\n",
        "#     # -o: æŒ‡å®šè¼¸å‡ºæª”å\n",
        "#     !pandoc \"{markdown_input_file}\" -o \"{pdf_output_file}\" --pdf-engine=xelatex -V mainfont=\"Noto Sans CJK TC\"\n",
        "\n",
        "#     # --- æª¢æŸ¥è¼¸å‡ºæª”æ¡ˆæ˜¯å¦æˆåŠŸå»ºç«‹ ---\n",
        "#     if os.path.exists(pdf_output_file):\n",
        "#         print(f\"\\nè½‰æ›æˆåŠŸï¼PDF æª”æ¡ˆå·²å„²å­˜ç‚º '{pdf_output_file}'ã€‚\")\n",
        "#         print(\"æ‚¨ç¾åœ¨å¯ä»¥å¾ Colab çš„æª”æ¡ˆç€è¦½å™¨ä¸‹è¼‰æ­¤ PDF æª”æ¡ˆã€‚\")\n",
        "#         # å¯é¸ï¼šè‡ªå‹•è§¸ç™¼ä¸‹è¼‰\n",
        "#         # from google.colab import files\n",
        "#         # files.download(pdf_output_file)\n",
        "#     else:\n",
        "#         print(\"\\néŒ¯èª¤ï¼šè½‰æ›å¤±æ•—ï¼Œæœªèƒ½ç”Ÿæˆ PDF æª”æ¡ˆã€‚è«‹æª¢æŸ¥ä¸Šæ–¹çš„ pandoc è¼¸å‡ºæ˜¯å¦æœ‰éŒ¯èª¤è¨Šæ¯ã€‚\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSdrqTGLTnwi",
        "outputId": "de7077c4-3387-416f-8706-a444d7aff920"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "æ‰¾åˆ°è¼¸å…¥æª”æ¡ˆ 'simplified_output_by_section.md'ï¼Œé–‹å§‹è½‰æ›...\n",
            "\n",
            "è½‰æ›æˆåŠŸï¼PDF æª”æ¡ˆå·²å„²å­˜ç‚º 'simplified_output.pdf'ã€‚\n",
            "æ‚¨ç¾åœ¨å¯ä»¥å¾ Colab çš„æª”æ¡ˆç€è¦½å™¨ä¸‹è¼‰æ­¤ PDF æª”æ¡ˆã€‚\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # @title ä¸‹è¼‰ PDF æª”æ¡ˆ\n",
        "\n",
        "# from google.colab import files\n",
        "# import os\n",
        "\n",
        "# pdf_filename = \"simplified_output.pdf\" # ç¢ºèªé€™æ˜¯æ‚¨ç”Ÿæˆçš„ PDF æª”å\n",
        "\n",
        "# # æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å­˜åœ¨\n",
        "# if os.path.exists(pdf_filename):\n",
        "#     print(f\"æº–å‚™ä¸‹è¼‰æª”æ¡ˆ: {pdf_filename}\")\n",
        "#     # è§¸ç™¼ç€è¦½å™¨ä¸‹è¼‰\n",
        "#     files.download(pdf_filename)\n",
        "# else:\n",
        "#     print(f\"éŒ¯èª¤ï¼šåœ¨ Colab ä¸­æ‰¾ä¸åˆ°æª”æ¡ˆ '{pdf_filename}'ï¼Œç„¡æ³•ä¸‹è¼‰ã€‚\")\n",
        "#     print(\"è«‹ç¢ºèªä¸Šä¸€æ­¥çš„ pandoc è½‰æ›æ˜¯å¦æˆåŠŸç”Ÿæˆäº†æ­¤æª”æ¡ˆã€‚\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4yKHohJLUc-w",
        "outputId": "31f4f6ea-49da-46da-adaf-2c53c48bb093"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "æº–å‚™ä¸‹è¼‰æª”æ¡ˆ: simplified_output.pdf\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_c4fc1bcd-4e22-4de2-9c47-b2ae2cbaceab\", \"simplified_output.pdf\", 250317)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}