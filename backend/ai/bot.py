import os
import re
import traceback
from langchain.prompts import ChatPromptTemplate
from langchain_ollama.llms import OllamaLLM
from langchain_core.output_parsers import StrOutputParser
from dotenv import load_dotenv
import jieba
# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

MODEL_NAME = os.getenv("MODEL_NAME")
SERVER_URL = os.getenv("SERVER_URL")  # ä¿®æ­£è®Šæ•¸åç¨±æ‹¼å¯«éŒ¯èª¤
SIMPLIFIED_MD_FILENAME = os.getenv("SIMPLIFIED_MD_FILENAME")
TARGET_DESCRIPTION_KEYWORDS=["çµå¡Š", "éç¯©", "é †åº", "å¸æ¿•", "ç¨ åº¦", "é»ç¨ ", "æµå‹•æ€§"] # ä»å¯è¼”åŠ©è­˜åˆ¥ï¼Œä½†æœå°‹ä¸»è¦é åŸæ–™
CHINESE_STOP_WORDS={"çš„", "å’Œ", "èˆ‡", "æˆ–", "äº†", "å‘¢", "å—", "å–”", "å•Š", "é—œæ–¼", "æœ‰é—œ", "è«‹", "è«‹å•", " ", ""}
ALLOWED_WORKSHEET_IDENTIFIERS=[
    "å·¥ä½œè¡¨: 9", "å·¥ä½œè¡¨: 10"
]

llm = OllamaLLM(
    model=MODEL_NAME,   #  modelåç¨±
    base_url=SERVER_URL  # ä¿®æ­£è®Šæ•¸åç¨±
)

# --- SOP æŸ¥è©¢ç›¸é—œå‡½å¼ ---

def load_markdown_sections(filename=SIMPLIFIED_MD_FILENAME):
    """è®€å–ä¸¦è§£æ Markdown æª”æ¡ˆ"""
    sections = []; print(f"æ­£åœ¨å˜—è©¦è¼‰å…¥æª”æ¡ˆ: {filename}")
    try:
        with open(filename, 'r', encoding='utf-8') as f: markdown_content = f.read()
        print(f"æª”æ¡ˆ {filename} è®€å–æˆåŠŸã€‚")
    except FileNotFoundError: print(f"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°æª”æ¡ˆ '{filename}'ã€‚"); return []
    except Exception as e: print(f"âŒ è®€å–æª”æ¡ˆ '{filename}' æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}"); return []
    parts = re.split(r'(## å·¥ä½œè¡¨:.*)', markdown_content)
    for i in range(1, len(parts), 2):
        if i + 1 < len(parts):
            title = parts[i].strip(); content = parts[i + 1].strip().removeprefix("Here is the simplified content:").strip().removeprefix("Here is the simplified output:").strip()
            if title and content: sections.append({"title": title, "content": content})
        elif i < len(parts):
            title = parts[i].strip()
            if title: sections.append({"title": title, "content": ""})
    if not sections: print(f"âš ï¸ è­¦å‘Šï¼šæœªèƒ½å¾æª”æ¡ˆ '{filename}' è§£æå‡ºä»»ä½•å·¥ä½œè¡¨å€å¡Šã€‚")
    return sections

def filter_sections_by_title(all_sections, allowed_identifiers=ALLOWED_WORKSHEET_IDENTIFIERS):
    """æ ¹æ“šæ¨™é¡Œé—œéµå­—éæ¿¾å€å¡Šåˆ—è¡¨"""
    if not all_sections: return []
    return [sec for sec in all_sections if any(allowed_id in sec.get("title", "") for allowed_id in allowed_identifiers)]

# --- *** ä¿®æ”¹ï¼šåŸºæ–¼è¦å‰‡çš„é—œéµå­—æå–å‡½å¼ (æ›´å´é‡åŸæ–™) *** ---
def extract_keywords_rule_based(user_input, target_keywords=TARGET_DESCRIPTION_KEYWORDS):
    """ä½¿ç”¨è¦å‰‡æ‹†åˆ†å’Œæ¯”å°é—œéµå­—ï¼Œä¸»è¦æå–åŸæ–™åç¨±ã€‚"""
    print(f"--- (éšæ®µ0) ä½¿ç”¨è¦å‰‡è§£æè¼¸å…¥ (ä¸»è¦æå–åŸæ–™): '{user_input}' ---")
    input_cleaned = user_input.strip().lower()
    if not input_cleaned:
        return None

    if jieba:
        tokens = list(jieba.cut_for_search(input_cleaned))
    else:
        input_no_punct = re.sub(r'[^\w\s]', ' ', input_cleaned)
        tokens = input_no_punct.split()
    print(f"   åˆ†è©çµæœ: {tokens}")

    potential_materials = []
    identified_characteristics = set() # ä»ç„¶è¨˜éŒ„ç‰¹æ€§è©ï¼Œä½†å®ƒå€‘ä¸ä½œç‚ºä¸»è¦æœå°‹ä¾æ“š

    for token in tokens:
        token_clean = token.strip()
        if not token_clean or token_clean in CHINESE_STOP_WORDS:
            continue

        is_characteristic = False
        # æª¢æŸ¥æ˜¯å¦ç‚ºç‰¹æ€§é—œéµå­— (å®Œå…¨åŒ¹é…)
        for target_char in target_keywords:
            if token_clean == target_char.lower():
                identified_characteristics.add(target_char) # è¨˜éŒ„åŸå§‹å¤§å°å¯«çš„ç‰¹æ€§é—œéµå­—
                is_characteristic = True
                break

        # å¦‚æœä¸æ˜¯ç‰¹æ€§è©ï¼Œä¸”ä¸æ˜¯æ•¸å­—ï¼Œä¸”é•·åº¦å¤§æ–¼0ï¼Œå‰‡è¦–ç‚ºæ½›åœ¨åŸæ–™
        if not is_characteristic:
            if not token_clean.isnumeric() and len(token_clean) > 0:
                potential_materials.append(token_clean)

    materials_list_unique = sorted(list(set(potential_materials)))
    characteristics_list_sorted = sorted(list(identified_characteristics))

    if not materials_list_unique:
        print("âš ï¸ è¦å‰‡è§£æå™¨ï¼šæœªèƒ½è­˜åˆ¥å‡ºä»»ä½•æ½›åœ¨çš„åŸæ–™åç¨±ã€‚")
        # å³ä½¿æ²’æœ‰åŸæ–™ï¼Œä¹Ÿè¿”å›çµæ§‹ï¼Œè®“å¾ŒçºŒåˆ¤æ–·
        # return {"åŸæ–™åç¨±": [], "ç‰¹æ€§æè¿°": characteristics_list_sorted}
        return None # æˆ–è€…ç›´æ¥è¿”å›Noneï¼Œå¦‚æœåš´æ ¼è¦æ±‚å¿…é ˆæœ‰åŸæ–™åæ‰èƒ½æŸ¥è©¢

    result = {"åŸæ–™åç¨±": materials_list_unique, "ç‰¹æ€§æè¿°": characteristics_list_sorted}
    print(f"   è¦å‰‡è§£æçµæœ (ä¸»è¦ç‚ºåŸæ–™ï¼Œè¼”åŠ©ç‰¹æ€§): {result}")
    return result
# --- *** å‡½å¼çµæŸ *** ---

def search_sections(sections_to_search, keywords):
    """åœ¨å·²éæ¿¾çš„å€å¡Šä¸­åˆæ­¥ç¯©é¸åŒ…å«ã€åŸæ–™åç¨±ã€‘é—œéµå­—çš„å·¥ä½œè¡¨"""
    relevant_sections = []
    if not keywords:  # å¢åŠ å° None çš„æª¢æŸ¥
        print("â„¹ï¸ é—œéµå­—å°è±¡ç‚ºç©ºï¼Œç„¡æ³•åŸ·è¡Œæœå°‹ã€‚")
        return []
        
    # *** ä¿®æ”¹ï¼šåªä½¿ç”¨åŸæ–™åç¨±é€²è¡Œåˆæ­¥æœå°‹ ***
    material_keywords = keywords.get("åŸæ–™åç¨±", [])
    all_keywords = [kw for kw in material_keywords if kw and isinstance(kw, str)]

    if not all_keywords:
        print("â„¹ï¸ æ²’æœ‰æœ‰æ•ˆçš„ã€åŸæ–™åç¨±ã€‘é—œéµå­—å¯ä¾›æœå°‹ã€‚")
        return []
    print(f"DEBUG: æ­£åœ¨ä½¿ç”¨ä»¥ä¸‹ä»»ä¸€ã€åŸæ–™åç¨±ã€‘é—œéµå­—æœå°‹: {all_keywords}")
    for section in sections_to_search:
        text_to_search = section.get("title", "") + section.get("content", "")
        if any(keyword.lower() in text_to_search.lower() for keyword in all_keywords): # å¿½ç•¥å¤§å°å¯«æœå°‹
            relevant_sections.append(section)
    return relevant_sections

# --- *** ä¿®æ”¹ï¼šç¬¬ä¸€éšæ®µ LLM Promptï¼Œå¼·èª¿å°ç¯„åœã€åƒ…åŸæ–™ç›¸é—œ *** ---
def extract_relevant_text(llm, sections, keywords):
    """(ç¬¬ä¸€éšæ®µ LLM) ä½¿ç”¨æ¥µåº¦èšç„¦çš„ Prompt æå–èˆ‡æŒ‡å®šã€åŸæ–™åç¨±ã€‘æœ€ç›´æ¥ç›¸é—œçš„ã€å°ç¯„åœæ–‡å­—ç‰‡æ®µã€‘ã€‚"""
    if not keywords:  # å¢åŠ å° None çš„æª¢æŸ¥
        print("â„¹ï¸ extract_relevant_text: é—œéµå­—å°è±¡ç‚ºç©ºã€‚")
        return []
        
    material_names = keywords.get('åŸæ–™åç¨±', [])
    if not material_names:
        print("â„¹ï¸ extract_relevant_text: æŸ¥è©¢ä¸­æœªæä¾›åŸæ–™åç¨±ã€‚")
        return []
    material_name_str = "ã€".join(material_names)

    # ç‰¹æ€§æè¿°å¯ä»¥ä½œç‚ºè¼”åŠ©ä¸Šä¸‹æ–‡ï¼Œå¹«åŠ©LLMç†è§£ï¼Œä½†æå–ç„¦é»æ˜¯åŸæ–™
    characteristics_list = keywords.get('ç‰¹æ€§æè¿°', [])
    description_keywords_str = ', '.join(characteristics_list)

    if characteristics_list:
        # é›–ç„¶ä¸»è¦æœåŸæ–™ï¼Œä½†å‘ŠçŸ¥LLMä½¿ç”¨è€…ä¹Ÿæåˆ°äº†é€™äº›ç‰¹æ€§ï¼Œå¯èƒ½æœ‰åŠ©æ–¼LLMç†è§£ä¸Šä¸‹æ–‡
        keywords_str_display_for_prompt = f"ä¸»è¦æŸ¥è©¢çš„åŸæ–™åç¨±ï¼šã€{material_name_str}ã€‘ã€‚(ä½¿ç”¨è€…åŒæ™‚æåŠäº†é€™äº›ç›¸é—œè©å½™ï¼Œä¾›åƒè€ƒï¼š{description_keywords_str})"
    else:
        keywords_str_display_for_prompt = f"ä¸»è¦æŸ¥è©¢çš„åŸæ–™åç¨±ï¼šã€{material_name_str}ã€‘"

    prompt_base = """
        ä½ çš„ä»»å‹™æ˜¯ä½œç‚ºä¸€å€‹ç²¾ç¢ºçš„æ–‡å­—æå–å™¨ã€‚
        åœ¨ä¸‹æ–¹æä¾›çš„ã€Œå·¥ä½œè¡¨å…§å®¹ã€ä¸­ï¼Œåƒ…æ‰¾å‡ºèˆ‡ã€Œä¸»è¦æŸ¥è©¢çš„åŸæ–™åç¨±ã€æœ€ç›´æ¥ç›¸é—œçš„ã€ä¸€å€‹æˆ–å¤šå€‹ç°¡çŸ­æ–‡å­—ç‰‡æ®µã€å¥å­æˆ–åˆ—è¡¨é …ã€‘ã€‚

        {keywords_str_display_for_prompt}

        å·¥ä½œè¡¨å…§å®¹ï¼š
        ```markdown
        {text}
        ```

        è¼¸å‡ºè¦æ±‚ï¼š
        1.  ç²¾ç¢ºå®šä½åˆ°åŒ…å«ã€Œä¸»è¦æŸ¥è©¢çš„åŸæ–™åç¨±ã€çš„å¥å­ã€æ“ä½œæ­¥é©Ÿæˆ–å…¶éå¸¸ç·Šå¯†çš„ä¸Šä¸‹æ–‡ã€‚
        2.  **ã€ç›´æ¥è¼¸å‡ºã€‘** æ‰¾åˆ°çš„ç›¸é—œå…§å®¹ã€‚æå–çš„ç¯„åœæ‡‰ç›¡å¯èƒ½å°è€Œç²¾æº–ã€‚ä¾‹å¦‚ï¼Œåƒ…æå–åŒ…å«è©²åŸæ–™åç¨±çš„é‚£å€‹æ­¥é©Ÿã€é‚£å¥æè¿°æˆ–ç›¸é—œçš„åˆ—è¡¨é …ã€‚
        3.  **ã€é¿å…æå–ã€‘** éé•·çš„æ®µè½æˆ–æ•´å€‹å­æ¨™é¡Œä¸‹çš„æ‰€æœ‰å…§å®¹ï¼Œé™¤éè©²åŸæ–™åç¨±åœ¨æ•´å€‹æ®µè½/å­æ¨™é¡Œä¸­éƒ½é«˜åº¦ç›¸é—œä¸”å…§å®¹ä¸å¯åˆ†å‰²ã€‚å„ªå…ˆæå–æœ€å°çš„æœ‰æ•ˆè³‡è¨Šå–®ä½ã€‚
        4.  **ã€çµ•å°ç¦æ­¢ã€‘** åŒ…å« '## å·¥ä½œè¡¨: ...' é€™å€‹æœ€é«˜ç´šåˆ¥çš„æ¨™é¡Œã€‚
        5.  **ã€çµ•å°ç¦æ­¢ã€‘** åŒ…å«ä»»ä½• 'è£½è¡¨æ—¥æœŸ', 'è£½è¡¨äºº' ç­‰é è…³è³‡è¨Šã€‚
        6.  **ã€çµ•å°ç¦æ­¢ã€‘** é€²è¡Œä»»ä½•æ‘˜è¦ã€æ”¹å¯«æˆ–æ·»åŠ ä»»ä½•ã€è§£é‡‹æ€§æ–‡å­—ã€å‰è¨€ã€çµèªã€‘(ä¾‹å¦‚ä¸è¦èªª "ä»¥ä¸‹æ˜¯æ‰¾åˆ°çš„å…§å®¹ï¼š" æˆ– "æ ¹æ“šé—œéµå­—..." ç­‰)ã€‚è¼¸å‡ºçµæœå¿…é ˆç›´æ¥å°±æ˜¯æå–çš„å…§å®¹æœ¬èº«ã€‚
        7.  å¦‚æœåœ¨æ­¤ã€Œå·¥ä½œè¡¨å…§å®¹ã€ä¸­æ‰¾ä¸åˆ°ç›´æ¥èˆ‡ã€Œä¸»è¦æŸ¥è©¢çš„åŸæ–™åç¨±ã€ç›¸é—œçš„å…§å®¹ï¼Œã€åªè¼¸å‡ºã€‘ä»¥ä¸‹å›ºå®šæ–‡å­—ï¼šã€Œåœ¨æ­¤æ–‡ä»¶ä¸­æœªæ‰¾åˆ°èˆ‡æ­¤åŸæ–™ç›´æ¥ç›¸é—œçš„å…§å®¹ã€ã€‚
        8.  å¿…é ˆä½¿ç”¨ã€ç¹é«”ä¸­æ–‡ã€‘è¼¸å‡º (å¦‚æœæå–çš„å…§å®¹æœ¬èº«æ˜¯ä¸­æ–‡)ã€‚
        å†æ¬¡å¼·èª¿ï¼šç›´æ¥è¼¸å‡ºæå–çš„ã€èˆ‡åŸæ–™ç›´æ¥ç›¸é—œçš„ã€å°ç¯„åœã€‘å…§å®¹ï¼Œä¸è¦åŠ ä»»ä½•å…¶ä»–æ–‡å­—ã€‚
        """

    # ä½¿ç”¨å¸¶æœ‰æ‰€æœ‰é æœŸè¼¸å…¥è®Šé‡çš„æ¨¡æ¿
    prompt_template = ChatPromptTemplate.from_template(prompt_base)
    chain = prompt_template | llm | StrOutputParser()

    relevant_texts = []
    # é¡¯ç¤ºä¸€æ¬¡å®Œæ•´çš„ keywords_str_display_for_promptï¼Œå› ç‚ºå®ƒå°æ–¼æ‰€æœ‰ section éƒ½æ˜¯ä¸€æ¨£çš„
    print(f"--- (éšæ®µ1) ä½¿ç”¨ LLM å®šä½ä¸¦æå–å­ç« ç¯€ï¼Œä½¿ç”¨çš„æŸ¥è©¢ä¸Šä¸‹æ–‡: {keywords_str_display_for_prompt} ---")

    for section in sections:
        section_title = section.get("title", "ç„¡æ¨™é¡Œå€å¡Š")
        section_content = section.get("content", "")
        section_full_text = f"{section_content}" # ç°¡åŒ–ï¼Œåªå‚³éå…§å®¹è®“ LLM å°ˆæ³¨

        print(f"   æ­£åœ¨è™•ç†å€å¡Š: {section_title} (æœå°‹åŸæ–™ '{material_name_str}')...")
        try:
            relevant_text = chain.invoke({
                "keywords_str_display_for_prompt": keywords_str_display_for_prompt,
                "text": section_full_text
            })
            relevant_text = relevant_text.strip()

            # åŸºæœ¬çš„å¾Œè™•ç†ï¼Œç§»é™¤ LLM å¯èƒ½ä¸å°å¿ƒåŠ ä¸Šçš„ markdown æ¨™ç±¤æˆ–å¼•å°è©
            if relevant_text.startswith("æ ¹æ“šé—œéµå­—") or relevant_text.startswith("ä»¥ä¸‹æ˜¯æ‰¾åˆ°"):
                lines = relevant_text.splitlines()
                relevant_text = "\n".join(lines[1:]).strip()
            if relevant_text.startswith("```markdown"):
                relevant_text = relevant_text.removeprefix("```markdown").removesuffix("```").strip()

            is_found_message = "åœ¨æ­¤æ–‡ä»¶ä¸­æœªæ‰¾åˆ°èˆ‡æ­¤åŸæ–™ç›´æ¥ç›¸é—œçš„å…§å®¹" in relevant_text

            if is_found_message or not relevant_text: # å¦‚æœæ˜¯ "æœªæ‰¾åˆ°" æˆ–æå–çµæœç‚ºç©º
                print(f"     â†³ åœ¨å€å¡Š '{section_title}' ä¸­æœªæ‰¾åˆ°æˆ–æå–åˆ°ç©ºå…§å®¹é—œæ–¼åŸæ–™ '{material_name_str}'ã€‚LLMå›æ‡‰: '{relevant_text}'")
                relevant_texts.append({"title": section_title, "text": relevant_text if relevant_text else "å…§å®¹æå–ç‚ºç©º", "found": False})
            else:
                print(f"     â†³ å¾ '{section_title}' æå–åˆ°å…§å®¹ (å‰100å­—): \"{relevant_text[:100].replace(os.linesep, ' ')}...\"")
                relevant_texts.append({"title": section_title, "text": relevant_text, "found": True})

        except Exception as e:
            print(f"âŒ å¾å€å¡Š '{section_title}' æå–æ™‚å‡ºéŒ¯: {e}")
            traceback.print_exc()
            relevant_texts.append({"title": section_title, "text": "LLM æå–å¤±æ•—", "found": False})
    return relevant_texts
# --- *** å‡½å¼çµæŸ *** ---

# --- *** ä¿®æ”¹ï¼šç¬¬äºŒéšæ®µ LLM Promptï¼Œå¼·èª¿åŸæ–‡å‘ˆç¾ã€ä¸ä¿®é£¾ã€çµ±ä¸€åˆ—è¡¨ *** ---
def synthesize_results(llm, keywords, extracted_texts):
    """(ç¬¬äºŒéšæ®µ LLM) å°‡æå–å‡ºçš„ã€å°ç¯„åœæ–‡å­—ç‰‡æ®µã€‘æ•´åˆæˆã€æ¥µç°¡ä¸”åŸæ–‡å‘ˆç¾çš„çµ±ä¸€æ ¼å¼åˆ—è¡¨ã€‘ã€‚"""
    if not keywords:  # å¢åŠ å° None çš„æª¢æŸ¥
        return "ç„¡æ•ˆçš„é—œéµå­—ï¼Œç„¡æ³•æ•´åˆçµæœã€‚"
        
    if not extracted_texts:
        return "æœªèƒ½æå–åˆ°ä»»ä½•ç›¸é—œå…§å®¹ä»¥ä¾›æ•´åˆã€‚"

    valid_extractions = [
        item['text'] for item in extracted_texts
        if item.get("found", False) and \
           "LLM æå–å¤±æ•—" not in item['text'] and \
           "åœ¨æ­¤æ–‡ä»¶ä¸­æœªæ‰¾åˆ°èˆ‡æ­¤åŸæ–™ç›´æ¥ç›¸é—œçš„å…§å®¹" not in item['text'] and \
           item['text'] and item['text'].strip() # ç¢ºä¿æ–‡æœ¬æœ¬èº«ä¸ç‚ºç©ºæˆ–åƒ…åŒ…å«ç©ºç™½
    ]

    if not valid_extractions:
        material_names_list = keywords.get('åŸæ–™åç¨±', [])
        material_name_str = "ã€".join(material_names_list) if material_names_list else "æ‰€æŸ¥è©¢çš„é …ç›®"
        # æª¢æŸ¥æ˜¯å¦æ‰€æœ‰å˜—è©¦æå–çš„å€å¡Šéƒ½è¿”å›äº† "æœªæ‰¾åˆ°" æˆ–é¡ä¼¼è¨Šæ¯
        all_attempted_but_not_found = all(
            not item.get("found") or \
            "åœ¨æ­¤æ–‡ä»¶ä¸­æœªæ‰¾åˆ°èˆ‡æ­¤åŸæ–™ç›´æ¥ç›¸é—œçš„å…§å®¹" in item['text'] or \
            item['text'].strip() == ""
            for item in extracted_texts
        )
        if extracted_texts and all_attempted_but_not_found: # ç¢ºä¿ extracted_texts ä¸æ˜¯ç©ºçš„
            return f"å·²æª¢æŸ¥æ‰€æœ‰ç›¸é—œSOPæ–‡ä»¶å€å¡Šï¼Œä½†å‡æœªæ‰¾åˆ°é—œæ–¼åŸæ–™ã€{material_name_str}ã€‘çš„ç›´æ¥æ“ä½œèªªæ˜æˆ–æ³¨æ„äº‹é …ã€‚"
        return f"é›–ç„¶åˆæ­¥å®šä½åˆ°å¯èƒ½ç›¸é—œçš„SOPå€å¡Šï¼Œä½†åœ¨å…¶ä¸­æœªèƒ½æ‰¾åˆ°é—œæ–¼åŸæ–™ã€{material_name_str}ã€‘çš„å…·é«”æ“ä½œèªªæ˜æˆ–æ³¨æ„äº‹é …ã€‚"


    print(f"\nğŸ”„ (éšæ®µ2) æ­£åœ¨æ•´åˆ {len(valid_extractions)} ä»½æå–çš„é‡é»å…§å®¹ä¸¦çµ±ä¸€æ ¼å¼ (åŠ›æ±‚åŸæ–‡ã€ç°¡æ½”)...")
    # å°‡æ‰€æœ‰æå–ç‰‡æ®µçµ„åˆï¼Œç”¨ç‰¹æ®Šåˆ†éš”ç¬¦æ˜ç¢ºå‘ŠçŸ¥LLMé€™æ˜¯ä¸åŒä¾†æºçš„ç¨ç«‹ç‰‡æ®µ
    combined_extracted_text = "\n\nç‰‡æ®µåˆ†éš”ç·š (è«‹å°‡æ¯å€‹ç‰‡æ®µè¦–ç‚ºç¨ç«‹è³‡è¨Šä¾†æº)\n\n".join(valid_extractions)

    material_names_list = keywords.get('åŸæ–™åç¨±', [])
    material_name = "ã€".join(material_names_list) if material_names_list else "æœªæŒ‡å®šåŸæ–™"

    characteristics_list = keywords.get('ç‰¹æ€§æè¿°', [])
    if characteristics_list:
        keywords_str_for_prompt = f"ä½¿ç”¨è€…ä¸»è¦æŸ¥è©¢çš„åŸæ–™åç¨±ç‚ºã€{material_name}ã€‘ã€‚(ä½¿ç”¨è€…æŸ¥è©¢æ™‚æåŠçš„ç›¸é—œè©å½™ï¼Œä¾›æ‚¨ç†è§£ä¸Šä¸‹æ–‡ï¼š{', '.join(characteristics_list)})"
    else:
        keywords_str_for_prompt = f"ä½¿ç”¨è€…ä¸»è¦æŸ¥è©¢çš„åŸæ–™åç¨±ç‚ºã€{material_name}ã€‘ã€‚"

    synthesis_prompt_template_str = """
        æ‚¨æ˜¯ä¸€ä½SOPå…§å®¹æ•´ç†å“¡ã€‚æ‚¨çš„ä»»å‹™æ˜¯å°‡ä¸‹æ–¹æä¾›çš„ã€å·²å¾SOPæ–‡ä»¶ä¸­æå–å‡ºçš„ã€èˆ‡æŒ‡å®šåŸæ–™ç›¸é—œçš„ã€å¤šå€‹ç¨ç«‹çš„ç°¡çŸ­æ–‡å­—ç‰‡æ®µã€‘ï¼Œæ•´ç†æˆä¸€å€‹ã€æ¥µç°¡çš„ã€çµ±ä¸€æ ¼å¼çš„æ•¸å­—ç·¨è™Ÿåˆ—è¡¨ã€‘ã€‚

        {keywords_str_for_prompt}

        å·²æå–çš„ç›¸é—œSOPç‰‡æ®µ (é€™äº›ç‰‡æ®µä¾†è‡ªä¸åŒåœ°æ–¹ï¼Œè«‹å°‡å®ƒå€‘è¦–ç‚ºç¨ç«‹çš„è³‡è¨Šé»ï¼Œä½¿ç”¨ã€Œç‰‡æ®µåˆ†éš”ç·šã€éš”é–‹)ï¼š
        ---
        {combined_extracted_text}
        ---

        æ‚¨çš„ä»»å‹™èˆ‡è¼¸å‡ºè¦æ±‚ï¼š
        1.  ä»”ç´°é–±è®€æ‰€æœ‰ã€Œå·²æå–çš„ç›¸é—œSOPç‰‡æ®µã€ã€‚æ¯å€‹ç‰‡æ®µéƒ½æ˜¯é—œæ–¼ä¸Šè¿°ã€Œä¸»è¦æŸ¥è©¢çš„åŸæ–™åç¨±ã€çš„ç›´æ¥ç›¸é—œå…§å®¹ã€‚
        2.  **ã€æ ¸å¿ƒä»»å‹™ã€‘ï¼šå°‡é€™äº›ç‰‡æ®µä¸­çš„ã€æ¯ä¸€å€‹ç¨ç«‹çš„è³‡è¨Šé»ã€æ“ä½œæ­¥é©Ÿã€æˆ–æ³¨æ„äº‹é …ã€‘æ•´ç†å‡ºä¾†ï¼Œä½œç‚ºåˆ—è¡¨ä¸­çš„ä¸€å€‹ç¨ç«‹é …ç›®ã€‚**
        3.  **ã€æ ¼å¼çµ±ä¸€ã€‘ï¼š** ä½¿ç”¨å¾ 1 é–‹å§‹çš„æ•¸å­—ç·¨è™Ÿåˆ—è¡¨ (æ ¼å¼å¦‚ï¼š1., 2., 3., ...)ã€‚
        4.  **ã€åŸæ–‡å‘ˆç¾ã€‘ï¼š** ç›¡æœ€å¤§å¯èƒ½ã€ç›´æ¥ä½¿ç”¨ã€‘æå–ç‰‡æ®µä¸­çš„ã€åŸæ–‡è¡¨è¿°ã€‘ã€‚**ã€åš´æ ¼ç¦æ­¢ã€‘ä»»ä½•å½¢å¼çš„æ”¹å¯«ã€æ‘˜è¦ã€è§£é‡‹ã€æ­¸ç´æˆ–æ·»åŠ æ‚¨è‡ªå·±çš„æ–‡å­—æˆ–è©•è«–ã€‚** æ‚¨çš„å·¥ä½œæ˜¯ã€Œå½™æ•´åŸæ–‡ã€å’Œã€Œæ ¼å¼åŒ–ç‚ºåˆ—è¡¨ã€ï¼Œè€Œä¸æ˜¯ã€Œå†å‰µä½œã€æˆ–ã€Œè§£é‡‹ã€ã€‚
        5.  å¦‚æœåŸå§‹ç‰‡æ®µæœ¬èº«å°±æ˜¯ä¸€å€‹çŸ­åˆ—è¡¨æˆ–åŒ…å«å­æ­¥é©Ÿï¼Œè«‹åœ¨æ–°çš„æ•¸å­—ç·¨è™Ÿä¸‹ï¼Œç›¡å¯èƒ½ä¿æŒå…¶åŸå§‹çµæ§‹ï¼Œä¾‹å¦‚ä½¿ç”¨ç¸®æ’å’Œ '-' æˆ– '*' ä¾†è¡¨ç¤ºå±¤ç´šé—œä¿‚ã€‚
        6.  **ã€æ¥µç°¡è¼¸å‡ºã€‘ï¼š** æ‚¨çš„æœ€çµ‚è¼¸å‡ºã€å¿…é ˆç›´æ¥æ˜¯é€™å€‹æ•¸å­—ç·¨è™Ÿåˆ—è¡¨æœ¬èº«ã€‘ã€‚**ã€åš´æ ¼ç¦æ­¢ã€‘** åŒ…å«ä»»ä½•å‰è¨€ï¼ˆä¾‹å¦‚ "å¥½çš„ï¼Œé€™æ˜¯æ•´ç†å¾Œçš„åˆ—è¡¨ï¼š" æˆ– "é—œæ–¼æ‚¨æŸ¥è©¢çš„..."ï¼‰ã€æ¨™é¡Œã€å¼•å°èªæˆ–çµèªã€‚
        7.  å¦‚æœå¤šå€‹ç‰‡æ®µæè¿°çš„æ˜¯ã€å®Œå…¨ç›¸åŒæˆ–å¹¾ä¹å®Œå…¨é‡è¤‡ã€‘çš„è³‡è¨Šï¼Œè«‹åªé¸æ“‡å…¶ä¸­ä¸€å€‹æœ€æ¸…æ™°çš„è¡¨è¿°æ”¾å…¥åˆ—è¡¨ï¼Œä»¥é¿å…ä¸å¿…è¦çš„å†—é¤˜ã€‚ä½†è‹¥æœ‰äº›å¾®å·®ç•°æˆ–è£œå……ï¼Œå¯§å¯ä¿ç•™å…©è€…ï¼Œé¿å…è³‡è¨Šéºå¤±ã€‚**åˆ¤æ–·é‡è¤‡çš„æ¨™æº–è¦éå¸¸åš´æ ¼ã€‚**
        8.  ä½¿ç”¨**ç¹é«”ä¸­æ–‡**ã€‚
        9.  å³ä½¿æœ€çµ‚æ•´ç†å‡ºä¾†çš„è³‡è¨Šé»å¾ˆå°‘ï¼ˆä¾‹å¦‚åªæœ‰ä¸€å…©æ¢ï¼‰ï¼Œä¹Ÿç›´æ¥æŒ‰åˆ—è¡¨æ ¼å¼è¼¸å‡ºã€‚
        10. **ã€çµ•å°ç¦æ­¢ã€‘** åœ¨è¼¸å‡ºä¸­æåŠä»»ä½•ã€Œå·¥ä½œè¡¨æ¨™é¡Œã€ã€ã€ŒSOPæ–‡ä»¶ä¾†æºã€æˆ–ã€Œç‰‡æ®µåˆ†éš”ç·šã€ç­‰å…ƒä¿¡æ¯ã€‚
        11. **ã€çµ•å°ç¦æ­¢ã€‘** æ·»åŠ ä»»ä½•è¶…å‡ºæ‰€æä¾›ç‰‡æ®µå…§å®¹ä¹‹å¤–çš„è³‡è¨Šæˆ–å»ºè­°ã€‚

        è«‹ç›´æ¥é–‹å§‹è¼¸å‡ºåˆ—è¡¨ï¼š
        """
    synthesis_prompt_template = ChatPromptTemplate.from_template(synthesis_prompt_template_str)
    synthesis_chain = synthesis_prompt_template | llm | StrOutputParser()

    try:
        final_response = synthesis_chain.invoke({
            "keywords_str_for_prompt": keywords_str_for_prompt,
            "combined_extracted_text": combined_extracted_text
        })
        final_response = final_response.strip()

        # å¾Œè™•ç†ï¼šå¼·åŠ›ç§»é™¤å·²çŸ¥çš„å‰ç¶´ (LLMæœ‰æ™‚é‚„æ˜¯æœƒå¿ä¸ä½åŠ ä¸Š)
        unwanted_prefixes = [
            "å¥½çš„ï¼Œé€™æ˜¯æ•´ç†å¾Œçš„åˆ—è¡¨ï¼š", "é€™æ˜¯ç‚ºæ‚¨æ•´ç†çš„åˆ—è¡¨ï¼š", "ä»¥ä¸‹æ˜¯æ•´ç†å¾Œçš„åˆ—è¡¨ï¼š",
            "æ ¹æ“šæ‚¨æä¾›çš„è³‡è¨Šï¼š", "é—œæ–¼æ‚¨æŸ¥è©¢çš„åŸæ–™", "é—œæ–¼æ‚¨æŸ¥è©¢çš„",
            "Okay, here is the list:", "Here is the list:", "Here is the output:",
            "é€™æ˜¯æ‚¨æŸ¥è©¢çš„çµæœï¼š", "é€™æ˜¯ç›¸é—œçš„è³‡è¨Šï¼š", "æŸ¥è©¢çµæœå¦‚ä¸‹ï¼š",
            "1. " # æœ‰æ™‚LLMæœƒè‡ªå·±åŠ åˆ—è¡¨ç·¨è™Ÿï¼Œä½†æˆ‘å€‘çš„promptè¦æ±‚å®ƒå¾1é–‹å§‹ï¼Œæ‰€ä»¥å¦‚æœå®ƒä»¥ "1. " é–‹é ­ï¼Œé€šå¸¸æ˜¯æ­£ç¢ºçš„
        ]
        # ç§»é™¤å‰ç¶´çš„é‚è¼¯éœ€è¦å°å¿ƒï¼Œé¿å…èª¤åˆªæ­£å¸¸çš„åˆ—è¡¨é–‹é ­
        cleaned_response = final_response
        for prefix in unwanted_prefixes:
            # ç¢ºä¿ prefix ä¸æ˜¯ "1. " é€™ç¨®å¯èƒ½èˆ‡æ­£ç¢ºè¼¸å‡ºè¡çªçš„
            if prefix.strip().startswith("1.") and cleaned_response.strip().startswith("1."):
                continue
            if final_response.lower().startswith(prefix.lower()):
                # è¨ˆç®—å‰ç¶´å¯¦éš›é•·åº¦ï¼Œç§»é™¤æ­¤å‰ç¶´åŠå…¶å¾Œçš„ç©ºç™½å’Œå†’è™Ÿ
                prefix_len = len(prefix)
                temp_response = final_response[prefix_len:].lstrip("ï¼š: ").strip()
                # åªæœ‰ç•¶ç§»é™¤å¾Œé‚„æœ‰å…§å®¹ï¼Œæˆ–è€…ç§»é™¤çš„æ˜¯æ˜ç¢ºçš„å¼•å°èªæ™‚æ‰æ›´æ–°
                if temp_response or prefix.lower() not in ["1."]: # é¿å… "1. " è¢«éŒ¯èª¤ç§»é™¤å¾Œè®Šç©º
                    cleaned_response = temp_response
                    print(f"   DEBUG: (Synthesize) ç§»é™¤äº†ä¸éœ€è¦çš„å‰ç¶´ '{prefix}'")
                    break # æ‰¾åˆ°ä¸¦ç§»é™¤ä¸€å€‹å°±ä¸å†æª¢æŸ¥å…¶ä»–å‰ç¶´

        # å†ä¸€æ¬¡æª¢æŸ¥æ˜¯å¦ä»¥æ•¸å­—åˆ—è¡¨é–‹é ­ï¼Œå¦‚æœä¸æ˜¯ï¼Œä¸”å…§å®¹çœ‹èµ·ä¾†åƒå¼•è¨€ï¼Œå˜—è©¦ç§»é™¤
        lines = cleaned_response.splitlines()
        if lines and not re.match(r"^\s*\d+\.\s*", lines[0]): # å¦‚æœç¬¬ä¸€è¡Œä¸æ˜¯ "æ•¸å­—." é–‹é ­
            # ä¸¦ä¸”ç¬¬ä¸€è¡Œçœ‹èµ·ä¾†åƒæ˜¯ä¸€å¥ç°¡çŸ­çš„å¼•è¨€è€Œä¸æ˜¯å¯¦éš›å…§å®¹
            first_line_lower = lines[0].lower()
            is_likely_intro = True
            # å¦‚æœç¬¬ä¸€è¡ŒåŒ…å«åŸæ–™åï¼Œå¯èƒ½ä¸æ˜¯å¼•è¨€ï¼Œé™¤ééå¸¸çŸ­
            for mat_kw in keywords.get("åŸæ–™åç¨±", []):
                if mat_kw.lower() in first_line_lower:
                    if len(lines[0]) > 20 : # å¦‚æœåŒ…å«åŸæ–™åä½†è¼ƒé•·ï¼Œå¯èƒ½å°±æ˜¯å…§å®¹
                        is_likely_intro = False
                    break
            if len(lines[0]) > 50: # å¦‚æœç¬¬ä¸€è¡Œå¾ˆé•·ï¼Œä¸åƒå¼•è¨€
                is_likely_intro = False

            if is_likely_intro and len(lines) > 1 : # ç¢ºä¿æœ‰å¤šè¡Œï¼Œç§»é™¤å¾Œé‚„æœ‰å…§å®¹
                print(f"   DEBUG: (Synthesize) è¼¸å‡ºé¦–è¡Œ '{lines[0]}' ä¸¦éæ¨™æº–åˆ—è¡¨é …ï¼Œå˜—è©¦ç§»é™¤ä½œç‚ºå¼•è¨€ã€‚")
                cleaned_response = "\n".join(lines[1:]).strip()
            elif is_likely_intro and len(lines) == 1: # åªæœ‰ä¸€è¡Œä¸”ä¸åƒåˆ—è¡¨é …
                print(f"   DEBUG: (Synthesize) è¼¸å‡ºåªæœ‰ä¸€è¡Œ '{lines[0]}' ä¸”ä¸¦éæ¨™æº–åˆ—è¡¨é …ï¼Œå¯èƒ½LLMæœªéµå¾ªæŒ‡ç¤ºã€‚")
                # æ­¤æ™‚å¯ä»¥è€ƒæ…®è¿”å›ä¸€å€‹å›ºå®šè¨Šæ¯æˆ–åŸå§‹æå–å…§å®¹çš„ç°¡å–®çµ„åˆ
                # cleaned_response = "LLMæœªèƒ½æŒ‰è¦æ±‚æ ¼å¼åŒ–è³‡è¨Šï¼Œè«‹é‡è©¦æˆ–æª¢æŸ¥æ—¥èªŒã€‚"

        if not cleaned_response.strip() and valid_extractions:
            print(f"   âš ï¸ è­¦å‘Š: (Synthesize) LLM æœ€çµ‚è¼¸å‡ºç‚ºç©ºæˆ–åƒ…å«ç©ºç™½ï¼Œä½†ä¹‹å‰æœ‰ {len(valid_extractions)} æ¢æœ‰æ•ˆæå–å…§å®¹ã€‚å¯èƒ½æ•´åˆå¤±æ•—ã€‚")
            return f"å·²æ‰¾åˆ°é—œæ–¼åŸæ–™ã€{material_name}ã€‘çš„ç›¸é—œè³‡è¨Šï¼Œä½†åœ¨æœ€çµ‚æ•´ç†æ ¼å¼æ™‚å‡ºç¾å•é¡Œã€‚è«‹ç¨å¾Œå†è©¦ã€‚"

        # å¦‚æœæ¸…ç†å¾Œ response è®Šç©ºï¼Œä½† valid_extractions æœ‰å…§å®¹ï¼Œèªªæ˜ LLM å¯èƒ½å®Œå…¨æ²’æŒ‰æŒ‡ç¤ºè¼¸å‡º
        if not cleaned_response and valid_extractions:
             return f"å·²æ‰¾åˆ°é—œæ–¼åŸæ–™ã€{material_name}ã€‘çš„ç›¸é—œè³‡è¨Šç‰‡æ®µï¼Œä½†ç„¡æ³•æŒ‰é æœŸæ ¼å¼åŒ–å‘ˆç¾ã€‚è«‹ç¨å¾Œé‡è©¦ã€‚"
         
        print(f"   æ•´åˆçµæœ (å‰100å­—): \"{cleaned_response[:100].replace(os.linesep, ' ')}...\"")
        return cleaned_response

    except Exception as e:
        print(f"âŒ æ•´åˆçµæœæ™‚ç™¼ç”Ÿåš´é‡éŒ¯èª¤ï¼š{e}")
        traceback.print_exc()
        return "æ•´åˆçµæœæ™‚ç™¼ç”Ÿåš´é‡éŒ¯èª¤ï¼Œè«‹æª¢æŸ¥ç³»çµ±æ—¥èªŒã€‚"
# --- *** å‡½å¼çµæŸ *** ---

def main():
    # 1. è¼‰å…¥ Markdown æª”æ¡ˆä¸¦éæ¿¾å·¥ä½œè¡¨
    all_sections = load_markdown_sections()
    filtered_sections = filter_sections_by_title(all_sections)
    
    if not filtered_sections:
        print("âš ï¸ æœªæ‰¾åˆ°ç¬¦åˆæ¢ä»¶çš„å·¥ä½œè¡¨ï¼Œè«‹æª¢æŸ¥ALLOWED_WORKSHEET_IDENTIFIERSè¨­å®šæˆ–Markdownæ–‡ä»¶ã€‚")
        return

    # 2. ä½¿ç”¨è€…è¼¸å…¥æŸ¥è©¢é—œéµå­—
    user_input = input("è«‹è¼¸å…¥æŸ¥è©¢çš„åŸæ–™åç¨±æˆ–ç‰¹æ€§æè¿°ï¼š")
    keywords = extract_keywords_rule_based(user_input)

    if not keywords:
        print("âš ï¸ ç„¡æ³•è­˜åˆ¥ä»»ä½•æœ‰æ•ˆçš„åŸæ–™åç¨±æˆ–ç‰¹æ€§æè¿°ï¼Œè«‹æª¢æŸ¥è¼¸å…¥ã€‚")
        return

    # 3. åœ¨å·²éæ¿¾çš„å€å¡Šä¸­æœå°‹é—œéµå­—
    relevant_sections = search_sections(filtered_sections, keywords)

    if not relevant_sections:
        print("âš ï¸ åœ¨æ‰€æœ‰å·¥ä½œè¡¨ä¸­æœªæ‰¾åˆ°èˆ‡æŸ¥è©¢ç›¸é—œçš„å…§å®¹ã€‚")
        return

    # 4. æå–ç›¸é—œæ–‡æœ¬
    extracted_texts = extract_relevant_text(llm, relevant_sections, keywords)

    # 5. æ•´åˆçµæœ
    final_response = synthesize_results(llm, keywords, extracted_texts)
    
    print(f"\nğŸ”„ æœ€çµ‚æ•´åˆçµæœï¼š\n{final_response}")

if __name__ == "__main__":
    main()