import os
import re
import traceback
import time  # Added for timing process_query
from langchain.prompts import ChatPromptTemplate
from langchain_ollama.llms import OllamaLLM  # type: ignore
from langchain_core.output_parsers import StrOutputParser  # type: ignore
from dotenv import load_dotenv
import jieba  # type: ignore

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

# --- å¸¸æ•¸è¨­å®š  ---
MODEL_NAME = os.getenv("MODEL_NAME")
SERVER_URL = os.getenv("SERVER_URL")
# ç¢ºä¿ SIMPLIFIED_MD_FILENAME æœ‰ä¸€å€‹é è¨­å€¼ï¼Œä»¥é˜² .env ä¸­æœªè¨­å®š
SIMPLIFIED_MD_FILENAME = os.getenv("SIMPLIFIED_MD_FILENAME", "simplified_output_by_section.md")
TARGET_DESCRIPTION_KEYWORDS = ["çµå¡Š", "éç¯©", "é †åº", "å¸æ¿•", "ç¨ åº¦", "é»ç¨ ", "æµå‹•æ€§"]
CHINESE_STOP_WORDS = {"çš„", "å’Œ", "èˆ‡", "æˆ–", "äº†", "å‘¢", "å—", "å–”", "å•Š", "é—œæ–¼", "æœ‰é—œ", "è«‹", "è«‹å•", " ", ""}
ALLOWED_WORKSHEET_IDENTIFIERS = [
    "å·¥ä½œè¡¨: 9", "å·¥ä½œè¡¨: 10"
]

# --- LLM åˆå§‹åŒ–  ---
print(f"æ­£åœ¨åˆå§‹åŒ– Ollama LLM: Model='{MODEL_NAME}', Server='{SERVER_URL}'")
if not MODEL_NAME or not SERVER_URL:
    print("âŒ éŒ¯èª¤: MODEL_NAME æˆ– SERVER_URL æœªåœ¨ .env æª”æ¡ˆä¸­è¨­å®šã€‚ç„¡æ³•åˆå§‹åŒ– LLMã€‚")
    llm = None  # ç¢ºä¿ llm ç‚º Noneï¼Œä»¥ä¾¿å¾ŒçºŒæª¢æŸ¥
else:
    try:
        llm = OllamaLLM(
            model=MODEL_NAME,
            base_url=SERVER_URL
        )
        print("   âœ… Ollama LLM åˆå§‹åŒ–æˆåŠŸã€‚")
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ– Ollama LLM æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        llm = None  # åˆå§‹åŒ–å¤±æ•—ï¼Œè¨­ç‚º None

# --- å…¨åŸŸè®Šæ•¸ (æ–°å¢/ç®¡ç†) ---
sections_to_search = []
initialization_success = False


# --- SOP æŸ¥è©¢ç›¸é—œå‡½å¼  ---

def load_markdown_sections(filename=None):  # è®“ filename å¯ä»¥è¢«å¤–éƒ¨å‚³å…¥
    """è®€å–ä¸¦è§£æ Markdown æª”æ¡ˆ"""
    # å¦‚æœ filename æœªå‚³å…¥ï¼Œå‰‡ä½¿ç”¨å…¨åŸŸè®Šæ•¸
    if filename is None:
        filename = SIMPLIFIED_MD_FILENAME

    sections = [];
    print(f"æ­£åœ¨å˜—è©¦è¼‰å…¥æª”æ¡ˆ: {filename}")
    if not filename:  # å¦‚æœ SIMPLIFIED_MD_FILENAME ä¹Ÿæœªè¨­å®š
        print(f"âŒ éŒ¯èª¤ï¼šæœªæŒ‡å®š Markdown æª”æ¡ˆåç¨±ã€‚")
        return []

    if not os.path.exists(filename):
        print(
            f"âŒ éŒ¯èª¤ï¼šæª”æ¡ˆ '{filename}' ä¸å­˜åœ¨æ–¼ç•¶å‰ç›®éŒ„ '{os.getcwd()}'ã€‚è«‹ç¢ºä¿æª”æ¡ˆè·¯å¾‘æ­£ç¢ºæˆ– .env ä¸­çš„ SIMPLIFIED_MD_FILENAME è¨­å®šæ­£ç¢ºã€‚")
        return []

    try:
        with open(filename, 'r', encoding='utf-8') as f:
            markdown_content = f.read()
        print(f"æª”æ¡ˆ {filename} è®€å–æˆåŠŸã€‚")
    except FileNotFoundError:
        print(
            f"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°æª”æ¡ˆ '{filename}' (å†æ¬¡æª¢æŸ¥)ã€‚"); return []  # FileNotFoundError å·²è¢« os.path.exists è™•ç†ï¼Œä½†ä¿ç•™ä»¥é˜²è¬ä¸€
    except Exception as e:
        print(f"âŒ è®€å–æª”æ¡ˆ '{filename}' æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}"); return []

    parts = re.split(r'(## å·¥ä½œè¡¨:.*)', markdown_content)
    parsed_sections_count = 0
    for i in range(1, len(parts), 2):
        if i + 1 < len(parts):
            title = parts[i].strip()
            content = parts[i + 1].strip().removeprefix("Here is the simplified content:").strip().removeprefix(
                "Here is the simplified output:").strip()
            if title and content:  # è¦æ±‚æ¨™é¡Œå’Œå…§å®¹éƒ½å­˜åœ¨
                sections.append({"title": title, "content": content})
                parsed_sections_count += 1
        elif i < len(parts):  # è™•ç†æœ€å¾Œä¸€å€‹æ¨™é¡Œå¯èƒ½æ²’æœ‰å…§å®¹çš„æƒ…æ³
            title = parts[i].strip()
            if title:  # å³ä½¿å…§å®¹ç‚ºç©ºä¹ŸåŠ å…¥ï¼Œä½†é€šå¸¸æˆ‘å€‘æœŸæœ›æœ‰å…§å®¹
                sections.append({"title": title, "content": ""})  # æ¨™è¨˜ç‚ºç„¡å…§å®¹
                # parsed_sections_count +=1 # å¦‚æœä¹Ÿè¨ˆå…¥ç„¡å…§å®¹çš„å€å¡Š
    if not sections:
        print(f"âš ï¸ è­¦å‘Šï¼šæœªèƒ½å¾æª”æ¡ˆ '{filename}' è§£æå‡ºä»»ä½•å·¥ä½œè¡¨å€å¡Šã€‚")
    else:
        print(f"   å¾ '{filename}' è§£æå‡º {len(sections)} å€‹å€å¡Š ({parsed_sections_count} å€‹æœ‰å…§å®¹)ã€‚")
    return sections


def filter_sections_by_title(all_sections, allowed_identifiers=ALLOWED_WORKSHEET_IDENTIFIERS):
    """æ ¹æ“šæ¨™é¡Œé—œéµå­—éæ¿¾å€å¡Šåˆ—è¡¨"""
    if not all_sections: return []
    return [sec for sec in all_sections if
            any(allowed_id in sec.get("title", "") for allowed_id in allowed_identifiers)]


def extract_keywords_rule_based(user_input, target_keywords=TARGET_DESCRIPTION_KEYWORDS):
    """ä½¿ç”¨è¦å‰‡æ‹†åˆ†å’Œæ¯”å°é—œéµå­—ï¼Œä¸»è¦æå–åŸæ–™åç¨±ã€‚"""
    print(f"--- (éšæ®µ0) ä½¿ç”¨è¦å‰‡è§£æè¼¸å…¥ (ä¸»è¦æå–åŸæ–™): '{user_input}' ---")
    input_cleaned = user_input.strip().lower()
    if not input_cleaned:
        return None

    try:
        if 'jieba' in globals() and jieba is not None:  # ç¢ºä¿ jieba å·²æ­£ç¢ºè¼‰å…¥
            tokens = list(jieba.cut_for_search(input_cleaned))
        else:  # åŸºæœ¬åˆ†è©
            print("   âš ï¸ è­¦å‘Š: jieba æœªè¼‰å…¥ï¼Œä½¿ç”¨åŸºæœ¬ç©ºæ ¼åˆ†è©ã€‚")
            input_no_punct = re.sub(r'[^\w\s]', ' ', input_cleaned)
            tokens = input_no_punct.split()
    except Exception as e:
        print(f"   âš ï¸ åˆ†è©æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}ã€‚å°‡ä½¿ç”¨åŸºæœ¬ç©ºæ ¼åˆ†è©ã€‚")
        input_no_punct = re.sub(r'[^\w\s]', ' ', input_cleaned)
        tokens = input_no_punct.split()

    print(f"   åˆ†è©çµæœ: {tokens}")
    potential_materials = []
    identified_characteristics = set()
    for token in tokens:
        token_clean = token.strip()
        if not token_clean or token_clean in CHINESE_STOP_WORDS: continue
        is_characteristic = False
        for target_char in target_keywords:
            if token_clean == target_char.lower():
                identified_characteristics.add(target_char);
                is_characteristic = True;
                break
        if not is_characteristic:
            if not token_clean.isnumeric() and len(token_clean) > 0:
                potential_materials.append(token_clean)
    materials_list_unique = sorted(list(set(potential_materials)))
    characteristics_list_sorted = sorted(list(identified_characteristics))
    if not materials_list_unique:
        print("âš ï¸ è¦å‰‡è§£æå™¨ï¼šæœªèƒ½è­˜åˆ¥å‡ºä»»ä½•æ½›åœ¨çš„åŸæ–™åç¨±ã€‚");
        return None
    result = {"åŸæ–™åç¨±": materials_list_unique, "ç‰¹æ€§æè¿°": characteristics_list_sorted}
    print(f"   è¦å‰‡è§£æçµæœ (ä¸»è¦ç‚ºåŸæ–™ï¼Œè¼”åŠ©ç‰¹æ€§): {result}")
    return result


def search_sections(sections_to_search_local, keywords_data):  # é¿å…èˆ‡å…¨åŸŸè®Šæ•¸è¡çª
    """åœ¨å·²éæ¿¾çš„å€å¡Šä¸­åˆæ­¥ç¯©é¸åŒ…å«ã€åŸæ–™åç¨±ã€‘é—œéµå­—çš„å·¥ä½œè¡¨"""
    relevant_sections = []
    if not keywords_data: print("â„¹ï¸ é—œéµå­—å°è±¡ç‚ºç©ºï¼Œç„¡æ³•åŸ·è¡Œæœå°‹ã€‚"); return []
    material_keywords = keywords_data.get("åŸæ–™åç¨±", [])
    all_keywords = [kw for kw in material_keywords if kw and isinstance(kw, str)]
    if not all_keywords: print("â„¹ï¸ æ²’æœ‰æœ‰æ•ˆçš„ã€åŸæ–™åç¨±ã€‘é—œéµå­—å¯ä¾›æœå°‹ã€‚"); return []
    print(f"DEBUG: æ­£åœ¨ä½¿ç”¨ä»¥ä¸‹ä»»ä¸€ã€åŸæ–™åç¨±ã€‘é—œéµå­—æœå°‹: {all_keywords} (åœ¨ {len(sections_to_search_local)} å€‹å€å¡Šä¸­)")
    for section in sections_to_search_local:
        text_to_search = section.get("title", "") + section.get("content", "")
        if any(keyword.lower() in text_to_search.lower() for keyword in all_keywords):
            relevant_sections.append(section)
    return relevant_sections


def extract_relevant_text(current_llm, sections, keywords_data):  # å‚³å…¥ current_llm
    """(ç¬¬ä¸€éšæ®µ LLM) ä½¿ç”¨æ¥µåº¦èšç„¦çš„ Prompt æå–èˆ‡æŒ‡å®šã€åŸæ–™åç¨±ã€‘æœ€ç›´æ¥ç›¸é—œçš„ã€å°ç¯„åœæ–‡å­—ç‰‡æ®µã€‘ã€‚"""
    if not current_llm: print("âŒ extract_relevant_text: LLM æœªæä¾›ã€‚"); return []
    if not keywords_data: print("â„¹ï¸ extract_relevant_text: é—œéµå­—å°è±¡ç‚ºç©ºã€‚"); return []
    material_names = keywords_data.get('åŸæ–™åç¨±', [])
    if not material_names: print("â„¹ï¸ extract_relevant_text: æŸ¥è©¢ä¸­æœªæä¾›åŸæ–™åç¨±ã€‚"); return []
    material_name_str = "ã€".join(material_names)
    characteristics_list = keywords_data.get('ç‰¹æ€§æè¿°', [])
    description_keywords_str = ', '.join(characteristics_list)
    if characteristics_list:
        keywords_str_display_for_prompt = f"ä¸»è¦æŸ¥è©¢çš„åŸæ–™åç¨±ï¼šã€{material_name_str}ã€‘ã€‚(ä½¿ç”¨è€…åŒæ™‚æåŠäº†é€™äº›ç›¸é—œè©å½™ï¼Œä¾›åƒè€ƒï¼š{description_keywords_str})"
    else:
        keywords_str_display_for_prompt = f"ä¸»è¦æŸ¥è©¢çš„åŸæ–™åç¨±ï¼šã€{material_name_str}ã€‘"
    prompt_base = """
        ä½ çš„ä»»å‹™æ˜¯ä½œç‚ºä¸€å€‹ç²¾ç¢ºçš„æ–‡å­—æå–å™¨ã€‚
        åœ¨ä¸‹æ–¹æä¾›çš„ã€Œå·¥ä½œè¡¨å…§å®¹ã€ä¸­ï¼Œåƒ…æ‰¾å‡ºèˆ‡ã€Œä¸»è¦æŸ¥è©¢çš„åŸæ–™åç¨±ã€æœ€ç›´æ¥ç›¸é—œçš„ã€ä¸€å€‹æˆ–å¤šå€‹ç°¡çŸ­æ–‡å­—ç‰‡æ®µã€å¥å­æˆ–åˆ—è¡¨é …ã€‘ã€‚
        {keywords_str_display_for_prompt}
        å·¥ä½œè¡¨å…§å®¹ï¼š
        ```markdown
        {text}
        ```
        è¼¸å‡ºè¦æ±‚ï¼š
        1.  ç²¾ç¢ºå®šä½åˆ°åŒ…å«ã€Œä¸»è¦æŸ¥è©¢çš„åŸæ–™åç¨±ã€çš„å¥å­ã€æ“ä½œæ­¥é©Ÿæˆ–å…¶éå¸¸ç·Šå¯†çš„ä¸Šä¸‹æ–‡ã€‚
        2.  **ã€ç›´æ¥è¼¸å‡ºã€‘** æ‰¾åˆ°çš„ç›¸é—œå…§å®¹ã€‚æå–çš„ç¯„åœæ‡‰ç›¡å¯èƒ½å°è€Œç²¾æº–ã€‚ä¾‹å¦‚ï¼Œåƒ…æå–åŒ…å«è©²åŸæ–™åç¨±çš„é‚£å€‹æ­¥é©Ÿã€é‚£å¥æè¿°æˆ–ç›¸é—œçš„åˆ—è¡¨é …ã€‚
        3.  **ã€é¿å…æå–ã€‘** éé•·çš„æ®µè½æˆ–æ•´å€‹å­æ¨™é¡Œä¸‹çš„æ‰€æœ‰å…§å®¹ï¼Œé™¤éè©²åŸæ–™åç¨±åœ¨æ•´å€‹æ®µè½/å­æ¨™é¡Œä¸­éƒ½é«˜åº¦ç›¸é—œä¸”å…§å®¹ä¸å¯åˆ†å‰²ã€‚å„ªå…ˆæå–æœ€å°çš„æœ‰æ•ˆè³‡è¨Šå–®ä½ã€‚
        4.  **ã€çµ•å°ç¦æ­¢ã€‘** åŒ…å« '## å·¥ä½œè¡¨: ...' é€™å€‹æœ€é«˜ç´šåˆ¥çš„æ¨™é¡Œã€‚
        5.  **ã€çµ•å°ç¦æ­¢ã€‘** åŒ…å«ä»»ä½• 'è£½è¡¨æ—¥æœŸ', 'è£½è¡¨äºº' ç­‰é è…³è³‡è¨Šã€‚
        6.  **ã€çµ•å°ç¦æ­¢ã€‘** é€²è¡Œä»»ä½•æ‘˜è¦ã€æ”¹å¯«æˆ–æ·»åŠ ä»»ä½•ã€è§£é‡‹æ€§æ–‡å­—ã€å‰è¨€ã€çµèªã€‘(ä¾‹å¦‚ä¸è¦èªª "ä»¥ä¸‹æ˜¯æ‰¾åˆ°çš„å…§å®¹ï¼š" æˆ– "æ ¹æ“šé—œéµå­—..." ç­‰)ã€‚è¼¸å‡ºçµæœå¿…é ˆç›´æ¥å°±æ˜¯æå–çš„å…§å®¹æœ¬èº«ã€‚
        7.  å¦‚æœåœ¨æ­¤ã€Œå·¥ä½œè¡¨å…§å®¹ã€ä¸­æ‰¾ä¸åˆ°ç›´æ¥èˆ‡ã€Œä¸»è¦æŸ¥è©¢çš„åŸæ–™åç¨±ã€ç›¸é—œçš„å…§å®¹ï¼Œã€åªè¼¸å‡ºã€‘ä»¥ä¸‹å›ºå®šæ–‡å­—ï¼šã€Œåœ¨æ­¤æ–‡ä»¶ä¸­æœªæ‰¾åˆ°èˆ‡æ­¤åŸæ–™ç›´æ¥ç›¸é—œçš„å…§å®¹ã€ã€‚
        8.  å¿…é ˆä½¿ç”¨ã€ç¹é«”ä¸­æ–‡ã€‘è¼¸å‡º (å¦‚æœæå–çš„å…§å®¹æœ¬èº«æ˜¯ä¸­æ–‡)ã€‚
        å†æ¬¡å¼·èª¿ï¼šç›´æ¥è¼¸å‡ºæå–çš„ã€èˆ‡åŸæ–™ç›´æ¥ç›¸é—œçš„ã€å°ç¯„åœã€‘å…§å®¹ï¼Œä¸è¦åŠ ä»»ä½•å…¶ä»–æ–‡å­—ã€‚
        """
    prompt_template = ChatPromptTemplate.from_template(prompt_base)
    chain = prompt_template | current_llm | StrOutputParser()
    relevant_texts = []
    print(f"--- (éšæ®µ1) ä½¿ç”¨ LLM å®šä½ä¸¦æå–å­ç« ç¯€ï¼Œä½¿ç”¨çš„æŸ¥è©¢ä¸Šä¸‹æ–‡: {keywords_str_display_for_prompt} ---")
    for section in sections:
        section_title = section.get("title", "ç„¡æ¨™é¡Œå€å¡Š");
        section_content = section.get("content", "")
        section_full_text = f"{section_content}"
        print(f"   æ­£åœ¨è™•ç†å€å¡Š: {section_title} (æœå°‹åŸæ–™ '{material_name_str}')...")
        try:
            relevant_text = chain.invoke(
                {"keywords_str_display_for_prompt": keywords_str_display_for_prompt, "text": section_full_text})
            relevant_text = relevant_text.strip()
            if relevant_text.startswith("æ ¹æ“šé—œéµå­—") or relevant_text.startswith("ä»¥ä¸‹æ˜¯æ‰¾åˆ°"):
                lines = relevant_text.splitlines();
                relevant_text = "\n".join(lines[1:]).strip()
            if relevant_text.startswith("```markdown"): relevant_text = relevant_text.removeprefix(
                "```markdown").removesuffix("```").strip()
            is_found_message = "åœ¨æ­¤æ–‡ä»¶ä¸­æœªæ‰¾åˆ°èˆ‡æ­¤åŸæ–™ç›´æ¥ç›¸é—œçš„å…§å®¹" in relevant_text
            if is_found_message or not relevant_text:
                print(
                    f"     â†³ åœ¨å€å¡Š '{section_title}' ä¸­æœªæ‰¾åˆ°æˆ–æå–åˆ°ç©ºå…§å®¹é—œæ–¼åŸæ–™ '{material_name_str}'ã€‚LLMå›æ‡‰: '{relevant_text}'")
                relevant_texts.append(
                    {"title": section_title, "text": relevant_text if relevant_text else "å…§å®¹æå–ç‚ºç©º",
                     "found": False})
            else:
                print(
                    f"     â†³ å¾ '{section_title}' æå–åˆ°å…§å®¹ (å‰100å­—): \"{relevant_text[:100].replace(os.linesep, ' ')}...\"")
                relevant_texts.append({"title": section_title, "text": relevant_text, "found": True})
        except Exception as e:
            print(f"âŒ å¾å€å¡Š '{section_title}' æå–æ™‚å‡ºéŒ¯: {e}");
            traceback.print_exc()
            relevant_texts.append({"title": section_title, "text": "LLM æå–å¤±æ•—", "found": False})
    return relevant_texts


def synthesize_results(current_llm, keywords_data, extracted_texts):  # å‚³å…¥ current_llm
    """(ç¬¬äºŒéšæ®µ LLM) å°‡æå–å‡ºçš„ã€å°ç¯„åœæ–‡å­—ç‰‡æ®µã€‘æ•´åˆæˆã€æ¥µç°¡ä¸”åŸæ–‡å‘ˆç¾çš„çµ±ä¸€æ ¼å¼åˆ—è¡¨ã€‘ã€‚"""
    if not current_llm: print("âŒ synthesize_results: LLM æœªæä¾›ã€‚"); return "LLMæœªå°±ç·’ï¼Œç„¡æ³•æ•´åˆçµæœã€‚"
    if not keywords_data: return "ç„¡æ•ˆçš„é—œéµå­—ï¼Œç„¡æ³•æ•´åˆçµæœã€‚"
    if not extracted_texts: return "æœªèƒ½æå–åˆ°ä»»ä½•ç›¸é—œå…§å®¹ä»¥ä¾›æ•´åˆã€‚"
    valid_extractions = [
        item['text'] for item in extracted_texts
        if item.get("found", False) and "LLM æå–å¤±æ•—" not in item['text'] and \
           "åœ¨æ­¤æ–‡ä»¶ä¸­æœªæ‰¾åˆ°èˆ‡æ­¤åŸæ–™ç›´æ¥ç›¸é—œçš„å…§å®¹" not in item['text'] and \
           item['text'] and item['text'].strip()
    ]
    if not valid_extractions:
        material_names_list = keywords_data.get('åŸæ–™åç¨±', [])
        material_name_str = "ã€".join(material_names_list) if material_names_list else "æ‰€æŸ¥è©¢çš„é …ç›®"
        all_attempted_but_not_found = all(
            not item.get("found") or "åœ¨æ­¤æ–‡ä»¶ä¸­æœªæ‰¾åˆ°èˆ‡æ­¤åŸæ–™ç›´æ¥ç›¸é—œçš„å…§å®¹" in item['text'] or item[
                'text'].strip() == ""
            for item in extracted_texts
        )
        if extracted_texts and all_attempted_but_not_found:
            return f"å·²æª¢æŸ¥æ‰€æœ‰ç›¸é—œSOPæ–‡ä»¶å€å¡Šï¼Œä½†å‡æœªæ‰¾åˆ°é—œæ–¼åŸæ–™ã€{material_name_str}ã€‘çš„ç›´æ¥æ“ä½œèªªæ˜æˆ–æ³¨æ„äº‹é …ã€‚"
        return f"é›–ç„¶åˆæ­¥å®šä½åˆ°å¯èƒ½ç›¸é—œçš„SOPå€å¡Šï¼Œä½†åœ¨å…¶ä¸­æœªèƒ½æ‰¾åˆ°é—œæ–¼åŸæ–™ã€{material_name_str}ã€‘çš„å…·é«”æ“ä½œèªªæ˜æˆ–æ³¨æ„äº‹é …ã€‚"
    print(f"\nğŸ”„ (éšæ®µ2) æ­£åœ¨æ•´åˆ {len(valid_extractions)} ä»½æå–çš„é‡é»å…§å®¹ä¸¦çµ±ä¸€æ ¼å¼ (åŠ›æ±‚åŸæ–‡ã€ç°¡æ½”)...")
    combined_extracted_text = "\n\nç‰‡æ®µåˆ†éš”ç·š (è«‹å°‡æ¯å€‹ç‰‡æ®µè¦–ç‚ºç¨ç«‹è³‡è¨Šä¾†æº)\n\n".join(valid_extractions)
    material_names_list = keywords_data.get('åŸæ–™åç¨±', [])
    material_name = "ã€".join(material_names_list) if material_names_list else "æœªæŒ‡å®šåŸæ–™"
    characteristics_list = keywords_data.get('ç‰¹æ€§æè¿°', [])
    if characteristics_list:
        keywords_str_for_prompt = f"ä½¿ç”¨è€…ä¸»è¦æŸ¥è©¢çš„åŸæ–™åç¨±ç‚ºã€{material_name}ã€‘ã€‚(ä½¿ç”¨è€…æŸ¥è©¢æ™‚æåŠçš„ç›¸é—œè©å½™ï¼Œä¾›æ‚¨ç†è§£ä¸Šä¸‹æ–‡ï¼š{', '.join(characteristics_list)})"
    else:
        keywords_str_for_prompt = f"ä½¿ç”¨è€…ä¸»è¦æŸ¥è©¢çš„åŸæ–™åç¨±ç‚ºã€{material_name}ã€‘ã€‚"
    synthesis_prompt_template_str = """
        æ‚¨æ˜¯ä¸€ä½SOPå…§å®¹æ•´ç†å“¡ã€‚æ‚¨çš„ä»»å‹™æ˜¯å°‡ä¸‹æ–¹æä¾›çš„ã€å·²å¾SOPæ–‡ä»¶ä¸­æå–å‡ºçš„ã€èˆ‡æŒ‡å®šåŸæ–™ç›¸é—œçš„ã€å¤šå€‹ç¨ç«‹çš„ç°¡çŸ­æ–‡å­—ç‰‡æ®µã€‘ï¼Œæ•´ç†æˆä¸€å€‹ã€æ¥µç°¡çš„ã€çµ±ä¸€æ ¼å¼çš„æ•¸å­—ç·¨è™Ÿåˆ—è¡¨ã€‘ã€‚
        {keywords_str_for_prompt}
        å·²æå–çš„ç›¸é—œSOPç‰‡æ®µ (é€™äº›ç‰‡æ®µä¾†è‡ªä¸åŒåœ°æ–¹ï¼Œè«‹å°‡å®ƒå€‘è¦–ç‚ºç¨ç«‹çš„è³‡è¨Šé»ï¼Œä½¿ç”¨ã€Œç‰‡æ®µåˆ†éš”ç·šã€éš”é–‹)ï¼š
        ---
        {combined_extracted_text}
        ---
        æ‚¨çš„ä»»å‹™èˆ‡è¼¸å‡ºè¦æ±‚ï¼š
        1.  ä»”ç´°é–±è®€æ‰€æœ‰ã€Œå·²æå–çš„ç›¸é—œSOPç‰‡æ®µã€ã€‚æ¯å€‹ç‰‡æ®µéƒ½æ˜¯é—œæ–¼ä¸Šè¿°ã€Œä¸»è¦æŸ¥è©¢çš„åŸæ–™åç¨±ã€çš„ç›´æ¥ç›¸é—œå…§å®¹ã€‚
        2.  **ã€æ ¸å¿ƒä»»å‹™ã€‘ï¼šå°‡é€™äº›ç‰‡æ®µä¸­çš„ã€æ¯ä¸€å€‹ç¨ç«‹çš„è³‡è¨Šé»ã€æ“ä½œæ­¥é©Ÿã€æˆ–æ³¨æ„äº‹é …ã€‘æ•´ç†å‡ºä¾†ï¼Œä½œç‚ºåˆ—è¡¨ä¸­çš„ä¸€å€‹ç¨ç«‹é …ç›®ã€‚**
        3.  **ã€æ ¼å¼çµ±ä¸€ã€‘ï¼š** ä½¿ç”¨å¾ 1 é–‹å§‹çš„æ•¸å­—ç·¨è™Ÿåˆ—è¡¨ (æ ¼å¼å¦‚ï¼š1., 2., 3., ...)ã€‚
        4.  **ã€åŸæ–‡å‘ˆç¾ã€‘ï¼š** ç›¡æœ€å¤§å¯èƒ½ã€ç›´æ¥ä½¿ç”¨ã€‘æå–ç‰‡æ®µä¸­çš„ã€åŸæ–‡è¡¨è¿°ã€‘ã€‚**ã€åš´æ ¼ç¦æ­¢ã€‘ä»»ä½•å½¢å¼çš„æ”¹å¯«ã€æ‘˜è¦ã€è§£é‡‹ã€æ­¸ç´æˆ–æ·»åŠ æ‚¨è‡ªå·±çš„æ–‡å­—æˆ–è©•è«–ã€‚** æ‚¨çš„å·¥ä½œæ˜¯ã€Œå½™æ•´åŸæ–‡ã€å’Œã€Œæ ¼å¼åŒ–ç‚ºåˆ—è¡¨ã€ï¼Œè€Œä¸æ˜¯ã€Œå†å‰µä½œã€æˆ–ã€Œè§£é‡‹ã€ã€‚
        5.  å¦‚æœåŸå§‹ç‰‡æ®µæœ¬èº«å°±æ˜¯ä¸€å€‹çŸ­åˆ—è¡¨æˆ–åŒ…å«å­æ­¥é©Ÿï¼Œè«‹åœ¨æ–°çš„æ•¸å­—ç·¨è™Ÿä¸‹ï¼Œç›¡å¯èƒ½ä¿æŒå…¶åŸå§‹çµæ§‹ï¼Œä¾‹å¦‚ä½¿ç”¨ç¸®æ’å’Œ '-' æˆ– '*' ä¾†è¡¨ç¤ºå±¤ç´šé—œä¿‚ã€‚
        6.  **ã€æ¥µç°¡è¼¸å‡ºã€‘ï¼š** æ‚¨çš„æœ€çµ‚è¼¸å‡ºã€å¿…é ˆç›´æ¥æ˜¯é€™å€‹æ•¸å­—ç·¨è™Ÿåˆ—è¡¨æœ¬èº«ã€‘ã€‚**ã€åš´æ ¼ç¦æ­¢ã€‘** åŒ…å«ä»»ä½•å‰è¨€ï¼ˆä¾‹å¦‚ "å¥½çš„ï¼Œé€™æ˜¯æ•´ç†å¾Œçš„åˆ—è¡¨ï¼š" æˆ– "é—œæ–¼æ‚¨æŸ¥è©¢çš„..."ï¼‰ã€æ¨™é¡Œã€å¼•å°èªæˆ–çµèªã€‚
        7.  å¦‚æœå¤šå€‹ç‰‡æ®µæè¿°çš„æ˜¯ã€å®Œå…¨ç›¸åŒæˆ–å¹¾ä¹å®Œå…¨é‡è¤‡ã€‘çš„è³‡è¨Šï¼Œè«‹åªé¸æ“‡å…¶ä¸­ä¸€å€‹æœ€æ¸…æ™°çš„è¡¨è¿°æ”¾å…¥åˆ—è¡¨ï¼Œä»¥é¿å…ä¸å¿…è¦çš„å†—é¤˜ã€‚ä½†è‹¥æœ‰äº›å¾®å·®ç•°æˆ–è£œå……ï¼Œå¯§å¯ä¿ç•™å…©è€…ï¼Œé¿å…è³‡è¨Šéºå¤±ã€‚**åˆ¤æ–·é‡è¤‡çš„æ¨™æº–è¦éå¸¸åš´æ ¼ã€‚**
        8.  ä½¿ç”¨**ç¹é«”ä¸­æ–‡**ã€‚
        9.  å³ä½¿æœ€çµ‚æ•´ç†å‡ºä¾†çš„è³‡è¨Šé»å¾ˆå°‘ï¼ˆä¾‹å¦‚åªæœ‰ä¸€å…©æ¢ï¼‰ï¼Œä¹Ÿç›´æ¥æŒ‰åˆ—è¡¨æ ¼å¼è¼¸å‡ºã€‚
        10. **ã€çµ•å°ç¦æ­¢ã€‘** åœ¨è¼¸å‡ºä¸­æåŠä»»ä½•ã€Œå·¥ä½œè¡¨æ¨™é¡Œã€ã€ã€ŒSOPæ–‡ä»¶ä¾†æºã€æˆ–ã€Œç‰‡æ®µåˆ†éš”ç·šã€ç­‰å…ƒä¿¡æ¯ã€‚
        11. **ã€çµ•å°ç¦æ­¢ã€‘** æ·»åŠ ä»»ä½•è¶…å‡ºæ‰€æä¾›ç‰‡æ®µå…§å®¹ä¹‹å¤–çš„è³‡è¨Šæˆ–å»ºè­°ã€‚
        è«‹ç›´æ¥é–‹å§‹è¼¸å‡ºåˆ—è¡¨ï¼š
        """
    synthesis_prompt_template = ChatPromptTemplate.from_template(synthesis_prompt_template_str)
    synthesis_chain = synthesis_prompt_template | current_llm | StrOutputParser()
    try:
        final_response = synthesis_chain.invoke(
            {"keywords_str_for_prompt": keywords_str_for_prompt, "combined_extracted_text": combined_extracted_text})
        final_response = final_response.strip()
        unwanted_prefixes = [
            "å¥½çš„ï¼Œé€™æ˜¯æ•´ç†å¾Œçš„åˆ—è¡¨ï¼š", "é€™æ˜¯ç‚ºæ‚¨æ•´ç†çš„åˆ—è¡¨ï¼š", "ä»¥ä¸‹æ˜¯æ•´ç†å¾Œçš„åˆ—è¡¨ï¼š", "æ ¹æ“šæ‚¨æä¾›çš„è³‡è¨Šï¼š",
            "é—œæ–¼æ‚¨æŸ¥è©¢çš„åŸæ–™", "é—œæ–¼æ‚¨æŸ¥è©¢çš„", "Okay, here is the list:", "Here is the list:",
            "Here is the output:", "é€™æ˜¯æ‚¨æŸ¥è©¢çš„çµæœï¼š", "é€™æ˜¯ç›¸é—œçš„è³‡è¨Šï¼š", "æŸ¥è©¢çµæœå¦‚ä¸‹ï¼š",
        ]
        cleaned_response = final_response
        for prefix in unwanted_prefixes:
            if prefix.strip().startswith("1.") and cleaned_response.strip().startswith("1."): continue
            if final_response.lower().startswith(prefix.lower()):
                prefix_len = len(prefix);
                temp_response = final_response[prefix_len:].lstrip("ï¼š: ").strip()
                if temp_response or prefix.lower() not in ["1."]:
                    cleaned_response = temp_response
                    print(f"   DEBUG: (Synthesize) ç§»é™¤äº†ä¸éœ€è¦çš„å‰ç¶´ '{prefix}'");
                    break
        lines = cleaned_response.splitlines()
        if lines and not re.match(r"^\s*\d+\.\s*", lines[0]):
            first_line_lower = lines[0].lower();
            is_likely_intro = True
            for mat_kw in keywords_data.get("åŸæ–™åç¨±", []):
                if mat_kw.lower() in first_line_lower:
                    if len(lines[0]) > 20: is_likely_intro = False; break
            if len(lines[0]) > 50: is_likely_intro = False
            if is_likely_intro and len(lines) > 1:
                print(f"   DEBUG: (Synthesize) è¼¸å‡ºé¦–è¡Œ '{lines[0]}' ä¸¦éæ¨™æº–åˆ—è¡¨é …ï¼Œå˜—è©¦ç§»é™¤ä½œç‚ºå¼•è¨€ã€‚")
                cleaned_response = "\n".join(lines[1:]).strip()
            elif is_likely_intro and len(lines) == 1:
                print(f"   DEBUG: (Synthesize) è¼¸å‡ºåªæœ‰ä¸€è¡Œ '{lines[0]}' ä¸”ä¸¦éæ¨™æº–åˆ—è¡¨é …ï¼Œå¯èƒ½LLMæœªéµå¾ªæŒ‡ç¤ºã€‚")
        if not cleaned_response.strip() and valid_extractions:
            print(f"   âš ï¸ è­¦å‘Š: (Synthesize) LLM æœ€çµ‚è¼¸å‡ºç‚ºç©ºï¼Œä½†ä¹‹å‰æœ‰ {len(valid_extractions)} æ¢æœ‰æ•ˆæå–ã€‚")
            return f"å·²æ‰¾åˆ°é—œæ–¼åŸæ–™ã€{material_name}ã€‘çš„ç›¸é—œè³‡è¨Šï¼Œä½†åœ¨æœ€çµ‚æ•´ç†æ ¼å¼æ™‚å‡ºç¾å•é¡Œã€‚"
        if not cleaned_response and valid_extractions:
            return f"å·²æ‰¾åˆ°é—œæ–¼åŸæ–™ã€{material_name}ã€‘çš„ç›¸é—œè³‡è¨Šç‰‡æ®µï¼Œä½†ç„¡æ³•æŒ‰é æœŸæ ¼å¼åŒ–å‘ˆç¾ã€‚"
        # print(f"   æ•´åˆçµæœ (å‰100å­—): \"{cleaned_response[:100].replace(os.linesep, ' ')}...\"") # å·²ç§»åˆ° process_query
        return cleaned_response
    except Exception as e:
        print(f"âŒ æ•´åˆçµæœæ™‚ç™¼ç”Ÿåš´é‡éŒ¯èª¤ï¼š{e}");
        traceback.print_exc()
        return "æ•´åˆçµæœæ™‚ç™¼ç”Ÿåš´é‡éŒ¯èª¤ï¼Œè«‹æª¢æŸ¥ç³»çµ±æ—¥èªŒã€‚"


# --- æ–°å¢/ä¿®æ”¹ï¼šç³»çµ±åˆå§‹åŒ–ã€æŸ¥è©¢è™•ç†ã€ä¸»åŸ·è¡Œå€å¡Š ---

def initialize_system():
    """åˆå§‹åŒ–ç³»çµ±è³‡æºï¼Œå¦‚è¼‰å…¥SOPæ–‡ä»¶ã€‚"""
    global sections_to_search, initialization_success, llm  # llm æ˜¯å¾å¤–éƒ¨å‚³å…¥çš„

    print("--- é–‹å§‹åŸ·è¡Œç³»çµ±åˆå§‹åŒ– ---")
    initialization_success = False

    # 1. æª¢æŸ¥ LLM æ˜¯å¦å·²åˆå§‹åŒ– (ç”±å¤–éƒ¨è…³æœ¬å®Œæˆ)
    if llm is None:
        print("âŒ éŒ¯èª¤ï¼šLLM æ¨¡å‹æœªåœ¨è…³æœ¬é–‹é ­æˆåŠŸåˆå§‹åŒ–ã€‚ç„¡æ³•ç¹¼çºŒã€‚")
        return False
    print("   âœ… LLM å·²ç”±å¤–éƒ¨åˆå§‹åŒ–ã€‚")

    # 2. è¼‰å…¥ä¸¦éæ¿¾ SOP æ–‡ä»¶å€å¡Š
    print("2. è¼‰å…¥ä¸¦éæ¿¾ SOP æ–‡ä»¶å€å¡Š...")
    # SIMPLIFIED_MD_FILENAME å·²æ˜¯å…¨åŸŸè®Šæ•¸
    all_loaded_sections = load_markdown_sections(SIMPLIFIED_MD_FILENAME)
    if all_loaded_sections:
        original_count = len(all_loaded_sections)
        # ALLOWED_WORKSHEET_IDENTIFIERS å·²æ˜¯å…¨åŸŸè®Šæ•¸
        filtered_sections = filter_sections_by_title(all_loaded_sections, ALLOWED_WORKSHEET_IDENTIFIERS)
        if filtered_sections:
            sections_to_search = filtered_sections  # ä½¿ç”¨éæ¿¾å¾Œçš„
            allowed_titles_str = ', '.join([s.get('title', '?') for s in sections_to_search])
            print(
                f"   âœ… æˆåŠŸè¼‰å…¥ {original_count} å€å¡Šï¼Œå·²éæ¿¾å‡º {len(sections_to_search)} å€‹ç›®æ¨™å€å¡Š: [{allowed_titles_str}]")
        else:
            print(f"   âš ï¸ è­¦å‘Šï¼šæˆåŠŸè¼‰å…¥ {original_count} å€å¡Šï¼Œä½†æ ¹æ“š ALLOWED_WORKSHEET_IDENTIFIERS æœªéæ¿¾å‡ºç›®æ¨™å€å¡Šã€‚")
            print(f"   å°‡å˜—è©¦åœ¨æ‰€æœ‰ {original_count} å€‹å·²è¼‰å…¥å€å¡Šä¸­æœå°‹ (å¦‚æœé€™æ˜¯é æœŸè¡Œç‚º)ã€‚")
            sections_to_search = all_loaded_sections  # è‹¥ç„¡éæ¿¾çµæœï¼Œå‰‡æœå°‹å…¨éƒ¨å·²è¼‰å…¥å€å¡Š
            if not sections_to_search:  # å¦‚æœ all_loaded_sections æœ¬èº«å°±ç‚ºç©º
                print(f"âŒ éŒ¯èª¤ï¼šæœªèƒ½å¾ '{SIMPLIFIED_MD_FILENAME}' è§£æå‡ºä»»ä½•å€å¡Šï¼Œä¸”éæ¿¾çµæœä¹Ÿç‚ºç©ºã€‚")
                return False
    else:
        print(f"âŒ éŒ¯èª¤ï¼šæœªèƒ½å¾ '{SIMPLIFIED_MD_FILENAME}' è¼‰å…¥ä»»ä½• SOP æ–‡ä»¶å€å¡Šã€‚ç³»çµ±ç„¡æ³•è™•ç†æŸ¥è©¢ã€‚")
        sections_to_search = []
        return False

    initialization_success = True
    print("--- ç³»çµ±åˆå§‹åŒ–æˆåŠŸå®Œæˆ ---")
    return True


def process_query(user_query):
    """è™•ç†å–®ä¸€ä½¿ç”¨è€…æŸ¥è©¢ä¸¦è¿”å›çµæœã€‚"""
    global llm, sections_to_search, initialization_success  # ä½¿ç”¨å…¨åŸŸ llm å’Œ sections_to_search

    if not initialization_success:
        error_message = "ç³»çµ±åˆå§‹åŒ–å¤±æ•—ï¼Œç„¡æ³•è™•ç†æŸ¥è©¢ã€‚"
        print(f"âŒ {error_message}")
        return error_message
    if not llm:  # å†æ¬¡ç¢ºèªï¼Œé›–ç„¶ initialize_system å·²æª¢æŸ¥
        error_message = "LLM æ¨¡å‹æœªå°±ç·’ã€‚"
        print(f"âŒ {error_message}")
        return error_message
    if not sections_to_search:
        error_message = "æ²’æœ‰å¯ä¾›æŸ¥è©¢çš„SOPæ–‡ä»¶å€å¡Šã€‚"
        print(f"âŒ {error_message}")
        return error_message

    print(f"\nè™•ç†æŸ¥è©¢: '{user_query}'")
    start_time = time.time()
    reply_text = "æŠ±æ­‰ï¼Œè™•ç†æ‚¨çš„è«‹æ±‚æ™‚ç™¼ç”Ÿå…§éƒ¨å•é¡Œã€‚"

    try:
        keywords_data = extract_keywords_rule_based(user_query, TARGET_DESCRIPTION_KEYWORDS)

        if keywords_data and keywords_data.get("åŸæ–™åç¨±"):
            # search_sections ä½¿ç”¨å…¨åŸŸ sections_to_search
            relevant_sop_sections = search_sections(sections_to_search, keywords_data)

            if relevant_sop_sections:
                print(
                    f"   åˆæ­¥æ‰¾åˆ° {len(relevant_sop_sections)} å€‹å¯èƒ½ç›¸é—œå€å¡Šï¼Œé‡å°åŸæ–™: {keywords_data.get('åŸæ–™åç¨±')}")
                # extract_relevant_text å’Œ synthesize_results ä½¿ç”¨å…¨åŸŸ llm
                extracted_texts = extract_relevant_text(llm, relevant_sop_sections, keywords_data)
                final_summary = synthesize_results(llm, keywords_data, extracted_texts)
                reply_text = final_summary
            else:
                material_name_str = "ã€".join(keywords_data.get("åŸæ–™åç¨±", ["æœªçŸ¥åŸæ–™"]))
                reply_text = f"åœ¨SOPæ–‡ä»¶ ({SIMPLIFIED_MD_FILENAME}) ä¸­ï¼Œæ‰¾ä¸åˆ°èˆ‡åŸæ–™ã€{material_name_str}ã€‘ç›´æ¥ç›¸é—œçš„å…§å®¹ã€‚"
                print(f"   åœ¨å·²è¼‰å…¥å’Œéæ¿¾çš„å€å¡Šä¸­æœªæ‰¾åˆ°èˆ‡åŸæ–™ã€{material_name_str}ã€‘ç›¸é—œçš„å…§å®¹ã€‚")
        else:
            reply_text = "ç„¡æ³•å¾æ‚¨çš„è¨Šæ¯ä¸­è§£æå‡ºæœ‰æ•ˆçš„åŸæ–™åç¨±é€²è¡ŒæŸ¥è©¢ã€‚è«‹å˜—è©¦è¼¸å…¥æ˜ç¢ºçš„åŸæ–™åã€‚"
            print("   æœªèƒ½å¾è¼¸å…¥è§£æå‡ºæœ‰æ•ˆçš„åŸæ–™åç¨± (è¦å‰‡è§£æå¤±æ•—æˆ–ç„¡åŸæ–™çµæœ)ã€‚")
    except Exception as e:
        print(f"!!!!!!!!!! è™•ç†æŸ¥è©¢ '{user_query}' æ™‚ç™¼ç”Ÿåš´é‡éŒ¯èª¤ !!!!!!!!!!")
        traceback.print_exc()
        reply_text = f"è™•ç†æŸ¥è©¢ '{user_query}' æ™‚é‡åˆ°æœªé æœŸçš„éŒ¯èª¤ï¼Œè«‹æª¢æŸ¥æ—¥èªŒã€‚"

    end_time = time.time()
    processing_time = end_time - start_time
    print(f"æŸ¥è©¢ \"{user_query}\" è™•ç†å®Œæˆï¼Œè€—æ™‚ {processing_time:.2f} ç§’ã€‚")

    if not reply_text or not reply_text.strip():
        print("âš ï¸è­¦å‘Šï¼šæœ€çµ‚å›è¦†å…§å®¹ç‚ºç©ºï¼Œå°‡è¿”å›é€šç”¨æç¤ºè¨Šæ¯ã€‚")
        reply_text = "æŠ±æ­‰ï¼Œæœªèƒ½æ‰¾åˆ°æ˜ç¢ºçš„è³‡è¨Šæˆ–è™•ç†æ™‚ç™¼ç”Ÿå•é¡Œã€‚è«‹å˜—è©¦æ›´æ›æŸ¥è©¢è©æˆ–ç¨å¾Œå†è©¦ã€‚"

    print(f"   æœ€çµ‚å›è¦† (å‰100å­—): \"{reply_text[:100].replace(os.linesep, ' ')}...\"")
    return reply_text


if __name__ == "__main__":
    print("--- SOP æŸ¥è©¢ç³»çµ± (æ§åˆ¶å°ç‰ˆæœ¬) ---")

    # åŸ·è¡Œç³»çµ±åˆå§‹åŒ–
    if initialize_system():
        print("\n--- ç³»çµ±å·²å°±ç·’ ---")
        print(f"å°‡å¾ '{SIMPLIFIED_MD_FILENAME}' æª”æ¡ˆä¸­æŸ¥è©¢ã€‚")
        if ALLOWED_WORKSHEET_IDENTIFIERS:
            print(f"é™å®šæŸ¥è©¢çš„å·¥ä½œè¡¨ç¯„åœé—œéµå­—: {ALLOWED_WORKSHEET_IDENTIFIERS}")
        else:
            print("æœªè¨­å®šç‰¹å®šå·¥ä½œè¡¨ç¯„åœï¼Œå°‡æœå°‹æ‰€æœ‰è§£æå‡ºçš„å€å¡Šã€‚")
        print("è¼¸å…¥æ‚¨çš„æŸ¥è©¢ (ä¾‹å¦‚ï¼š'é£Ÿé¹½ çµå¡Š')")
        print("è¼¸å…¥ 'exit' æˆ– 'quit' ä¾†çµæŸç¨‹å¼ã€‚")

        while True:
            try:
                user_input = input("\næ‚¨çš„æŸ¥è©¢: ")
                if user_input.strip().lower() in ['exit', 'quit']:
                    print("æ­£åœ¨çµæŸç¨‹å¼...")
                    break
                if not user_input.strip():
                    continue

                result = process_query(user_input)
                print("\n========== æŸ¥è©¢çµæœ ==========")
                print(result)
                print("==============================")

            except KeyboardInterrupt:
                print("\nåµæ¸¬åˆ°ä½¿ç”¨è€…ä¸­æ–· (Ctrl+C)ï¼Œæ­£åœ¨çµæŸç¨‹å¼...")
                break
            except Exception as e:
                print(f"\nåœ¨ä¸»æŸ¥è©¢è¿´åœˆä¸­ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤: {e}")
                traceback.print_exc()
                # é¸æ“‡æ˜¯å¦ç¹¼çºŒæˆ–ä¸­æ–·
                # break
    else:
        print("\nâŒ å› ç³»çµ±åˆå§‹åŒ–å¤±æ•—ï¼Œç„¡æ³•å•Ÿå‹• SOP æŸ¥è©¢ç³»çµ±ã€‚è«‹æª¢æŸ¥ä¸Šæ–¹çš„éŒ¯èª¤è¨Šæ¯ã€‚")

    print("--- ç¨‹å¼åŸ·è¡Œå®Œç•¢ ---")